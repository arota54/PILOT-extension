// TODO support multiple levels of includes 
//  When all nimbodes go down and one or few of them come up   Unfortunately there might not be an exact way to know which one contains the most updated blob, 
//  TODO: this assumes key is only from the one field   if not we need to have order of fields in PK 
//  the following code is duplicated in the constructor of MqttPublisher   we reproduce it here so we fail on the client side if SSL is misconfigured, rather than when the topology   is deployed to the cluster 
//  should remove the third blob, because the first has the reset timestamp 
//  Get rid of multiple '/' in url
//  Todo: Track file offsets instead of line number 
//  TODO support more configuration options, for now we're defaulting to the hbase-*.xml files found on the classpath 
// This is a bit ugly, but it works.  In order to maintain the same directory structure that existed before   we need to have storm conf, storm jar, and storm code in a shared directory, and we need to set the   permissions for that entire directory, but the tracking is on a per item basis, so we are going to end   up running the permission modification code once for each blob that is downloaded (3 times in this case).   Because the permission modification code runs in a separate process we are doing a global lock to avoid   any races between multiple versions running at the same time.  Ideally this would be on a per topology   basis, but that is a lot harder and the changes run fairly quickly so it should not be a big deal. 
// Could not find a "fully" compatible version.  Look to see if there is a possibly compatible version right below it 
// ResourceUtils.java is not a available on the classpath to let us parse out the resources we want. 
//  Files/move with non-empty directory doesn't work well on Windows   This is not atomic but it does work 
/*  Not thread safe. Have one instance per producer thread or synchronize externally  */
//  TODO substitution implementation is not exactly efficient or kind to memory... 
// TODO need a better way to do this 
//  TODO: Need to be able to set the tick tuple time to the message timeout, ideally without parameterization 
/*              * Connection is unavailable. We will drop pending messages and let at-least-once message replay kick in.             *             * Another option would be to buffer the messages in memory.  But this option has the risk of causing OOM errors,             * especially for topologies that disable message acking because we don't know whether the connection recovery will             * succeed  or not, and how long the recovery will take.              */
//  TODO add metrics
//  TODO: consider adding a shuffle grouping after the spout to avoid so much routing of the args/return-info all over the place   (at least until its possible to just pack bolt logic into the spout itself) 
//  This is a hack for non-ras scheduler topology and worker resources 
// TODO move this logic to the model class 
// For some reason on the new code if ackers is null we get 0??? 
// This is a bit of a hack.  If it is a list, then it is [component, stream]  we want to format this as component:stream 
// TODO: DRY this code up with what's in ChainedAggregatorImpl 
//  should we fail the tuple or kill the worker? 
// TODO streams should be uniquely identifiable 
// TODO I would love to standardize this... 
//  FIXME: This class can be moved to webapp once UI porting is done. 
//  This isn't strictly necessary, but it doesn't hurt and ensures that the machine stays up   to date even if callbacks don't all work exactly right 
//  To work around it we should get it an alternative (working) way. 
//  read s1 last....  This should cause s2 to be evicted on next put 
//  TODO more collection content type checking 
//  FIXME: We're using default config since it cannot be serialized   We still needs to provide some options externally 
// TODO this can only ever be null if someone is doing something odd with mocking   We should really fix the mocking and remove this 
// TODO put in some better exception mapping...  TODO move populateContext to a filter... 
//  should we fail the batch or kill the worker? 
// Getting the exact response code is a bit more complex.  TODO should use a better client 
//  TODO: de-duplicate this logic with the code in nimbus
/*      * TODO: should worker even take the topologyId as input? this should be deducible from cluster state (by searching through assignments)     * what about if there's inconsistency in assignments? -> but nimbus should guarantee this consistency.     *     * @param conf           - Storm configuration     * @param context        -     * @param topologyId     - topology id     * @param assignmentId   - assignment id     * @param supervisorPort - parent supervisor thrift server port     * @param port           - port on which the worker runs     * @param workerId       - worker id      */
// This is a bit ugly, but it shows how this would happen in a worker so we will use the same APIs 
//  FIXME: stores map (topoConf), topologyContext and expose these to derived classes 
// Original implementation doesn't actually check if delete succeeded or not. 
//  it doesn't seem like you should have to do this, but java serialization is wacky, and doesn't call the default constructor. 
// This is a bit ugly The JSON we are expecting should be in the form   {"component": {"resource": value, ...}, ...}   But because value is coming from JSON it is going to be a Number, and we want it to be a Double.   So the goal is to go through each entry and update it accordingly 
// We may (though unlikely) lose metering here if state transition is too frequent (less than a millisecond) 
//  do we want to throw an exception if path doesn't exist?? 
// TODO a better way to do this would be great. 
// TODO in the future this might be better in a common webapp location 
//  FIXME: it should be moved to storm-client when serialization-test.clj can be removed 
// TODO figure out how we want to deal with overrides. Users may want to add streams even when overriding other   properties. For now we just add them blindly which could lead to a potentially invalid topology. 
// More accurate that thread.sleep, but still not great 
//  todo, ignore the master batch coordinator ? 
// This test is rather ugly, but it is the only way to see if the error messages are working correctly. 
//  TODO: need to be able to replace existing fields with the function fields (like Cascading Fields.REPLACE) 
// TODO: wrap this to set the stream name 
// TODO: this isn't right... it's not in the map anymore 
// TODO: add a method for drpc stream, needs to know how to automatically do return results, etc   is it too expensive to do a batch per drpc request? 
// TODO: do the inner join incrementally, emitting the cross join with this tuple, against all other sides  TODO: only do cross join if at least one tuple in each side 
// TODO once everything is in java this should not be possible any more 
// heck for backward compatibility, we need to pass TOPOLOGY_AUTO_CREDENTIALS to hbase conf  the conf instance is instance of persistentMap so making a copy. 
//  TODO: handle regular rich spout without batches (need lots of updates to support this throughout)
// This is a hack to allow ZooKeeperMain to be called by this command. 
// Perhaps there is a better way to do this??? 
//  TODO: can optimize further by only querying backing map for keys not in the cache 
// TODO this is causing issues... 
/*  TODO: need to invoke a hook provided by the topology, giving it a chance to create user resources.         * this would be part of the initialization hook         * need to separate workertopologycontext into WorkerContext and WorkerUserContext.         * actually just do it via interfaces. just need to make sure to hide setResource from tasks          */
/*  Not implemented  */
//  TODO: What would be a good test to ensure that RankableObjectWithFields is at least somewhat defensively copied? 
//  TODO: in the future, want a way to include this logic in the spout itself,   or make it unnecessary by having storm include metadata about which grouping a tuple   came from 
//  TODO: does this work well on windows? 
// Not enough guaranteed use the age of the topology instead.  TODO need a good way to only do this once... 
//  Use parseWithException instead of parse so we can capture deserialization errors in the log.   They are likely to be bugs in the spout code. 
// TODO: add logging that not all tuples were received
//  TODO: take away knowledge of storm's internals here 
// TODO we should handle '\n'. ref DelimitedRecordFormat 
/*  TODO: make sure test these two functions in manual tests  */
// TODO get this from type instead of hardcoding to Nimbus.  establish client-server transport via plugin 
// There is a bug in some versions that returns 0 for the uptime. 
//  need to set active false before calling onKill() - current implementation does not return. 
// this is kind of unnecessary for clojure 
//  TODO this class is reserved for supporting messages with different schemas.   current only one schema in the cache 
//  -- bad files dir config 
//  FIXME: we can filter by listKeys() with local blobstore when STORM-1986 is going to be resolved   as a workaround, we call getBlobMeta() for all keys 
// 1 min.  This really means something is wrong.  Even on a very slow node 
// TODO check for null grouping args 
// TODO perhaps we can adjust the frequency later... 
// re-establish connection to eventhub servers using the right offset  TBD: might be optimized with cache. 
// TODO Handle the case where there may be no schema 
// It seems safer not to follow symlinks, since we don't expect them here 
//  this is because json doesn't allow numbers as keys...   TODO: replace json with a better form of encoding 
//  Todo: optimize this computation... perhaps inner loop can be outside to avoid rescanning tuples 
// Likely it is because of a bug, so try to get it another way 
//  while disabling we retain the sampling pct. 
//  hearbeat upon it 
//  REQUESTED_MEMONHEAP 
//  test environment variable substitution 
//  ASSIGNMENTS 
// Verify that all messages are emitted, ack all the messages 
//  EXECUTION_COMMAND 
//  boolean to track deactivated state 
/*              * track punctuation in non-batch mode so that the             * punctuation is acked after all the processors have emitted the punctuation downstream.              */
//  local executors and localTaskIds running in this worker 
//  only emit if we have declared fields. 
//  all types of files included 
// We want to capture the full time range, so the target size is as   if we had one bucket less, then we do 
// use batch size that matches the default credit size 
//  use the local setting for the login config rather than the topology's 
//  Get the nimbodes with the latest version 
//  destination taskId 
//  component 
// Fail the last emitted tuple and verify that the spout won't retry it because it's above the emit limit. 
//  If we generate a null response, then authentication has completed   (if not, warn), and return without sending a response back to the   server. 
// supervisor assignment id/supervisor id 
//  the WindowManager scan loop early. 
//  failed to send the JMS message, fail the tuple fast
// Set the first assignment 
// com.mysql.jdbc.jdbc2.optional.MysqlDataSource  jdbc:mysql://localhost/test  root 
/*                                                *  Update the count in the state. Here the first argument 0L is the initial value for the                                               *  count and                                               *  the second argument is a function that increments the count for each value received.                                                */
//  test write 
//  KEY 
//  TOPOLOGY_ID 
// Start emitting right now 
// format of date in worker logs 
/*  Map  */
//  use different blobstore dir so it doesn't conflict with other test 
//  timestamp either in milliseconds or seconds at which this metric is occurred. 
//  NO-OP 
// Update the current count for this object 
//  Ranked fourth since rack-3 has alot of memory but not cpu 
/*      * Each ControlMessage is encoded as:     *  code (<0) ... short(2)     * Each TaskMessage is encoded as:     *  task (>=0) ... short(2)     *  len ... int(4)     *  payload ... byte[]     *      */
//  This call returns immediately 
//  For testing 
//  Test2: test when no more workers are available due to topology worker max heap size limit but there is memory is still available   wordSpout2 is going to contain 5 executors that needs scheduling. Each of those executors has a memory requirement of 128.0 MB   The cluster contains 4 free WorkerSlots. For this topolology each worker is limited to a max heap size of 128.0   Thus, one executor not going to be able to get scheduled thus failing the scheduling of this topology and no executors of this 
//  set clean time really high so doesn't kick in 
// Set it for nimbus only 
//  1s   no lag 
//  creates array[largestId+1] filled with nulls 
//         builder.setStateSpout("stateSpout", mock(IRichStateSpout.class), 0);      } 
//  close the input stream 
//  Dependency uploading only makes sense for distributed mode 
//  read 2nd line and ACK 
//  USED_CPU 
//  2) If no failed tuples to be retried, then send tuples from hdfs 
//  it happens when part is just '*' rather than denoting some directory 
// Next verify that the blob store is correct before we start it up. 
//  The connection is ready once the channel is active.   See:   - http://netty.io/wiki/new-and-noteworthy-in-4.0.html#wiki-h4-19 
//  test for keylist to download 
//  tuples should be available in store before they are added to window manager 
//  memory required by topo-t0   memory required by topo-t1 
//  This is a DSL (YAML, etc.) topology... 
//  check for required fields   check for sub-struct validity 
//  periodically calls refreshLoad in 1 sec to simulate worker load update timer 
//  [remoteTaskId] -> JCQueue. Some entries maybe null (if no emits to those tasksIds from this worker)
//  Encoders 
//  -- commit frequency - count 
// check if iterable 
//  set acl to below so that it can be shared by other users as well, but allows only read 
// Emit any remaining messages 
// If AutoHDFS is specified, do not attempt to login using keytabs, only kept for backward compatibility. 
//  ======== Ack ======= 
//  task ids should be pulled first 
//  referenced from a metric. 
//  PULSE 
//  add reference to one and then remove reference again so it has newer timestamp 
//  writes multiple metric values into the database as a batch operation.  The tree map keeps the keys sorted 
//  Get first and last block times for multiple runs and strategies 
// The worker is up and running check for profiling requests 
//  if the other config does not have it set. 
//  MEM_OFF_HEAP 
//  CAPACITY 
//  new line is at beginning of each line (instead of end) for better recovery from 
//  Exceptions are captured and thrown at the end of the batch by the executor 
//  This thread will send out messages destined for remote tasks (on other workers) 
// do another conversion (lets just make this all common) 
//  Store the time at which the query started executing. The SQL   standard says that functions such as CURRENT_TIMESTAMP return the   same value throughout the query. 
//  Maintain backward compatibility for 0.10 
//  checkpoint the state every 5 seconds 
// We cannot launch the container yet the resources may still be updating 
// Use a bootstrap tuple to wait for topology to be running 
//  AZE 
// Spout - 500 MB, 50% CPU, 0 GPU  bolt-1 - 500 MB, 50% CPU, 0 GPU  bolt-3 500 MB, 50% cpu, 2 GPU  Total 1500 MB, 150% CPU, 2 GPU -> this node has 0 MB, 0% CPU, 0 GPU left 
/*                         This case will arise in case of non-sequential offset being processed.                        So, if the topic doesn't contain offset = nextCommitOffset (possible                        if the topic is compacted or deleted), the consumer should jump to                        the next logical point in the topic. Next logical offset should be the                        first element after nextCommitOffset in the ascending ordered emitted set.                      */
// A topology can set resources in terms of CPU and Memory for each component 
//  add with past ts 
//  When moving to pacemaker workerbeats can be leaked too... 
//  5 seconds 
//  race condition with another thread, and we lost. try again 
// The producers are shut down first, so keep going until the queue is empty. 
//  we don't allow any cluster wide configuration 
//  evicted metadata needs to be stored immediately.  Metadata lookups count on it being in the cache 
//  will have the correct settings that cannot be overriden by the submitter. 
//  A map describing which topologies are using which slots on this node.  The format of the map is the following: 
//  track how many times each supervisor slot has been listed as bad 
//  when they are successfully processed. 
// all sent events are stored in pending 
//  topo2 has 4 large tasks 
// Ignore any exceptions we might be doing a test for authentication 
// First failure is the initial delay, so not interesting 
/*      * Bolt-specific configuration for windowed bolts to specify the window length in time duration.      */
//  RESOURCES_MAP 
//  contains one Tuple per Stream being joined   refs to fields that will be part of output fields 
// remote subject 
/*          *  By default parser uses [ ] for quoting identifiers. Switching to DQID (double quoted identifiers)         *  is needed for array and map access (m['x'] = 1 or arr[2] = 10 etc) to work.          */
// The second tuple should NOT be ack'd because the batch should be cleared and this will be
// We need to adjust the throughput accordingly (so that it stays the same in aggregate) 
/*      * test scheduling does not cause negative resources      */
//  Return now *WITHOUT* sending upstream here, since client   not authorized. 
//  reader 
//  Ranked first since rack-0 has the most balanced set of resources 
//  Configs 
// Waiting to be returned 
//  validate path defined 
//  Blostore launch command with topology blobstore map   Here we are giving it a local name so that we can read from the file   bin/storm jar examples/storm-starter/storm-starter-topologies-0.11.0-SNAPSHOT.jar   org.apache.storm.starter.BlobStoreAPIWordCountTopology bl -c 
//  for some odd reason they are leaked.
// get topology constraints 
// Ensure the second file has a later modified timestamp, as the spout should pick the first file first. 
//  launch heartbeat threads immediately so that slow-loading tasks don't cause the worker to timeout 
//  Tuple contains String Object in JSON format 
/*      * returns list of Tuple3 (key, val from table, val from row)      */
// bolt-3 - 500 MB, 50% CPU, 2 GPU  Total 500 MB, 50% CPU, 2 - GPU -> this node has 1000 MB, 100% cpu, 0 GPU left 
// The priority of a topology describes the importance of the topology in decreasing importance   starting from 0 (i.e. 0 is the highest priority and the priority importance decreases as the priority number increases).  Recommended range of 0-29 but no hard limit set.   If there are not enough resources in a cluster the priority in combination with how far over a guarantees 
// The second tuple is used to wait for the spout to rotate its pending map 
//  StatefulBoltExecutor does the actual ack when the state is saved. 
//  unknown version should be treated as "current version", which supports RPC heartbeat 
// There are a few possible files that we would want to clean up  baseDir + "/" + "_tmp_" + baseName  baseDir + "/" + "_tmp_" + baseName + ".current"  baseDir + "/" + baseName.<VERSION>  baseDir + "/" + baseName.current  baseDir + "/" + baseName.version  In general we always want to delete the _tmp_ files if they are there. 
// Check for : in case someone called their user "User Name"  This line contains the user name for the pid we're looking up  Example line: "User Name:    exampleDomain\exampleUser" 
//  need to reverse the order of elements in PQ to delete files from oldest to newest
// Ignored the file did not match
// cleanup internal assignments 
//  JMS options 
//  has been acked 
// check if exec satisfy spread 
// if no event sent, no checkpoint shall be created 
//  dynamic fields 
//  wait for more ACKs before proceeding 
// get query filter 
//  allocate another array to be switched 
// JUMP 
// verify that Offset 10 was last committed offset, since this is the offset the spout should resume at 
//   "modprinc -maxlife 3mins <principal>" in kadmin. 
//  acquire lock on file1 
//  now, selecting from the full set should cause the fourth task to be chosen. 
//  NODE 
//  PREV_STATUS 
//  configs 
// The new partition should be discovered and the message should be emitted
// error message returned is something went wrong
//  The failed executions should not cause rotations and any new files 
//  Producers 
//  only log/set when there's been a change to the assignment 
//  SingleRel 
//  only if some data has arrived on each input stream 
//  if an ack is for a message that failed once at least and was re-emitted then the record itself will be in   failedAndFetchedRecords. We use that to   determine if the FailedMessageRetryHandler needs to be told about it and then remove the record itself to 
//  metric name 
// bolt-1 - 500 MB, 50% CPU, 0 GPU  bolt-2 - 500 MB, 50% CPU, 1 GPU  bolt-2 - 500 MB, 50% CPU, 1 GPU  Total 1500 MB, 150% CPU, 2 GPU -> this node has 0 MB, 0% CPU, 0 GPU left 
//  COMPONENT_EXECUTORS 
//  The manually set STORM_WORKER_CGROUP_CPU_LIMIT config on supervisor will overwrite resources assigned by 
//  When this master is not leader and get heartbeats report from supervisor/node, just ignore it. 
//  try locking another file2 at the same time 
// Each time we try to schedule a new component simulate taking 1 second longer 
// Now we need to build the array 
// File 1 should be moved to archive
// 1) create a couple files to consume 
//  REQUESTED_REGULAR_OFF_HEAP_MEMORY 
//  Current state 
//  if # of workers requested is less than we took   then we know some workers we track died, since we have more workers than we are supposed to have 
// returns void if the event should continue, false if the event should not be done 
// user jerry submits another topology into a full cluster 
//  Doing nothing (probably due to an oom issue) and hoping Utils.handleUncaughtException will handle it 
//  should remain unchanged @ 2 
// Just to be sure 
//  LAUNCH_TIME_SECS 
//  there won't be a BatchInfo for the success stream 
//  Strategy to determine the fetch offset of the first realized by the spout upon activation 
// start accepting requests 
//  This could fail if a blob gets deleted by mistake.  Don't crash nimbus.
//  STATUS 
//  Create a default pipeline implementation. 
//  public Object execute(Context) 
// Scheduling changed after we killed all of the processes 
//  this finds all active topologies blob keys from all local topology blob keys 
// Sasl transport 
// For cgroups no limit is max long. 
// Not available 
//  scheduled 
//  config main.methods
// if any error/exception thrown, fetch it from zookeeper 
//  initial state 
//  error/exception thrown, just skip 
//  METRIC_NAME 
// Verify the signature... 
// Empty 
//  set really small so will do cleanup 
//  Look for deleted log timeouts 
//  [ authz, authn, password ] 
//  ASSIGNED_REGULAR_ON_HEAP_MEMORY 
//  But that is only if there is a bug in one of the password providers.
//  NUM_USED_WORKERS 
//  bolt3 should also receive from checkpoint streams of bolt1, bolt2 
// It already points where we want it to 
//  to really make this work well. 
// Mapping of key->upstreamBolt->count 
//  Equivalent create command on command line 
// topoConf.put(Config.TOPOLOGY_STATE_PROVIDER_CONFIG, "{\"keyClass\":\"String\"}"); 
//  1) If there are any abandoned files, pick oldest one 
// assigning internally 
// topology priority 
//  TOTAL_SHARED_OFF_HEAP 
//  retry till at least 1 element is drained 
//  schedule mid block (10% - 90%) 
//  key fields 
// NO OP 
// request from authorized hosts and group should be allowed. 
// Now we need to free up some resources... 
//  All internal state except for the current buckets are 
// update the isLeader field for each nimbus summary 
// has the streamid/outputFields of the node it's doing the partitioning on 
//  https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.5 
//  let's checkpoint so that we can get the last checkpoint when restarting. 
// allow requesting slots number bigger than available slots 
//  activation expired list should contain even the ones expired due to EXPIRE_EVENTS_THRESHOLD 
//  Not sure what could cause this.
//  SPECIFIC 
//  flag indicating HiveWriter was closed 
//  a stream of (number, cube) pairs 
//  default 128 K 
//  run default scheduler on non-isolated topologies 
//  baseDir/supervisor/usercache/user1 
//  list all daemon logs 
//  we already checked this 
//  add any autocredential expiry metrics from the worker 
//  this ensures that list of values is always written the same way, regardless   of whether it's a java collection or one of clojure's persistent collections    (which have different serializers)   Doing this lets us deserialize as ArrayList and avoid writing the class here 
//  1) create couple of input files to read 
// We follow the model of service loaders (Even though it is not a service). 
//  should be at 6   double ack on same msg   should still be at 6 
//  do not process events beyond current ts 
// generate some supervisors that are depleted of one resource 
//  2) run spout 
//  Await all results 
//  worker slot which was never back to normal in tolerance period will be removed from cache 
//  ATTENTION: whb can be null
// Waiting for this also ensures that the first tuple gets failed if reset-timeout doesn't work 
//  propagate interrupt 
//  save the metadata for all types of strings it matches 
// populate node to component Assignments 
//  for serialization 
//  no wildcard, directory 
/*      * Just output the word value with a count of 1.      */
//  send a watermark event, which should trigger three windows. 
//  Tests for case when subject == null (security turned off) and 
//  Both things are not expected and should not happen.
// thenReturn always returns the same object, which is already consumed by the time User3 tries to getBlob! 
//  process all metadata 
//  Update the key.current symlink. First create tmp symlink and do 
// arbitrary message to be returned when scheduling is done 
//  If resource is also present in resources map will overwrite the above 
// heartbeats "stats" 
//  can be 'local' or 'shuffle' 
// request to impersonate users from unauthroized groups should be rejected.
// Lets reread the children in STORMS as the source of truth and see if a new one was created in the background 
/*  Naive implementation, but it might be good enough  */
// This is a special case where the jar was not uploaded so we will not download it (it is already on the classpath) 
//  DEPENDENCY_ARTIFACTS 
/*                 * update the word counts in the state.                * Here the first argument 0L is the initial value for the state                * and the second argument is a function that adds the count to the current value in the state.                 */
// Parallelism is same 
// No need to search more it is not going to help.
//  Would only happen at 2 PB so we are OK for now 
//  Test for replication. 
//  DIRECT 
//  -- lock dir config 
// Once every UPDATE_RATE_PERIOD_NS 
// Kill the container and restart it 
// avoid system dependent things 
// For this part of the test we interleve the differnt rotation types. 
//  guarantees a list of unused string Ids exists.  Once the list is empty, creates a new list 
//  reached EOF, didnt read anything 
//  Set the shard iterator for last fetched sequence number to start from correct position in shard 
/*          * Don't wait for Timetrigger to fire since this could lead to timing issues in unit tests.         * Set it to a large value and trigger manually.          */
//  bolt that subscribes to the intermediate bolt, and publishes to a JMS Topic 
// Just go on and try to delte the others 
/*  tuple payload serializer is specified via configuration  */
//  also called from processLogConfigChange 
//  load the first part of entries 
//  If future got interrupted exception, we want to interrupt parent thread itself. 
// Ignore changes to scheduling while downloading the topology blobs   We don't support canceling the download through the future yet,   because pending blobs may be shared by multiple workers and cancel it   may lead to race condition   To keep everything in sync, just wait for all workers 
//  DOUBLE_ARG 
// Node ID and supervisor ID are the same. 
//  3) read 6th line and see if another log entry was made 
//  5) read initial lines in file, then check if lock exists 
// Now lets get the creds for the topos so we can verify those as well. 
//  used to recognize the pattern of active log files, we may remove the "current" from this list 
//  baseDir/supervisor/usercache/user1/filecache/archives 
/*  * This class consists exclusively of static factory main.methods that create instances that are essential to work with the *  Jpmml library.  */
//  11 seconds passed by, not timing out 
//  it's null if one of:     a) a later transaction batch was emitted before this, so we should skip this batch     b) if didn't exist and was created (in which case the StateInitializer was invoked and        it was emitted
//  end executor summary 
//  Test whether the integer is a power of 2. 
// Release things that don't need to wait for us to finish downloading. 
//  RocksDB should insert in sorted key order 
//  Updating file few times every 5 seconds 
// Dropping the parallelism of the bolts to 3 instead of 11 so we can find a solution in a reasonable amount of work when backtracking. 
// Emit and ack the rest 
//  Configs for memory enforcement done by the supervisor (not cgroups directly) 
//  ICredentialsRenewer 
//  Only need to keep track of failed tuples if commits to Kafka are controlled by   tuple acks, which happens only for at-least-once processing semantics 
// TODO: file rotation 
// Copy it in case we want to modify it 
//  this is to read default value for other configurations 
//  This should trigger the scan to find   the next aligned window end ts, but not produce any activations 
//  ARGS_LIST 
//  consume both files 
//  JMS Topic spout 
// set up nimbus-info to zk 
// What we want... 
//  Gets Nimbus Subject with NimbusPrincipal set on it 
//  include partition id in the file name so that index for different partitions are independent. 
//  go with known best input 
// Too fast not reported 
//  if a record is returned put the sequence number in the emittedPerShard to tie back with ack or fail 
//  1 for point1, 4 for point2 
//  Null: log any unhandled errors to stderr. 
//  TARGET_LOG_LEVEL 
// Creates a MongoClient described by a URI. 
// Empty (Still 500) 
// 1) Retire writers 
//  IPrincipalToLocal  
//  SentenceSpout --> MyBolt 
//  if the Resource Aware Scheduler is used, 
//  since user derek has exceeded his resource guarantee while user jerry has not topo-5 or topo-4 could be evicted because they have the same priority 
// class Offset 
// The connection is not sent unless a response is requested 
//  Now that the root is fine we can start to look at the other paths under it. 
//  Convenience data structure to speedup lookups 
//  Returns the recorded throughput since the last call to getCurrentThroughput()       or since this meter was instantiated if being called for fisrt time. 
// NOOP  We could add in configs through the web.xml if we wanted something stand alone here... 
//  Storm 
//  offset where processing will resume upon spout restart 
//  File offset and byte offset should always be zero when searching multiple workers (multiple ports). 
//  1 grab lock 
// Gets a collection. 
// Blobs are not supported in local mode.  Return nothing 
// Filtered negative value 
// NOOP, we don't actually want to change log levels for tests 
//  transfer encoding should be set as jersey sets it on by default. 
//  REQUESTED_SHARED_ON_HEAP_MEMORY 
//  agg spout stats 
//  Keep this constructor for backward compatibility 
// NOOP on purpose 
//  3) deleting closed file  - should return true 
//  stream name is specified 
//  validate 4 metrics (aggregations) found for m4 for all agglevels when searching by port 
// assume it is a topology id 
//  With no principals in the subject ACL should always be set to WORLD_EVERYTHING 
//  ERROR_TIME_SECS 
//  it shouldn't be happen
//  TOPO_IDS 
// Generously adapted from:  https://github.com/kijiproject/kiji-express/blob/master/kiji-express/src/main/scala/org/kiji/express/flow/framework/serialization   /AvroSerializer.scala  Which has as an ASL2.0 license 
//  just ignore the exception 
//  Receives msgs from remote workers and feeds them to local executors. If any receiving local executor is under Back Pressure, 
//  to avoid reordering of emits, stop at first failure 
//  If we didn't take GPUs into account everything would fit under a single slot   But because there is only 1 GPU per node, and each of the 2 spouts needs a GPU   It has to be scheduled on at least 2 nodes, and hence 2 slots.   Because of this all of the bolts will be scheduled on a single slot with one of   the spouts and the other spout is on its own slot.  So everything that can be shared is   shared. 
//         producerFwdConsumer();      // -- measurement 5 
//  we try uploading second one and it should be failed throwing RuntimeException 
// start the thread pool 
// The spout must be able to reemit all retriable tuples, even if the maxPollRecords is set to a low value compared to maxUncommittedOffsets. 
//  {stream id -> metric -> value}, note that sid->out-stats may contain both long and double values 
// All the events should be expired when the next watermark is received 
//  Test for subject with no principals and acls set to DEFAULT 
//  Copy the 2nd half of the buffer to the first half. 
// Remove something randomly... 
// We found the topology, lets get the conf 
//  since we made sys components visible, the component map has all system components 
// noop 
/*      * Computes tumbling window average      */
//  check lock file contents 
//  QueryPlanner on Streams mode configures the topology with compiled classes,   so we need to add new classes into topology jar   Topology will be serialized and sent to Nimbus, and deserialized and executed in workers. 
// All done we can launch the worker now 
// login and also update the subject field of this instance to 
//  map from stream name to batch id 
//  Create symbolic link relative to tar parent dir 
// Set the metrics sample rate to 1 to force update the executor stats every time something happens  This is necessary because getAllTimeEmittedCount relies on the executor emit stats to be accurate 
// check if node is alive 
// Even if the topology is not valid we still need to remap it all 
//  ASSIGNED_ON_HEAP_MEMORY 
// Translating the name (this call) happens in a different callback from validating   the user name and password. This has to be stateless though, so we cannot save   the password provider away to be sure we got the same one that validated the password.   If the password providers are written correctly this should never happen,   because if they cannot read the name they would return a null.   But on the off chance that something goes wrong with the translation because of a mismatch   we try to skip the bad one. 
// Now rebalance and add a new partition 
// shutdown server process since we could not handle Thrift requests any more 
//  when worker bootup, worker will start to setup initial connections to   other workers. When all connection is ready, we will count down this latch   and spout and bolt will be activated, assuming the topology is not deactivated. 
//  json parsing fail -> error received
//  MEMORY_USAGE 
//  object handling interaction with kinesis 
//  spout stats 
//  Spout implementation 
//  ======== emit  ========= 
//  BOLT_OBJECT 
//  Use custom class loader set in testing environment 
//  topo1 has one single huge task that can not be handled by the small-super
// user derek submits another topology into a full cluster  topo6 should not be able to scheduled initially, but since topo6 has higher priority than topo5  topo5 will be evicted so that topo6 can be scheduled 
//  Throws IOExceptions for 3rd & 4th call to next(), succeeds on 5th, thereafter
//  remove reverse lookup from map 
// Now rebalance 
//  scans the database to look for a metadata string and returns the metadata info 
//  NUM_TASKS 
// deprecated in favor of non-threaded RotatingMap 
//  we trust that the file exists 
// this test works because mocking a spout splits up the tuples evenly among the tasks 
//  Mapping: from storm tuple -> rocketmq Message 
//  USED_MEM 
//  =====================================================================================   key transformers   ===================================================================================== 
//   If any events are scheduled, sleep until   event generation. If any recurring events   are scheduled then we will always go   through this branch, sleeping only the   exact necessary amount of time. We give   an upper bound, e.g. 1000 millis, to the   sleeping time, to limit the response time   for detecting any new event within 1 secs. 
// We don't want to override the client if there is a thrift server up and running, or we would not test any   Of the actual thrift code 
//  each drpc request is always a single attempt 
//  add to the callers cache.  We can't add it to the stringMetadataCache, since that could cause an eviction   database write, which we want to only occur from the inserting DB thread.
// This partition is new, and should start at the committed offset 
//  assuming sideFields are preserving its order 
// Blocking call 
//  above gives an extra empty string at the end. below   removes that
// remote address 
//  So far it matches.  Keep going... 
// So if we fail we are forced to try again 
//  attempt to find the string in the database 
//  Setup a test message 
// Prior to the org.apache change 
//  delete one worker of r000s000 (failed) from topo1 assignment to enable actual schedule for testing 
//  actual ret is Map<String, Map<String, Long/Double>> 
//  time to sleep between retries in milliseconds 
// Ranked fifth since rack-2 has not cpu resources 
//  time of last flush on this writer 
// This is an attempt to give all of the streams an equal opportunity to emit something. 
//  this is the end key for whole scan 
//  Transfers messages destined to other workers 
//  Make sure that we have received at least an integer (length) 
//  NAME 
//  test write again 
// Will be closed automatically when shutting down the DFS cluster 
//  should remove key1 
//  if Batch spout then id contains txId 
//  Called with a bad port (not in the config) No searching should be done. 
//  for partitionpersist 
// The dir is empty, so try to delete it, may fail, but that is OK
//  Empty class 
//  seems this can fail by returning false or throwing exception   convert false ret value to exception 
//  found the next offset to commit 
// This is only used for logging/metrics. Don't crash the process over it.
//  counter for spout wait strategy   counter for back pressure wait strategy 
// Now lets create a token and verify that we can connect... 
//  TODO log 
// this validates the structure of the topology 
//  note that port1Dir is active worker containing active logs 
//  we may output many tuples for a given input tuple 
//  Now do multiple 
/*                      * If the user wants to explicitly set an auto offset reset policy, we should respect it, but when the spout is                     * configured for at-least-once processing we should default to seeking to the earliest offset in case there's an offset                     * out of range error, rather than seeking to the latest (Kafka's default). This type of error will typically happen                     * when the consumer requests an offset that was deleted.                      */
//  hash 
//  Bookkeeping 
//  Instances of this type are sent from NettyWorker to upstream WorkerTransfer to indicate BackPressure situation 
//  assumption: there're no put and delete for same target in parameter list 
// by default no need to do this as a different user 
//  set the default heap memory size for supervisor-test 
//  COMMON 
// This is the first time so initialize the resources. 
// We need to do enforcement on a topology level, not a single worker level...   Because in for cgroups each page in shared memory goes to the worker that touched it   first. We may need to make this more plugable in the future and let the resource   isolation manager tell us what to do 
/*  headers  */
// Update to set the second assignment 
//  create reader and do some checks 
//  PRINCIPAL 
//  Non-static impl main.methods exist for mocking purposes.
// topo-2 evicted since user bobby don't have any resource guarantees and topo-2 is the next lowest priority for user bobby 
//  ==================   Evaluator   ================== 
//  REPLICATION_COUNT 
// 3 delete file and retry creation 
//  given/when 
//  If the latch is not started yet, start it 
//  Offsets emitted are 0,1,2,3,4,<void>,8,9 
//  but topo-4 was submitted earlier thus we choose that one to evict (somewhat arbitrary) 
/* DON'T include sys */
//  storm topology name 
//  defaults to INFO level when the logger isn't found previously 
//  no specific reason to mock... this is one of easiest ways to make dummy instance 
//  [[remoteTaskId] -> true/false : indicates if remote task is under BP. 
// use the default server port. 
// play and ack 1 tuple 
//  emit the averages downstream 
// for reporting errors
//  common aggregate 
//  Bind and start to accept incoming connections. 
//  package access for unit test 
//  This is a test where we are configured to point right at a single artifact 
//  required   required   required   required   required   required   required   required   required 
//  assume filter choices have been made and since no selection was made, all levels are valid 
// open Sasl transport with the login credential 
//  3 let go first lock 
// 10 min values 
//  validate search by port 
// merge with existing assignments 
//  initialize state for batch 
// Offset 0 to maxUncommittedOffsets - 2 are pending, maxUncommittedOffsets - 1 is failed but not retriable 
// null isTimedOut means worker never reported any heartbeat 
//  rw-r--r-- 
// One time scheduling. 
//  Regardless of TICKET_RENEW_WINDOW setting above and the ticket expiry time,   thread will not sleep between refresh attempts any less than 1 minute (60*1000 milliseconds = 1 minute). 
// and to wait for the message to get through to the spout (acks use the same path as timeout resets) 
// subprocesses must send their pid first thing 
//  null for __system bolt 
//  assume that Get doesn't have any families defined. this is for not digging deeply...
// The best way to force backtracking is to change the heuristic so the components are reversed, so it is hard   to find an answer. 
// in local mode there is no jar 
//  Extract spout resource info 
//  Class path entries that are neither directories nor archives (.zip or JAR files)   nor the asterisk (*) wildcard character are ignored.
//  TOPOLOGY_VERSION 
//  Convert sources to a JSON serializable format 
//  CPU_GUARANTEE 
//  Toplogy:  WorGenSpout -> FieldsGrouping -> CountBolt 
//  Be careful about adding additional tests as the dfscluster will be shared 
//  1) Start metastore 
// get transaction id  if it already exists and attempt id is greater than the attempt there 
//  TYPE 
// Set up committed so it looks like some messages have been committed on each partition 
//  should not be returned since this executor is not part of the topology's assignment
//  required   required   required   required   required   required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
// JPROFILE START 
// 1) Find retirement candidates 
//  Committed offset, i.e. the offset where processing will resume upon spout restart. Initially it is set to fetchOffset. 
//  allow freq_sec to expire 
//  schedule topo2 
//  If FlatMapFunction is aware of prepare, let it handle preparation 
//  Advance time and then trigger call to kafka consumer commit 
// Verify that the tuple is not emitted again 
//  use this instead of storm's built in one so that we can specify a singleemitbatchtopartition   without knowledge of storm's internals 
//  2 -  Setup HFS Bolt   -------- 
//  we wait for 3 seconds 
//  put global stream id for spouts 
//  by using a monotonically increasing attempt id, downstream tasks   can be memory efficient by clearing out state for old attempts   as soon as they see a higher attempt id for a transaction 
//  how to partition for second stage of aggregation 
//  required   optional   optional   optional   optional 
//  Try the loader plugin, if configured 
//  bail out 
// The user is not set so lets see what the request context is. 
//  i being the position in the fields of this seq, the remainder of the seq is the size 
// not_jump (closed in strict mode) 
/*      * Asserts that commitSync has been called once,     * that there are only commits on one topic,     * and that the committed offset covers messageCount messages      */
// Allow poll if the partition is not at the maxUncommittedOffsets limit 
// When is this ever null?
//  read error and input streams as this would free up the buffers 
// Test for replication with NIMBUS as user 
// 3 
/*      The below field declarations are also used in common.clj to define the event logger output fields       */
//  RESULT 
//  IAutoCredentials 
//  topology.getbolt (AKA sys tasks most specifically __acker tasks) 
//  WINDOW_TO_TRANSFERRED 
// There is at most one schedule per message id 
// Make sure that even though nextTuple() doesn't receive valid data,  the offset will be checkpointed after checkpointInterval seconds. 
//  retrieve any existing aggregation matching this one and update the values 
// private constructor 
//  taskIds first 
//  keeps track of in flight tuples) 
// logger.info("emit for partition " + partition.getId() + ", offset=" + offset); 
// 4 
// This is because the tests are checking that a hard cap of maxUncommittedOffsets + maxPollRecords - 1 uncommitted offsets exists 
// Good so far, check if we are in a CGroup 
//  supervisor which was never back to normal in tolerance period will be removed from cache 
//  maybe it needs a start phase (where it can do a retrieval, an update phase, and then a finish phase...?   shouldn't really be a one-at-a-time interface, since we have all the tuples already?   TOOD: used for the new values stream   the list is needed to be able to get reduceragg and combineragg persistentaggregate   for grouped streams working efficiently
//  in java classpath, '*' is expanded to all jar/JAR files in the directory 
// verify that offset 4 was committed for the given TopicPartition, since processing should resume at 4. 
//  could not lock, so try another file. 
//  use the hash index for prefix searches 
// 3, 4 compacted away 
// DONE 
// check if Map 
// Tuples may be acked/failed after the spout deactivates, so we have to be able to handle this too
//  zkHostString for Solr gettingstarted example 
/*  Not a Blocking call. If cannot emit, will add 'tuple' to 'pendingEmits' and return 'false'. 'pendingEmits' can be null  */
// Waiting to be fetched 
//  a stream of (number, square) pairs 
//  OUTPUT_FIELDS 
//  Timers 
// 5 
//  Do not use canonical file name here as we are using   symbolic links to read file data and performing atomic move   while updating files
//  Initiate connection to remote destination 
//  Assume that there could be a worker already on the node that is under the minWorkerCpu budget.   It's possible we could combine with it.  Let's disregard minWorkerCpu from the request   and validate that CPU as a rough fit. 
//  by generating a list of random numbers and removing the ones that already are in use. 
//  Client API to invoke blob store API functionality 
/*  DELETE EVERYTHING IN HDFS  */
//  do not send upstream to other handlers: no further action needs 
//  Test class to override the write directory 
// Retry once after a minute 
//  update this only after writing to hdfs 
//  Ranked second since rack-1 has a balanced set of resources but less than rack-0 
// 6 
//  comma separated list of topics   consumer group id for which the offset needs to be calculated   bootstrap brokers   security protocol to connect to kafka   properties file containing additional kafka consumer configs
//  configs - hdfs bolt 
//  aggregating metric did not exist, don't look for further ones with smaller timestamps 
//  rotate files when they reach 5MB 
// create an authentication callback handler 
//  port test-shuffle-load-even 
//  given 
//  consume file 1 
// (Requested + Assigned - Guaranteed)/Available 
// JPROFILE DUMP 
// 7 
// We are launching it now... 
// Tests that isScheduled, isReady and earliestRetriableOffsets are mutually consistent when there are multiple messages scheduled on a partition 
//  can be UpdateStateByKey or StateQuery processors 
//  These are mandatory parameters 
/*              * scan the entire window to handle out of order events in             * the case of time based windows.              */
//  Reader type config 
//  WINDOW_TO_STATS 
// find number of constraints per component  Key->Comp Value-># of constraints 
//  Set up the in-memory filesystem. 
// whether find all documents according to the query filter  updateBolt.withMany(true); 
//  Metrics are off, verify null 
// Check that if one message fails repeatedly, the retry cap limits how many times the message can be reemitted 
// mock state store and receiver 
//  test commit second creates properly 
//  out of order events should be processed upto the lag 
// Process has not terminated.  So check if it has completed  if not just destroy it. 
//  don't update unless there are tuples   this helps out with things like global partition persist, where multiple tasks may still   exist for this processor. Only want the global one to do anything 
// Compute the stats for the different input streams 
// Because the simple topology was scheduled first we want to be sure that it didn't put anything on   the GPU nodes. 
//  wait for available queue 
//  USER_NAME 
// Make sure we can store the worker tokens even if no creds are provided. 
//  this bolt does not emit anything 
// Should only happen on a badly configured system 
//  ignores invalid user/topo/key 
// are cleared 
//  return false to stop scan 
//   Submit topology to storm cluster 
// Prevent daemon log reads from pathing into worker logs 
//  TOTAL_NODE_SHARED 
// @{link org.apache.storm.trident.planner.Node} and several other trident classes inherit from DefaultResourceDeclarer   These classes are serialized out as part of the bolts and spouts of a topology, often for each bolt/spout in the topology.   The following are marked as transient because they are never used after the topology is created so keeping them around just wastes   space in the serialized topology
//  Read once. Since the first file is empty, the spout should continue with file 2 
// fail supervisor 
//  update blob interface 
//  default is aways "distributed" but here local cluster is being used. 
// This is cheating a bit since maxPollRecords would normally spread this across multiple polls 
//  Spy object that tries to mock the real object store 
// Should not show files outside log root. 
//  WORKER_HEARTBEATS 
//  Zero-out the 2nd half to prevent accidental matches. 
// Sleep a bit between emits to ensure that we don't reach the cap too quickly, since this spout is used to test time based windows 
//  transferred totals 
//  consume file 2 
//  test listkeys 
//  REQUESTED_TOTAL_MEMORY 
// Congested we contract much more quickly 
//  setup Broker 
//  bolt aggregate 
//  memory requirement is large enough so that two executors can not be fully assigned to one node 
//  
//  Now let's load w/o setting up gets and we should still get valid map back 
// DISABLE_LOGIN_CACHE indicates whether or not to use the LoginCache.  So we exclude it from the keyString 
//  if commands contains one or more null value, spout is compiled with lower version of storm-kafka-client 
//  NAMED_LOGGER_LEVEL 
//  NOTE: the queue has to be thread safe.
// Access foo to make it most recently used 
//  Other scenario it covers is when max-seq-number and nimbus seq number are equal. 
/*                 * compute the word counts in the last two second window                 */
//  Dispatches to the right join method (inner/left/right/outer) based on the joinInfo.joinType 
// partition info does not change in EventHub 
//  TIMESTAMP 
//  3 seconds 
//  add all fetched records to the set of failed records if they are present in failed set 
//  used by Stream#persistentAggregate 
//  Create multiple copies of a test topology 
// Now we need to map them all back again 
//  Now compile! 
//  EVENTLOG_PORT 
// sync zk assignments/id-info to local 
//  ensure 5 fields per tuple and no null fields 
// Offset 0 is committed, 1 to maxUncommittedOffsets-1 are failed, maxUncommittedOffsets to maxUncommittedOffsets + maxPollRecords-1 are emitted  Fail the last tuples so only offset 0 is not failed.  Advance time so the failed tuples become ready for retry, and check that the spout will emit retriable tuples  for all the failed tuples that are within maxUncommittedOffsets tuples of the committed offset 
//  assume key is the first field 
//  use redis for state persistence 
// Found an existing token and it is not going to expire any time soon, so don't bother adding in a new   token. 
//  spout notified that message returned by us for retrying was actually emitted. hence remove it from set and   wait for its ack or fail   but still keep it in counts map to retry again on failure or remove on ack 
//  current version supports RPC heartbeat 
//  certain states might only accept one-tuple keys - those should just throw an error  
// Set the size in case we are recovering an already downloaded object 
//  find primary key from constructor 
// sort executors based on component constraints 
//  should be at 5 
//  Config settings 
//  Test1: Launch topo 1-3 together, it should be able to use up either mem or cpu resource due to exact division 
//  File offset should be zero, since search-archived is false. 
//  ACCESS 
// The spout should now commit all the offsets, since all offsets are either acked or were missing when retrying 
//  Servers should be rotated **BEFORE** the old client is removed from clientForServer   or a race with getWriteClient() could cause it to be put back in the map. 
//  now test regular updateBlob 
//  (Re-)Cancel if current thread also interrupted 
// Also fail the last tuple from partition two. Since the failed tuple is beyond the maxUncommittedOffsets limit, it should not be retried until earlier messages are acked. 
// Test3: "Launch topo5 only, both mem and cpu should be exactly used up" 
//  => field name does not match static field, test if it matches dynamic field 
// sleep to prevent race conditions 
//  weak supervisor node 
// default is ordered. 
//  ensure the loggers are configured in the worker.xml before   trying to use them here 
//  release lock on file1 and check 
// Also put a generic resource with 0 value in the resources list, to verify that it doesn't affect the sorting 
//  OFF_HEAP_NODE 
//  It's OK if new-byte-offset is negative.   This is normal if we are out of bytes to read from a small file. 
//  don't bother blocking on a full queue, just drop metrics in case we can't keep up 
//  Update scenario 2 and 3 explain the code logic written here   especially when nimbus crashes and comes up after and before update 
//  0 means DEFAULT_EVENT_LOOP_THREADS   https://github.com/netty/netty/blob/netty-4.1.24.Final/transport/src/main/java/io/netty/channel/MultithreadEventLoopGroup.java#L40 
//  save the evicted key/value to the database immediately 
//  it should be org.apache.storm:flux-core:jar:1.0.0 and commons-cli:commons-cli:jar:1.2 
//  LOCAL_OR_SHUFFLE 
// The sym link we are pointing to 
// If CPU or memory is not set the values stored in topology.component.resources.onheap.memory.mb,   topology.component.resources.offheap.memory.mb and topology.component.cpu.pcore.percent 
//  initialize server-side SASL functionality, if we haven't yet   (in which case we are looking at the first SASL message from the   client). 
//  resend BP status, in case prev notification was missed or reordered 
//  slots will be null while supervisor has been removed from cached supervisors 
// we just return the first metric we meet 
//  boolean cache for local mode decision 
// When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We 
//  Kinesis Spout KinesisConfig object 
//  SPOUTS 
//  add new ports to cached supervisor.  We need a modifiable set to allow removing ports later. 
//  BINARY_ARG 
//  JMS Producer 
//  for a failed message add it to failed set if it will be retried, otherwise ack it; remove from emitted either way 
//  if there are no trigger values in earlier attempts or this is a new batch, emit pending triggers. 
//  when 
//  This code logic covers the update scenarios in 2 when the nimbus-1 goes down   before syncing the blob to nimbus-2 and an update happens.   If seq-num for nimbus-2 is 2 and max-seq-number is 3 then next sequence number is 4   (max-seq-number + 1). 
//  We should be done now... 
//  getters 
//  trigger the window 
//  offset 2   offset 6   offset 14   offset 18   offset 22   offset 26   offset 30   offset 34 
//  Non Blocking call. If cannot emit to destination immediately, such tuples will be added to `pendingEmits` argument 
//  renewal thread's main loop. if it exits from here, thread will exit. 
//  validate 
//  Shutdownable main.methods
// new supervisor to cache 
//  need to get ACL from meta 
//  Test4: Scheduling a new topology does not disturb other assignments unnecessarily 
//  WINDOW_TO_EMITTED 
//  The encryption key must be hexadecimal. 
// Execute and process latency... 
// There should now be maxUncommittedOffsets + maxPollRecords emitted in all. 
//  first just set the keys to null, then flag to remove them at beginning of next commit when we know the current and last value   are both null 
/*         Without fragmentation, the cluster would be able to schedule both topologies on each node. Let's call each node        with both topologies scheduled as 100% scheduled.        We schedule the cluster in 3 blocks of topologies, measuring the time to schedule the blocks. The first, middle        and last blocks attempt to schedule the following 0-10%, 10%-90%, 90%-100%. The last block has a number of        scheduling failures due to cluster fragmentation and its time is dominated by attempting to evict topologies.        Timing results for scheduling are noisy. As a result, we do multiple runs and use median values for FirstBlock        and LastBlock times. (somewhere a statistician is crying). The ratio of LastBlock / FirstBlock remains fairly constant.        TestLargeFragmentedClusterScheduling took 91118 ms        DefaultResourceAwareStrategy, FirstBlock 249.0, LastBlock 1734.0 ratio 6.963855421686747        GenericResourceAwareStrategy, FirstBlock 215.0, LastBlock 1673.0 ratio 7.78139534883721        ConstraintSolverStrategy, FirstBlock 279.0, LastBlock 2200.0 ratio 7.885304659498208        TestLargeFragmentedClusterScheduling took 98455 ms        DefaultResourceAwareStrategy, FirstBlock 266.0, LastBlock 1812.0 ratio 6.81203007518797        GenericResourceAwareStrategy, FirstBlock 235.0, LastBlock 1802.0 ratio 7.6680851063829785        ConstraintSolverStrategy, FirstBlock 304.0, LastBlock 2320.0 ratio 7.631578947368421        TestLargeFragmentedClusterScheduling took 97268 ms        DefaultResourceAwareStrategy, FirstBlock 251.0, LastBlock 1826.0 ratio 7.274900398406374        GenericResourceAwareStrategy, FirstBlock 220.0, LastBlock 1719.0 ratio 7.8136363636363635        ConstraintSolverStrategy, FirstBlock 296.0, LastBlock 2469.0 ratio 8.341216216216216        TestLargeFragmentedClusterScheduling took 97963 ms        DefaultResourceAwareStrategy, FirstBlock 249.0, LastBlock 1788.0 ratio 7.180722891566265        GenericResourceAwareStrategy, FirstBlock 240.0, LastBlock 1796.0 ratio 7.483333333333333        ConstraintSolverStrategy, FirstBlock 328.0, LastBlock 2544.0 ratio 7.7560975609756095        TestLargeFragmentedClusterScheduling took 93106 ms        DefaultResourceAwareStrategy, FirstBlock 258.0, LastBlock 1714.0 ratio 6.6434108527131785        GenericResourceAwareStrategy, FirstBlock 215.0, LastBlock 1692.0 ratio 7.869767441860465        ConstraintSolverStrategy, FirstBlock 309.0, LastBlock 2342.0 ratio 7.5792880258899675        Choose the median value of the values above        DefaultResourceAwareStrategy    6.96        GenericResourceAwareStrategy    7.78        ConstraintSolverStrategy        7.75         */
//  find the number of bytes with non-leading zeros 
//  check avoids multiple log msgs when in a idle loop 
//  schedule left over system tasks 
//  this doesn't follow symlinks, which is what we want 
//  Build put query 
//  window partitions 
//  3) Take ownership of stale lock 
//  We are capturing exceptions thrown in Blitzer's child threads into this data structure so that we can properly   pass/fail this test.  The reason is that Blitzer doesn't report exceptions, which is a known bug in Blitzer 
//  set size to cleanup another one 
// Should not seek on the paused partition 
// Verify digest is rejected... 
// System bolt is not a part of backpressure. 
//  check just the one port 
//  add with current ts 
//  Required 
//  value of the metric 
//  overflowQ size at the time the last BPStatus was sent 
//  tests download of topology blobs in local mode on a topology without resources folder 
//  Retry locking and verify 
//  default 
//  contents of the key starts with nimbus host port information 
//  context is not used by the default implementation, but is included   in the interface in case it is useful to subclasses 
//  if we get more than one stateful operation, we need to process the   current group so that we have one stateful operation per stateful bolt 
//  do nothing for conf now 
//  Add our messages and verify no metrics are recorded   
// Looks like usage might not be supported 
// Ensure Nimbus assigns topologies as quickly as possible 
//  Put in legacy values 
// Try to get the topology conf from nimbus, so we can reuse it. 
//  FUNC_ARGS 
//  use "|" instead of "," for field delimiter 
//  complexity is that of a linear scan on a TreeMap 
// nextOffset is the last offset from last batch + 1 
//  spout aggregate 
// use storm's zookeeper servers if not specified. 
//  clean up memory 
//  If configs are present in Generic map and legacy - the legacy values will be overwritten 
//  for each isolated topology:     compute even distribution of executors -> workers on the number of workers specified for the topology     compute distribution of workers to machines   determine host -> list of [slot, topology id, executors]   iterate through hosts and: a machine is good if:     1. only running workers from one isolated topology     2. all workers running on it match one of the distributions of executors for that topology     3. matches one of the # of workers   blacklist the good hosts and remove those workers from the list of need to be assigned workers   otherwise unassign all other workers for isolated topologies if assigned 
// Now check that the spout will not emit anything else since nothing has been committed 
// == 4 
//  NONE 
//  the metrics store is not critical to the operation of the cluster, allow Nimbus to come up 
// check the credential of our principal 
//  System bolt doesn't call reportError() 
//  join against diff stream compared to testThreeStreamLeftJoin_1 
//  parsed Topology definition 
//  compile parameters 
//  check for all ports 
// streaId indicates where tuple came from 
// For the node we don't know if we have another one unless we look at the contents 
//  WORKER_HOOKS 
//  validate search by stream id 
//  DEBUG_OPTIONS 
//  Storm support to launch workers of older version.   If the config of TOPOLOGY_SCHEDULER_STRATEGY comes from the older version, replace the package name. 
// Update the watermark to this timestamp 
//  create spouts 
//  need to wait until sasl channel is also ready 
//  perf critical path. would be nice to avoid iterator allocation here and below 
//  all column families 
// generate some that has alot of memory but little of cpu 
//  baseDir/supervisor/usercache/user1/ 
//  userSpout ==> jdbcBolt 
//  all things are from dependencies 
// do nothing 
//  test exist with non-existent key 
//  Testing whether acls are set to WORLD_EVERYTHING 
//  iterate through all executor heartbeats 
// There are free slots that we can take advantage of now. 
// need more data 
//  PORT 
// register call back for blob-store 
//  TASK_START 
//  Tests for case when subject != null (security turned on) and   acls for the blob are set to WORLD_EVERYTHING 
// RESTART 
//  Performs projection on the tuples based on 'projectionFields' 
// We are running so we should recover the blobs. 
// Something happened and we couldn't find the file, so ignore it for now. 
//  1 -  Setup Spout   -------- 
//  callback to caller 
//  Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because   the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have 
//  INPUTS 
//  3) make sure 6 lines (3 from each file) were read in all 
//  1st topology 
//  look for a public instance variable 
//  need one large set of all and then clean via LRU 
//  In all other cases check for the latest update sequence of the blob on the nimbus   and assign the appropriate number. Check if all are have same sequence number, 
//  local worker heartbeat can be null cause some error/exception 
//  because in local mode, its not a separate   process. supervisor will register it in this case   if ConfigUtils.isLocalMode(conf) returns false then it is in distributed mode. 
//  compare contents of files 
//  True if this OffsetManager has made at least one commit to Kafka 
//  minimum 0.x version of supporting STORM-2448 would be 0.10.3 
//  ASSIGNED_CPU 
//  Sleep before cleaning up files 
//  ==== 
//  state query is added to the existing stateful bolt 
//  attempts to lookup the unique Id for a string that may not exist yet.  Returns INVALID_METADATA_STRING_ID 
//  only used for TimedRotationPolicy 
//  BOLT 
// Error occurs, but assignment has not changed 
// We ensure there is only the default stream when configuring the spout, so it should be safe to ignore the parameter here. 
//  batch 1 is played with 25 tuples initially. 
// Save it to try again later... 
// This was 0 byte in test 
//  pendingPrepare has no entries 
//  Static State 
//  We got at least one GET_PULSE_RESPONSE message. 
//  if overrides are disabled, we won't replace anything that already exists 
// Fall through on purpose 
//  mock the supervisor r000s000 as a failed supervisor 
//  Restart topology with a different topology id 
//  set null and get the old value 
//  Test with integer value 
// Schedule nimbus code sync thread to sync code from other nimbuses. 
// Assignment has changed 
//  Pretend to be storm. 
//  required   required   required 
// Now verify that when it is switched we can recover 
//  doesn't manipulate tuples (lists of stuff) so that things like aggregating into   cassandra is cleaner (don't need lists everywhere, just store the single value there) 
//  wrapper to hold global and window averages 
//  Start at the part of the log file we are interested in. 
//  Windows should set this to false cause symlink in compressed file doesn't work properly. 
//  Set closing to true to prevent any further reconnection attempts. 
// There are some different levels of accuracy here, and we want to deal with all of them 
// The spout should have emitted the tuple, and must have committed it before emit 
// populate with existing assignments 
//  ignore ... if t is not a TupleImpl type .. faster than checking and then casting 
//  store can be null during testing when mocking utils. 
// Output the sum of all the known counts so for this key 
//  read the short field 
//  filesystem path to the resource 
//  TOPO_HISTORY 
//  should rewrite this to do a file move
//  tests by subclassing. 
//  Send response to client. 
//  get trigger values only if they have more than zero 
// All messages except the first acked message should have been emitted 
//  There's enough bytes in the buffer. Read it.   
//  get host -> all assignable worker slots for non-blacklisted machines (assigned or not assigned)   will then have a list of machines that need to be assigned (machine -> [topology, list of list of executors])   match each spec to a machine (who has the right number of workers), free everything else on that machine and assign those slots   (do one topology at a time)   blacklist all machines who had production slots defined   log isolated topologies who weren't able to get enough slots / machines   run default scheduler on isolated topologies that didn't have enough slots + non-isolated topologies on remaining machines   set blacklist to what it was initially 
// NOOP prepare should have already been called 
// Demonstrate that the spout doesn't ack pending tuples when skipping compacted tuples. The pending tuples should be allowed to finish normally. 
//  If we failed to get anything from Artifactory try to get it from our local cache 
//  Test1: When a worker fails, RAS does not alter existing assignments on healthy workers 
//  Keep track of how many times we see each taskId 
//  Make sure that we have received at least a short  
//  previous code used this method to generate the string, ensure the two match 
//  should be ack-ed once 
//  window length 
//  need to add an empty string else it is nto added as query param. 
// STORM-3141 regression test  Verify that remote worker can handle many tasks in one executor 
//  Attempt to schedule multiple copies of 2 different topologies (topo-t0 and topo-t1) in 3 blocks.   Without fragmentation it is possible to schedule all topologies, but fragmentation causes topologies to not   schedule for the last block. 
/*                 * a two seconds tumbling window                 */
// This verifies that partitions can't prevent each other from retrying tuples due to the maxUncommittedOffsets limit. 
//  another thread could be writing out the metadata cache to the database. 
// duplicate case 
//  Needed to keep SimpleFileObject constructor happy. 
// constraints and spreads 
//  remove executor details assigned to the failed worker 
//  COMPONENT_TO_SHARED_MEMORY 
//  EXECUTOR_INFO 
// Otherwise don't bother them 
//  Aggregate matching metrics over bucket timeframes.   We'll process starting with the longest bucket.  If the metric for this does not exist, we don't have to 
// this is the topology page, so we know the user is authorized 
//  verifyZeroInteractions(collector); 
//  the offset of the last available message + 1. 
//  We call fireMessageRead since the client is allowed to   perform this request. The client's request will now proceed   to the next pipeline component namely StormClientHandler. 
/* with exhibitor */
//  Creating nimbus hosts containing latest version of blob 
//  JSON_CONF 
//  check lock file presence 
//  interval at which to commit offsets to zk in milliseconds 
//  remove the slot from the existing assignments 
//  spout with 5 parallel instances 
//  Is the topology ZooKeeper authentication configuration unset?
// Just ignore these for now.  We are going to throw it away anyways 
//  holds remaining streams 
//  Class that has the logic to handle tuple failure.
//  Sets {@link SolrFieldsMapper} to use the default Solr collection if there is one defined 
//  Adds the serialized and base64 file to the credentials map as a string with the filename as   the key. 
//  works by emitting null to the collector. since the planner knows this is an ADD node with   no new output fields, it just passes the tuple forward 
//  get existing tuples and pending/unsuccessful triggers for this operator-component/task and add them to WindowManager 
//  =========== Consumer Rebalance Listener - On the same thread as the caller =========== 
//  if no truststore file, assume the truststore is the keystore. 
//  if we needed we could make config for update thread pool size 
//  Updating a blacklist file periodically with random words 
// all failed events are put in toResend, which is sorted by event's offset 
// anonymous user 
// error
//  PlannerImpl.transform() optimizes RelNode with ruleset 
// kill or newly submit 
// No task is under backpressure initially 
// Enable metrics 
//  5 partitions evicted to window state 
// for each owner, get resources, configs, and aggregate 
//  ISOLATED_NODE_GUARANTEE 
// get mapping of components to executors 
//  indexed by id 
//  configs - topo parallelism 
//  next scheduled refresh is sooner than (now + MIN_TIME_BEFORE_LOGIN). 
// fileOffset = one past last scanned file 
// This partition was previously assigned, so the consumer position shouldn't change 
//  catch any runtime exceptions caused by eviction 
//  this is ignored by javac currently but useJavaUtilZip should be 
//  required   required   required   optional   optional   optional   optional   optional   optional   optional 
//  make sure the property was actually set 
/*      * Bolt-specific configuration for windowed bolts to specify the sliding interval in time duration.      */
//  sanity check of the provided test data 
/*                 * split the sentences to words                 */
//  Otherwise poll to see if any new event   was scheduled. This is, in essence, the   response time for detecting any new event   schedulings when there are no scheduled   events. 
//  if lock file has been updated since last time, then leave this lock file alone 
//  Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple() 
//  META 
//  Sends the same tuple (list of scored/predicted values) to all the declared streams 
//  always check recQ if ACKing enabled 
//  package access for unit tests 
//         JCQueue spoutQ = new JCQueue("spoutQ", 1024, 100, 0);          JCQueue ackQ = new JCQueue("ackQ", 1024, 100, 0);            final AckingProducer ackingProducer = new AckingProducer(spoutQ, ackQ);          final Acker acker = new Acker(ackQ, spoutQ);            runAllThds(ackingProducer, acker); 
//  task Ids experiencing BP. can be null   task Ids no longer experiencing BP. can be null 
//  Instantiate the HdfsBolt 
//  this method should return sequential numbers starting at 0 
//  STATE_SPOUTS 
//  So we have copied and pasted some of the needed main.methods here. (with a few changes to logging)
//  GSID_TO_INPUT_STATS 
// Only place we fall though to do the loop over again... 
// Thread died before we could get the info, skip 
// Should not have flushed to file system yet 
// Make sure the worker is down before we try to shoot any child processes 
//  if not assign the highest sequence number. 
// slotsUsed < origRequest 
// Max heap size for a worker used by topology 
// tmpDir will be handled separately 
//  that store is passed to WindowStateUpdater to remove them after committing the batch. 
/*              * This is an arbitrary choice to make the result consistent with calculateMin. Any value would be valid here, becase there are             * no (non-zero) resources in the total set of resources, so we're trying to average 0 values.              */
//  if the component is a system (__*) component and we are hiding   them in UI, keep going 
//  ERRORS 
// metaStoreURI = "jdbc:derby:;databaseName="+System.getProperty("java.io.tmpdir") +"metastore_db;create=true"; 
/*  Ack the tuple, and commit.         * Since the tuple is more than max poll records behind the most recent emitted tuple, the consumer won't catch up in this poll.          */
/*          * Load DriverManager first to avoid any race condition between         * DriverManager static initialization block and specific driver class's         * static initialization block. e.g. PhoenixDriver         *         * We should take this workaround since prepare() method is synchronized         * but an worker can initialize multiple AbstractJdbcBolt instances and         * they would make race condition.         *         * We just need to ensure that DriverManager class is always initialized         * earlier than provider so below line should be called first         * than initializing provider.          */
//  class DirectInserter 
//  Keys sorted in descending order 
//  un-pin the last partition 
// Now check for autoCreds that are missing from the command line, but only if the   command line is used. 
//  store inprocess triggers of a batch in store for batch retries for any failures 
// Deleting this early does not hurt anything 
// request from hosts that are not authorized should be rejected 
// Ignored/expected... 
// The user from the token is bob, so verify that the name was set correctly... 
//  package level for unit tests 
// case 2: SaslTokenMessageRequest 
//  2nd topology 
// Gets a Database. 
// server 
//  hive principal storm-hive@WITZEN.COM   storm hive keytab /etc/security/keytabs/storm-hive.keytab   hive.metastore.uris : "thrift://pm-eng1-cluster1.field.hortonworks.com:9083" 
//  SHARED_MEM_ON_HEAP 
//  -- ignore file names config 
//  Normal create update sync scenario returns the greatest sequence number in the set 
//  for jackson 
//  heartbeat, ensure its no longer stale and read back the heartbeat data 
// This cannot happen since we're using a standard charset 
/*      * How often to poll Exhibitor cluster in millis.      */
//  intended behavior 
// expected 
// index == null if it is memory or CPU 
//  should the topology be active or inactive 
// get mapping of execs to components 
// play 1st tuple 
// We expect the bolt to log exactly one decorated line per emit 
// This must be defensively copied, because a bolt probably has only one rotation policy object 
//  test that the set-logger-level function was not called 
//  required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
//  printTimeCostArray(totalPointsArray); 
// cluster forgets about its previous status, so if it is scheduled just leave it. 
// Try again, or go to empty if assignment has been nulled 
//  Lastly, the default servlet for root content (always needed, to satisfy servlet spec) 
//  baseDir/supervisor/usercache/user1/filecache/files 
//  1) read 5 lines in file, 
//  Object associated with JSON field is already JSON 
//  required   required   required   required   required 
// check component is declared for spreading 
// First we need some configs 
//  trigger occurred, create an aggregation and keep them in store 
// Check if we can run a topology with that version of storm. 
//  if re-partitioning is involved, does a per-partition aggregate by key before emitting the results downstream 
//  deletes metadata strings before the provided timestamp 
// We assume that within 5% of the minimum congestion is still fine.  Not congested we grow (but slowly) 
//  the worker to orphan   the worker that fails   the healthy worker 
/*      * Same txid can be prepared again, but the next txid cannot be prepared     * when previous one is not committed yet.      */
// Topology Id -> executor ids -> component -> stats(...) 
//  merge contents of `config` into topology config 
/*      * Convenience method for registering CombinedMetric.      */
//  map from topology id -> set of sets of executors 
//  Make a key list to download 
//  TOTAL_WORKERS 
//  all tuples acked 
// Update the size of the objects 
//  should be at 7 
//  COMPLETE_LATENCY_MS 
// Copy the data 
// get metric by name 
//  If there are no remote outbound tasks, don't start the thread. 
//  Download missing blobs from potential nimbodes 
// just ignore any error/exception. 
//  avoid allocating SpoutAckInfo obj if not necessary 
// NOT going to timeout for a while 
// Try to emit all messages. Ensure only maxUncommittedOffsets are emitted 
//  Only try reading once. 
//  Jpmml evaluator 
/*         For each partition the spout is allowed to retry all tuples between the committed offset, and maxUncommittedOffsets ahead.        It is not allowed to retry tuples past that limit.        This makes the actual limit per partition maxUncommittedOffsets + maxPollRecords - 1,        reached if the tuple at the maxUncommittedOffsets limit is the earliest retriable tuple,        or if the spout is 1 tuple below the limit, and receives a full maxPollRecords tuples in the poll.          */
//  submit topology 
//  poll metrics every minute, then kill topology after specified duration 
//  validate at least two agg level none metrics exist 
// Allow the failed record to retry 
/*  * Listens for all metrics, dumps them as text to a configured host:port * * To use, add this to your topology's configuration: * * ```java *   conf.registerMetricsConsumer(org.apache.storm.testing.ForwardingMetricsConsumer.class, "<HOST>:<PORT>", 1); * ``` * * Or edit the storm.yaml config file: * * ```yaml *   topology.metrics.consumer.register: *     - class: "org.apache.storm.testing.ForwardingMetricsConsumer" *     - argument: "example.com:9999" *       parallelism.hint: 1 * ``` *  */
//  For now we do not make a transaction when removing a topology assignment from local, an overdue   assignment may be left on local disk.   So we should check if the local disk assignment is valid when initializing:   if topology files does not exist, the worker[possibly alive] will be reassigned if it is timed-out;   if topology files exist but the topology id is invalid, just let Supervisor make a sync;   if topology files exist and topology files is valid, recover the container. 
//  zk node under which to commit the sequence number of messages. e.g. /committed_sequence_numbers 
// Just do a few polls to check that nothing more is emitted 
// topo-3 should be evicted since its been up the longest 
//  ack few 
//  https://github.com/netty/netty/blob/netty-4.1.24.Final/transport/src/main/java/io/netty/channel/MultithreadEventLoopGroup.java#L40 
//  clean up the profiler actions that are not being processed 
//  Cassandra doesn't actually shut down until jvm shutdown so need to wait for that first. 
//  Authenticate: Removed after authentication completes 
// get a sorted list of unassigned executors based on number of constraints 
// Now schedule GPU but with the simple topology in place. 
//  I don't have anything 
//  targetSize in Bytes 
// Spout Settings 
//  defaults 
//  used for reporting used ports when heartbeating 
// any change to this code must be serializable compatible or there will be problems
/*                 * aggregate the count                 */
//  no-op 
//  make sure the timestamp on the metadata has the latest time 
//  Some components might have different resource configs. 
//  iterate again 
// Just quit 
// additional safety check to make sure that topologySubmitter is going to be a valid value 
// com.mysql.jdbc.jdbc2.optional.MysqlDataSource  jdbc:mysql://localhost/test  root  password 
// don't exit if not running, unless it is an Error 
// Unknown should only happen during compilation or some unit tests. 
//  Convert targets to a JSON serializable format 
//  a stream of words 
//  insert child in-between parent and its current child nodes 
//  SHUFFLE 
// Check that only the tuple on the currently assigned partition is retried 
//  optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
// just ignore the exception. 
// From LocalizerTest 
//  EXEC_STATS 
//  first 50 should have expired due to expire events threshold 
//  33 seconds passed by, timed out 
/*  If topic compaction is enabled in Kafka, we sometimes need to commit past a gap of deleted offsets         * Since the Kafka consumer should return offsets in order, we can assume that if a message is acked         * then any prior message will have been emitted at least once.         * If we see an acked message and some of the offsets preceding it were not emitted, they must have been compacted away and should be skipped.          */
//  since we made sys components hidden, the component map is empty for this worker 
//  NODE_HOST 
/*      * Methods for validating confs      */
// Also add in support for worker tokens 
//  1) Build phase - Segregate tuples in the Window into streams.      First stream's tuples go into probe, rest into HashMaps in hashedInputs 
// Now check for overlap on the node 
//  the window partition that holds the events 
//  number of retry attempts for zk 
/*      * Same txid can be committed again but the     * txid to be committed must be the last prepared one.      */
//  Make every attempt to sync the data we have.  If it can't be done then kill the bolt with   a runtime exception.  The filesystem is presumably in a very bad state. 
// Fall through if not supported 
// When tuple tracking is enabled, the spout must not commit acked tuples in at-most-once mode because they were committed before being emitted 
// To avoid locking we will go through the map twice.  It should be small so it is probably not a big deal 
/*                 * emit the count for the words that occurred                * at-least five times in the last two seconds                 */
// For backwards compatability 
//  nothing expires until threshold is hit 
//  default 30 seconds. (we cache the size so it is cheap to do) 
// Ack all emitted messages and commit them 
// Normally this is a NOOP 
//  perf critical path. don't use iterators. 
//  do nothing 
// Read the partitions, transaction ids and offsets from the old storm-kafka /user path 
//  max number of events to be cached in memory 
// get document 
//  required   required   optional   optional 
// read from state store, if not found, use startingOffset 
//  0 - validate args 
//  The match at this candidate offset failed, so start over with the   next candidate byte from the buffer. 
// ensure always same order for registrations with TreeMap 
//  Initialize spout using the same populated data (i.e same kafkaUnitRule) 
//  Refresh the Ticket Granting Ticket (TGT) periodically. How often to refresh is determined by the   TGT's existing expiry date and the configured MIN_TIME_BEFORE_RELOGIN. For testing and development,   you can decrease the interval of expiration of tickets (for example, to 3 minutes) by running : 
// populate worker to comp assignments 
/*  In non error scenarios, for the Azure Data Lake Store File System (adl://),                               the output stream must be closed before the file associated with it is deleted.                               For ADLFS deleting the file also removes any handles to the file, hence out.close() will fail.  */
//  no wildcard, file 
// 'groups username' command return is non-consistent across different unixes 
// Cached topology -> executor ids, used for deciding timeout workers of heartbeatsCache. 
//  create a windowed stream of five seconds duration 
// Spout should ack failed messages after they hit the retry limit 
//  take one's complement' 
//  environment variable substitution 
// Don't modify the original 
//  Do nothing. 
// The frequency of reporting 
//  sleep for 10 seconds 
// early detection/early fail 
//  ensures... overflowCount <= overflowLimit. if set to 0, disables overflow limiting. 
//  initialize the hashedInputs data structure 
// The current lat and count buckets are protected by a different lock   from the other buckets.  This is to reduce the lock contention   When doing complex calculations.  Never grab the instance object lock 
//  ====== 
//  found 
// Emit and fail the same tuple until we've reached retry limit 
// Don't pro-rate anything, it is all approximate so an extra bucket is not that bad. 
// parts[1] is empty for CGroups V2 else what is mapped that we are looking for 
// Wrapping it makes it mutable 
//  test deep equal 
// Save the current user to help with recovery 
//  [taskId]-> JCQueue :  initialized after local executors are initialized 
//  if shard iterator not present for this message, get it 
//  this triggers java.lang.RuntimeException: Cannot convert null to int
// This also tracks how many times worker transitioning out of a state 
//  storm blobstore update --file blacklist.txt key 
//  note that we delete the return value 
//  WAIT_SECS 
//  Get the list of keys from blobstore 
//  the following are required if we're defining a core storm topology DAG in YAML, etc. 
//  we use this "weird" wrapper pattern temporarily for mocking in clojure test 
//  -- archive dir config 
//  remove 
//  track resources - user to resourceSet  ConcurrentHashMap is explicitly used everywhere in this class because it uses locks to guarantee atomicity for compute and   computeIfAbsent where as ConcurrentMap allows for a retry of the function passed in, and would require the function to have   no side effects. 
// Write the mocking backwards so the actual method is not called on the spy object 
//  a user is will decide which topologies are run and which ones are not. 
// topo-3 evicted (lowest priority) 
//  if the retry service returns a message that is not in failed set then ignore it. should never happen 
// The key of the map is the worker id and the value is the corresponding workerslot object 
//  specified via bolt.select() ... used in declaring Output fields      protected String[] dotSeparatedOutputFieldNames; // fieldNames in x.y.z format w/o stream name, used for naming output fields 
//  check lock file is gone 
//  map from batchgroupid to coordspec 
//  test another topology getting blob with updated version - it should update version now 
// make sure all workers on scheduled in rack-1   The favored nodes would have put it on a different rack, but because that rack does not have free space to run the   topology it falls back to this rack 
//  Base for exponential function in seconds for retrying for second, third and so on failures 
//  {TopologyId -> {WorkerId -> {Executors}}} 
//  timestamp to decide when to commit to zk again 
//  HA Nimbus on being newly elected as leader. Change to a recurring pattern addresses these problems. 
//  Authorize 
//  wait for locks to expire 
//  should have created blobDir 
// free slot 
//  Test2: Launch topo 1, 2 and 4, they together request a little more mem than available, so one of the 3 topos will not be 
//  EXECUTE_MS_AVG 
//  CUSTOM_SERIALIZED 
//  class Pair 
//  try to modify the list, which should fail 
// default batch size 15000 
//  add default user ACL when only empty user ACL is not present 
// totalSpoutLag = totalLatestTimeOffset-totalLatestCompletedOffset 
// Stats... 
// Make the consumer return a single message for each partition 
//  Adding metadata to avoid null pointer exception 
// add a END_OF_BATCH indicator 
//  works out 
//  rebalance 
//  wait and check for expiry again 
/*      * The maximum number of states that will be searched looking for a solution in the constraint solver strategy      */
//  length of parts should be greater than 0 
//  Call prepare with our available taskIds 
//  We consume the iterator by traversing and thus "emptying" it. 
//  'stream' cannot be null, 
// taken care in finally block 
//  3 - submit topology, wait for a few min and terminate it 
// since user jerry has enough resource guarantee 
//  abandon file 
//  this filter makes sure to receive only Key or row but not values associated with those rows. 
/* without exhibitor */
// for backward compatibility. 
//  calc cid+sid->input_stats 
// Emit a message on each partition and revoke the first partition 
//  acls are not set for the blob (DEFAULT) 
// Metrics related 
//  required   required   required   optional   optional   optional   optional   optional   optional   optional   optional 
/*  IMeta  */
// Routine level 
// This should not happen because we checked for PUBLIC in isFieldAllowed 
//  bolts 
/*  Container of a collection of tuples  */
// The maxUncommittedOffsets limit should not be enforced, since it is only meaningful in at-least-once mode 
// Need  more data 
//  while 
//  spouts that use them.  Or if only one uses it they can be created inline with the add 
//  haven't found string, keep searching 
//  prevent issue when the implementation of fieldNames is not serializable   getRowType().getFieldNames() returns Calcite Pair$ which is NOT serializable 
// the json_conf is populated by TopologyBuilder (e.g. boltDeclarer.setMemoryLoad) 
//  group by node 
//  ACL 
//  Heartbeat here so that worker process dies if this fails   it's important that worker heartbeat to supervisor ASAP so that supervisor knows 
//  UPTIME_SECS 
//  if a HDFS keytab/principal have been supplied login, otherwise assume they are   logged in already or running insecure HDFS. 
//  Is this a directory or is it a file?
//  producer 
//  we might need to set the number of acker executors and eventlogger executors to be the estimated number of workers. 
//  HBase has some entries 
// We will adjust weights based off of the minimum load 
// Tried all of the slots and none of them worked. 
// No need to block, the task run by the executor is safe to run even after metrics are closed 
//  COMPONENT_DEBUG 
//  should be throw
// Nimbus Admin 
// component overrides 
//  Let us add the kerbClientPrinc and kerbTicket   We need to clone the ticket because java.security.auth.kerberos assumes TGT is unique for each subject   So, sharing TGT with multiple subjects can cause expired TGT to never refresh. 
// Stop emitting at a certain point, because log rolling breaks the tests. 
//  shard iterator corresponding to position in shard for new messages 
//  level 3 - longer idling with Thread.sleep() 
//  This does not have to be atomic, worst case we update when one is not needed 
//  if the windows should be persisted in state 
//  Define our taskIds and loads 
// Test for Supervisor Admin 
//  3) read another line and see if another log entry was made 
//  No control of cpu usage 
//  Emitted Offsets List 
// If there is an acked tupled after a compaction gap, the spout should commit it immediately 
//  main 
//  To Commit to Solr and Ack according to the commit strategy 
// Spouts 
/*      * Even though normally bolts do not need to care about thread safety, this particular bolt is different.     * It maintains a static field that is prepopulated before the topology starts, is written into by the topology,     * and is then read from after the topology is completed - all of this by potentially different threads.      */
//  ======== Activate / Deactivate / Close / Declare Outputs ======= 
// This will only ever grow, so no need to worry about falling off the end 
//  CPU_GUARANTEE_REMAINING 
//  update streamState based on stateUpdates 
//  Equivalent update command on command line 
// The partition revocation hook must be called before the new partitions are assigned to the consumer, 
// new topology needs to be scheduled 
//  A small control algorithm to adjust the amount of time that we sleep to make it more accurate 
// Cached GlobalStreamId 
// Special cases for storm... 
//  E 
//  Partition state path =   "/{prefix}/{topologyName}/{namespace}/{entityPath}/partitions/{partitionId}/state"; 
//  Everything should fit in a single slot 
//  last fetched sequence number corresponding to position in shard 
//  sorted acked sequence numbers - needed to figure out what sequence number can be committed 
// Check that null meta makes the spout seek to LATEST, and that the returned meta is correct 
//  tuple ts 
//  TODO: jsonify StormTopology   at the minimum should send source info 
// Timer 
//  Don't allow topoConf to override various cluster-specific properties.   Specifically adding the cluster settings to the topoConf here will make sure these settings   also override the subsequently generated conf picked up locally on the classpath.     We will be dealing with 3 confs:   1) the submitted topoConf created here   2) the combined classpath conf with the topoConf added on top   3) the nimbus conf with conf 2 above added on top.     By first forcing the topology conf to contain the nimbus settings, we guarantee all three confs 
//  Switch to the new assignment even if localization hasn't completed, or go to empty state 
//  Calcite ensures that the value is structurized to the table definition   hence we can use PK index directly   To elaborate, if table BAR is defined as ID INTEGER PK, NAME VARCHAR, DEPTID INTEGER   and query like INSERT INTO BAR SELECT NAME, ID FROM FOO is executed,   Calcite makes the projection ($1 <- ID, $0 <- NAME, null) to the value before INSERT. 
// clearing assignments 
// With earliest, the spout should also resume where it left off, rather than restart at the earliest offset. 
// 0 no load to 1 fully loaded  0 no load to 1 fully loaded 
//  2) Emit results 
//  setup timer for commit elapse time tracking 
// This additional check and download is for nimbus high availability in case you have more than one nimbus 
//  write the tuple 
//  Get the most recent artifact as a String, and then parse the yaml 
//  proposedRefresh is too far in the future: it's after ticket expires: simply return now. 
// Cached CuratorFramework, mainly used for BlobStore. 
//  Construct a message containing the SASL response and send it to the server. 
// Now check that the spout will emit another maxUncommittedOffsets messages 
//  PROCESS_LATENCY_MS 
//  thread safety: assumes Collector.emit*() calls are externally synchronized (if needed). 
// Test for a user having WRITE or ADMIN privileges to change replication of a blob 
//  message was acked after being retried. so clear the state for that message 
//  reset commit timer such that commit happens on next call to nextTuple() 
// The retry schedules for two messages should be unrelated 
/*      * Bolt-specific configuration for windowed bolts to specify the time interval for generating     * watermark events. Watermark event tracks the progress of time when tuple timestamp is used.     * This config is effective only if {@link org.apache.storm.windowing.TimestampExtractor} is specified.      */
//  group field 
/*          * A stream of words emitted by the QuerySpout is used as         * the keys to query the state.          */
// Add in supervisors that might have crashed but workers are still alive 
// put [owner-> StormBase-list] mapping to ownerToBasesMap  if this owner (the input parameter) is null, add all the owners with stormbase and guarantees  else, add only this owner (the input paramter) to the map 
// If the system is low on memory we cannot be kind and need to shoot something 
// The child processes typically exit in < 1 sec.  If 2 mins later they are still around something is wrong 
// Only log if the leader has changed.  It is not interesting otherwise. 
// A race happened and it is probably not running
//  required   required   optional 
//  Authorize: client is allowed to doRequest() if and only if the client 
//  This spout is added to test purpose, so just failing fast doesn't hurt much 
//  minimum 1.x version of supporting STORM-2448 would be 1.0.4 
// Advance the time and replay the failed tuple.  
/*      * Bolt-specific configuration for windowed bolts to specify the sliding interval as a count of number of tuples.      */
//  Only need to keep track of acked tuples if commits to Kafka are controlled by   tuple acks, which happens only for at-least-once processing semantics 
//  do not close this InputStream in method: it will be used from jetty server 
//  partition ids 0 .. 19 
// if the current assignment is already running, new assignment will never be promoted to currAssignment,   because Timer is not being compared in #equals or #equivalent, meaning newAssignment always equals to currAssignment. 
// This happens if the min id is too small 
//  config 
// Should not show files outside worker log root.
//  valid to delete before what's been committed since    those batches will never be accessed again 
//  Client should not be sending other-than-SASL messages before   SaslServerHandler has removed itself from the pipeline. Such   non-SASL requests will be denied by the Authorize channel handler   (the next handler upstream in the server pipeline) if SASL   authentication has not completed. 
//  window system state 
//  used internally to merge values in groupByKeyAndWindow 
// http://supervisor2:8000/download/DemoTest-26-1462229009%2F6703%2Fworker.log  http://supervisor2:8000/log?file=SlidingWindowCountTest-9-1462388349%2F6703%2Fworker.log 
//  register the newly established channel 
//  when storeTuplesInStore is false then the given windowStoreFactory is only used to store triggers and 
// obtain a serializer object 
// Verify correct unwrapping of partitions and delegation of assignment 
// Meter declared here can be registered by any daemon, and is currently used by Supervisor 
//  send flush tuple to all local executors 
/*          * Include the topology name & worker port in the file name so that         * multiple event loggers can log independently.          */
//  Another KafkaSpout instance (of this topology) already committed, therefore FirstPollOffsetStrategy does not apply. 
// Adjust the divisor for the average to account for any skipped resources (those where the total was 0) 
// bolt overrides 
//  RATE 
//  1) schedule the heartbeat on one thread in pool 
// Ensure the commit timer has expired 
//  if we didn't get the group - just return empty list; 
// parts[0] == 0 for CGroup V2, else maps to hierarchy in /proc/cgroups 
// Yes we are putting in a config that is not the same type we pulled out. 
//  REPLICATION_FACTOR 
//  cleanup 
//  max number of window events in memory 
//  Non Blocking. returns count of how many inserts succeeded 
//  1 concurrent deletion - only one thread should succeed 
//  Now let's update it, but not advance time.  Should get old map again. 
// Remove the unneeded entries from the graph  We want to keep all of our nodes, and the nodes that they are connected directly to (parents and children). 
//  we have enough resources now... 
//  Don't emit anything .. allow configured spout wait strategy to kick in 
//  Was scheduled for retry and re-emitted, so remove from schedule. 
//  ON_HEAP 
//  no filter configured, allow anyone 
// Setup spout with mock consumer so we can get at the rebalance listener    
//  NOTE: Variable used in lambda expression should be final or effectively final   (or it will cause compilation error),   and variable type should implement the Serializable interface if it isn't primitive type   (or it will cause not serializable exception). 
// Hosts this user is authorized to impersonate from. 
/*                  * If a worker crashes, the states of all workers are rolled back and an initState message is sent across                 * the topology so that crashed workers can initialize their state.                 * The bolts that have their state already initialized need not be re-initialized.                  */
//  ENABLE 
//  note that sometimes the tuples active may be less than max_spout_pending, e.g.   max_spout_pending = 3   tx 1, 2, 3 active, tx 2 is acked. there won't be a commit for tx 2 (because tx 1 isn't committed yet),   and there won't be a batch for tx 4 because there's max_spout_pending tx active 
//  Multivalue field split by non default token %   to match dynamic fields of the form "*_txt"   this field won't be indexed by solr 
// A map consisting of all workers on the node. 
//  if we had a timeout, but the timeout is no longer active 
//  sample fieldDescriptor = "stream1:x.y.z" 
// This is an odd case for a rolling upgrade where the user on the old assignment may be null,   but not on the new one.  Although in all other ways they are the same.   If this happens we want to use the assignment with the owner. 
// Don't override the host name, or everything looks like it is on nimbus 
//  REQUESTED_REGULAR_ON_HEAP_MEMORY 
//  this is also a helpful optimization that state implementations don't need to manually do 
// sigar uses JNI and does not work in local mode 
//  for mesos, this is {hostname}-{topologyid} 
// WritableByteChannel is a Channel which implements Closeable.   Hence although declared AutoCloseable super#close here should only throws IOException  We rethrow to conform the signature 
// 2) Retire them 
// create a factory class 
//  ASSIGNMENT_ID 
//  ASSIGNED_MEMOFFHEAP 
//  a topology with multiple spouts 
//  populate metric values using the provided key 
//  Or done separately like with setting the 
//  since 2 is the largest un-pinned entry before 3 is loaded 
// Commits offsets during deactivation 
//  no reason to try to execute a previous attempt than we've already seen 
//  pipeline component. 
//  Should have been re-emitted 
//  MEMORY_GUARANTEE_REMAINING 
//  Add the event logger details. 
// To support topologies of older version to run, we might have to loose the constraints so that  the configs of older version can pass the validation. 
// There is a race on backpressure too... 
//  free the error stream buffer 
// no name 
//  in partitioned example, in case an emitter task receives a later transaction than it's emitted so far,   when it sees the earlier txid it should know to emit nothing 
//  Some times a Bolt or Spout will have some memory that is shared between the instances   These are typically caches, but could be anything like a static database that is memory   mapped into the processes. These can be declared separately and added to the bolts and 
// Creates a MongoURI from the given string. 
// Verify that it only committed the message on the assigned partition 
// hierarchy-ID:controller-list:cgroup-path 
//  ID 
// This can happen when a topology is first coming up   It's thrown by the blobstore code 
// fall back to string, which is already set 
// There can be more then one line if cgroups are mounted in more then one place, but we assume the first is good enough 
// The resource that is not used should count as if it is being used 0% 
//  could block 
//  One tuple and one rotation should yield one file with data 
//  if null, acks every tuple 
//  explicit delete for ephemeral node to ensure this session creates the entry. 
// The spout must respect maxUncommittedOffsets even if some tuples have been acked but not committed 
//  projection 
//  search all metadata strings 
// This should never happen because only the primary nimbus is active, but just in case   declare the race safe, even if we lose it. 
//  should remove the blob since cache size set really small 
//  if newReader is true and tuple is null then it is an empty reader 
//  Populate user -> password map with JAAS configuration entries from the "Server" section.   Usernames are distinguished from other options by prefixing the username with a "user_" prefix. 
//  3) read remaining lines in file, then ensure lock is gone 
//  We haven't received the entire object yet, return and wait for more bytes. 
//  EXECUTOR_START_TIME_SECS 
// Schedule the simple topology first 
//  first offset of this batch 
//  this tuple should be removed from emitted only inside the ack() method. This is to ensure   that the OffsetManager for that TopicPartition is updated and allows commit progression 
// Add an element and bar should drop out 
//  below calls shouldn't propagate any exceptions 
//     private String timestampField; 
//  we need to be able to lookup bolts by id, then switch based 
//  parse the output   clear the input stream buffer 
// Treat it like a jar 
// This is very private and does not need to be exposed 
//  fetch records from kinesis starting at sequence number for message passed as argument. Any other messages fetched   and are in the failed queue will also 
//  It's possible for target to have multiple tasks if it reads multiple sources 
//  Tests for case when subject != null (security turned on) and 
//  make sure samplingPct is within bounds. 
// will fail since org.apache.storm.nimbus.NimbusInfo doesn't implement or extend org.apache.storm.networktopography   .DNSToSwitchMapping 
// port range 
//  2 try to grab another lock while dir is locked 
// schedule tasks that are not part of components returned from topology.get_spout or 
//  function to call when timer is killed 
//  1 -  Setup Const Spout   -------- 
// Now schedule all of the topologies that need to be scheduled 
//  in critical path. don't use iterators. 
// good 
// this default ensures things expire at most 50% past the expiration time 
// This should be the load metrics.   There will usually only be one message, but if there are multiple we only process the latest one. 
//  should remove the second blob first 
//  get close enough 
// comma separated offsets 
// 2 close file and retry creation 
//  Track serialized size of messages. 
// Map the value 
//  string does not exist 
//  sync the filesystem after every 1k tuples 
//  for kryo compatibility 
//  Log median ratios for different strategies 
// grouping assignment by node to see the nodes diff, then notify nodes/supervisors to synchronize its owned assignment  because the number of existing assignments is small for every scheduling round, 
//  {metric -> value} 
// is removed since that is what is going to trigger the retry for cleanup 
//  level 1 - no waiting 
//  topology.blobstore.map='{"key":{"localname":"blacklist.txt", "uncompress":"false"}}' 
//  If this partition was previously assigned to this spout, 
// Add new records to Kafka, and check that the next batch contains these records 
// Scheduler histogram 
// PRECONDITION: The new and current assignments must be equivalent 
//  Submit to Storm cluster 
// For example, we have a three nodes(supervisor1, supervisor2, supervisor3) cluster:  slots before sort:  supervisor1:6700, supervisor1:6701,  supervisor2:6700, supervisor2:6701, supervisor2:6702,  supervisor3:6700, supervisor3:6703, supervisor3:6702, supervisor3:6701  slots after sort:  supervisor3:6700, supervisor2:6700, supervisor1:6700,  supervisor3:6701, supervisor2:6701, supervisor1:6701,  supervisor3:6702, supervisor2:6702,  supervisor3:6703 
// .. ignore 
// This shouldn't throw on a Check, because nothing is configured yet 
//  drain 1 element and ensure BP is relieved (i.e tryPublish() succeeds) 
// Save the private worker key away so we can test it too. 
//  we found a spout   spout 
//  [taskId-indexingBase] => queue : List of all recvQs local to this worker 
//  SPOUT_OBJECT 
//  In the short term the goal is to not shoot anyone unless we really need to.   The on heap should limit the memory usage in most cases to a reasonable amount   If someone is using way more than they requested this is a bug and we should   not allow it 
//  For native protocol V3 or below, all variables must be bound.   With native protocol V4 or above, variables can be left unset,   in which case they will be ignored server side (no tombstones will be generated). 
//  Location of the file in the artifactory archive.  Also used to name file in cache. 
// INT + SHORT 
// Migrate the /coordinator currtx, currattempts and meta directories.  The new spout expects the list of topic partitions as coordinator meta. 
/* ========== Implementations ========== */
// Emit maxUncommittedOffsets messages, and fail all of them. Then ensure that the spout will retry them when the retry backoff has passed 
//  Generate SASL response (but we only actually send the response if 
//  Overloading the readInt method accomodate Subject in order to check for authorization (security turned on) 
// Get and set the start time before getting current time in order to avoid potential race with the longest-scheduling-time-ms gauge 
//  Make sure we don't process too frequently. 
//  FIELDS 
//  c.f. HADOOP-6559 
//  boltMsgQueue should have at least one entry at the moment 
//  intermediate bolt, subscribes to jms spout, anchors on tuples, and auto-acks 
//  map to track number of failures for each kinesis message that failed 
/*                                                * Transform the stream of words to a stream of (word, 1) pairs                                                */
// Yes this should be a topo name, but it makes this simpler... 
// create a transport factory that will invoke our auth callback for digest 
//  for the given processor node, if we received punctuation from all tasks of its parent windowed streams 
// Test for Nimbus Admin 
//  max lag 
//  bolt stats 
//  STATE_SPOUT_OBJECT 
//  or database. 
// emit second batch 
//  when there is no input field then the whole tuple is considered for comparison. 
//  for each executor -> node+port pair 
//  publish a retained message to the broker 
//  this is to distinguish from TransactionAttempt 
// Just go on and try to delete the others 
// We created it so lets chmod it properly 
// Stop searching if the message is known to be ready for retry 
// Daemon common main.methods
// bobby has no guarantee so topo-2 and topo-3 evicted 
//  validate search by topology id and executor id 
// hard coded max number of states to search 
// Consumer. Sets up a topology that reads the given Kafka spouts and logs the received messages 
//  this is to support things like persisting off of drpc stream, which is inherently unreliable   and won't have a tx attempt 
// convert NodePort to NodeInfo (again!!!). 
//  effectively disable commits based on time 
// If .current and .version do not match, we roll back the .version file to match   what .current is pointing to. 
// Any other failure result  The assumption is that the strategy set the status... 
//  Shutdownable via INimbusCredentailPlugin 
//  we do this since to concat a null String will actually concat a "null", which is not the expected: "" 
//  Storm config 
// Test for replication using SUPERVISOR access 
//  its possible a string is used by multiple types of metadata strings 
//  This avoids a race condition with cancel-timer. 
// Resources missing from used are using none of that resource 
//  idx - index of the type of this field in the FieldType list 
//  Build get query 
//  delete db and all tables in it 
//  acls for the blob are set to DEFAULT (Empty ACL List) only for LocalFsBlobstore 
//  ===== 
/*  The output field of the spout ("lambda") is provided as the boltMessageField          so that this gets written out as the message in the kafka topic.          The tuples have no key field, so the messages are written to Kafka without a key. */
// MemFree:        14367072 kB  Buffers:          536512 kB  Cached:          1192096 kB   MemFree + Buffers + Cached 
// Nothing 
//  client). 
// Right now this is only used for sending metrics to nimbus,   but we may want to combine it with the heartbeatTimer at some point 
//  default cache size 10GB, converted to Bytes 
//  it should never happened since we apply UUID 
//  checkpoint spout should 've been added 
//  locate expired lock files (if any). Try to take ownership (oldest lock first) 
// This may or may not be reported depending on when process exits 
//  sort by available slots size: from large to small 
// case 1: plugin is a IContext class 
//  able to delete the blob without checking meta's ACL   skip checking everything and continue deleting local files 
// Ignored if cgroups is not setup don't do anything with it 
// The spout must reemit failed messages waiting for retry even if it is not allowed to poll for new messages due to maxUncommittedOffsets being exceeded 
// Check to see if we have enough slots before trying to get them 
//  to the supervisor 
//  go off to blobstore and get it   assume dir passed in exists and has correct permission 
//  Share some common metadata strings to validate they do not get deleted 
// simulate worker loss 
//  a topology with two unconnected partitions 
//  inputFields can be equal to outFields, but multiple aggregators cannot have intersection outFields 
//  Run only once. 
//  Acknowledge all changing blobs as futures 
// The resources are already normalized 
//  used to recognize the pattern of some meta files in a worker log directory 
//  name 
/*      * Just output the word value with a count of 1.     * The HBaseBolt will handle incrementing the counter.      */
// Have not moved to a java worker yet 
//  update to the latest timestamp and add to the string cache 
//  schedule first block (0% - 10%) 
//  close the state to force flush 
/*          * When partitions are reassigned, the spout should seek with the first poll offset strategy for new partitions.         * Previously assigned partitions should be left alone, since the spout keeps the emitted and acked state for those.          */
//  OFF_HEAP_WORKER 
//  JMS Topic provider 
//  class DirLockingThread 
// still need to return the first of pending list 
//  timestamp to be used for shardIteratorType AT_TIMESTAMP - can be null 
// get factory class name 
//  blocking call that can be interrupted using Thread.interrupt() 
//  Download updated blobs from potential nimbodes 
//  consumer 
//  Tell cassandra where the configuration files are. Use the test configuration file. 
//  if maximum uncommitted records count has reached, so dont emit any new records and return 
//  5 min 
//  check lock creation/deletion and contents 
// Always have a space in between 
//  window-state table should already be created with cf:tuples column 
//  archive passed in must contain symlink named tmptestsymlink if not a zip file 
// bolt 
//  TOPOLOGIES 
/*                 * Join the squares and the cubes stream within the window.                * The values in the squares stream having the same key as that                * of the cubes stream within the window will be joined together.                 */
//  acquire lock on file1 and verify if worked 
// Scheduling changed 
//  please pick small artifact which has small transitive dependency   and let's mark as Ignore if we want to run test even without internet or maven central is often not stable 
//  last evaluated and last expired message ids per task stream (source taskid + stream-id) 
// For some reasons, we can not get supervisor port info, eg: supervisor shutdown,  Just skip for this scheduling round. 
//  need to set memory.memsw.limit_in_bytes after setting memory.limit_in_bytes or error   might occur 
//  CPU 
//  required   required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
//  save a little typing 
// no op. 
//  The events should be put in a window when the first watermark is received 
/*  * To change this template, choose Tools | Templates * and open the template in the editor.  */
//  only one of the multireducers will receive the tuples 
//  merge stdout and stderr 
//  no capacity for spout 
// This spout owns 2 partitions: 6 and 14 
//  CPU_USAGE 
//  wait for the process to finish and check the exit code 
//  the "old" trident kafka spout always returns true, like this 
//  utility main.methods
//  late tuple stream 
// When tuple tracking is enabled, the spout must not replay tuples in no guarantee mode 
//  Memory is the constraining resource. 
//  tuple values are mapped with   metric, timestamp, value, Map of tagK/tagV respectively. 
//  adds addressedTuple to destination Q if it is not full. else adds to pendingEmits (if its not null) 
//  then facing backpressure 
//  hidden sys component 
// Sleep a bit to avoid hogging the CPU. 
/*                      * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed, lots of (more                     * than max.poll.records) later messages were acked, and the failed message then gets acked. The consumer may only be                     * part way through "catching up" to where it was when it went back to retry the failed tuple. Skip the consumer forward                     * to the committed offset.                      */
//  for 
//  INFO 
// J_PROFILE_START is not used.  When you see a J_PROFILE_STOP   start profiling and save it away to stop when timeout happens 
// register metrics 
//  SERVER_PORT 
// Try to maintain rolling upgrade compatible with 0.10 releases 
//  Extract the field from tuple. Field may be nested field (x.y.z) 
// 1 try to append to an open file 
// until Ctrl-C 
// for the topology which wants rebalance (specified by the scratchTopoId)   we exclude its assignment, meaning that all the slots occupied by its assignment   will be treated as free slot in the scheduler code. 
//  if not in blacklist then add it and set the resume time according to config 
//  first topology should get evicted for higher priority (lower value) second topology to successfully schedule 
//  strong supervisor node 
//  TOPOLOGY STATE TRANSITIONS 
//  all events since last clear 
//  KEYS 
//  Deny unsupported operations. 
//  flush db to disk 
/*                  * No events were found in the previous window interval.                 * Scan through the events in the queue to find the next                 * window intervals based on event ts.                  */
//  if null then use zookeeper used   by Storm 
// The other tuples are used to reset the first tuple's timeout, 
//  Builds a JSON list 
//  this store is used only for storing triggered aggregated results but not tuples as storeTuplesInStore is set   as false int he below call. 
//  graph with 3 kinds of nodes:   operation, partition, or spout   all operations have finishBatch and can optionally be committers 
// Emit all remaining messages. Failed tuples retry immediately with current configuration, so no need to wait. 
// We are in the same process we cannot recover anything 
//  sorted failed sequence numbers - needed to figure out what sequence number can be committed 
// Skip special case if `storm kill_workers` is already invoked 
//  WINDOW_TO_ACKED 
// There is a race on logconfig where they can be leaked in some versions of storm. 
// Expected... 
//  common stats 
//  Allow searching when start-byte-offset == file-len so it doesn't blow up on 0-length files 
// NOOP no need to create links in local mode 
//  We don't need to take care of sync, cause we're always updating heartbeat 
//  Producers. This is just to get some data in Kafka, normally you would be getting this data from elsewhere 
//  Get start/end indices for blocks 
// There is no security so we are done. 
// Log file permissions 
// Security off  is admin  is in allowed group  is an allowed user 
//  optional   optional   optional   optional   optional   optional   optional 
// Prevent fileName from pathing into worker logs, or outside daemon log root  
/*  weight: 100-1000  */
//  test commit creates properly 
// Ignored... 
// Get the nodes 
// Some tests rely on reading the worker log. If there are too many emits and too much is logged, the log might roll, breaking the test.  Ensure the time based windowing tests can emit for 5 minutes 
//  TOTAL_RESOURCES 
//  [taskId-indexingBase] => queue, some entries can be null. : outbound Qs for this executor instance 
//  TOPOLOGY_STATS 
//  batch 1 is replayed with 50 tuples. 
// We want to update longest scheduling time in real time in case scheduler get stuck   Get current time before startTime to avoid potential race with scheduler's Timer 
/*          * atomically decrement the count if its greater than threshold and         * return if the event should be evicted          */
//  We ran out of buffer for the search. 
//  returns true if there was a change in the BP situation 
// proxy timeout 
// set topology name so that sample Trident topology can use it as stream name. 
// look for available port 
//  stores.length+1 as 2 users in Bengaluru 
//  Add messages 
// Always make sure to clean up everything else before worker directory 
// Races are okay this is just to avoid extra work for each page load. 
//  10th partition should not have been evicted 
//  Note, we could add support for setting the replication factor 
//  delete anything older than an hour 
// We only have a set amount of time we can wait for before looping around again 
/*  LOOK AT LOCAL BLOBSTORE  */
//  100 events with past ts should expire 
/*  IFn  */
//  The list of all executors (preferably sorted to make assignments simpler). 
//  this is for backwards compatibility 
//  see if lockTimeoutSec time has elapsed since we last selected the lock file 
// The supervisor on the node down so add an orphaned slot to hold the unsupervised worker 
// Not set if RECOVER_PARTIAL  Not set if RECOVER_PARTIAL 
//  We can use the task index (starting from 0) as the partition ID 
//  Read the length field. 
// if authorizationId is not set, set it to authenticationId. 
// the first transaction in the new batch 
//  end include processing 
//  Reusing TupleInfo object as we directly call executor.ackSpoutMsg() & are not sending msgs. perf critical 
//  ack 1 tuple 
// Verify that the commit logic can handle offset voids due to log compaction 
// The maximum number of state to search before stopping. 
// so Kafka must be able to return more messages than that in order for the tests to be meaningful 
// Log the user in and get the TGT 
//  LONG_ARG 
//  Make sure if there's enough bytes in the buffer. 
//  Check to see if there are any existing files already localized. 
//  generate a random storm id 
//  A null value or a String value is acceptable 
//  since this processor type is a committer, this occurs in the commit phase 
// Ignore the future 
//  get the worker count back s.t. we can assert in each test function 
//  MetricsCollectorConfig 
//  22 seconds passed by, still not timing out 
// We are going to skip over CPU and Memory, because they are captured elsewhere 
//  TOPOLOGY_ACTION_OPTIONS 
//  this code here ensures that only one attempt is ever tracked for a batch, so when 
// Discard the pending records that are already committed 
// We will not follow sym links 
//  To Commit to Solr and Ack every tuple 
// local node 
//  calls this before actually killing the worker locally...   sends a "task finished" update 
//  to keep track of free slots 
//  should pass 
//  optional   optional   optional   optional   optional 
//  if there are no more pending consumed messages   and storm delivered ack for all 
//  timer == null only if the processing guarantee is at-most-once 
//  Give a hook chance to alter the clock. 
// populating request context 
// Allow poll if there are retriable tuples within the maxUncommittedOffsets limit 
//  Build cluster and connect 
//  2) check log file contents 
//  create thread to process insertion of all metrics 
//  2) deleting open file - should return true 
//  Most of clojure tests currently try to access the blobs using getBlob. Since, updateKeyForBlobStore   checks for updating the correct version of the blob as a part of nimbus ha before performing any   operation on it, there is a necessity to stub several test cases to ignore this method. It is a valid   trade off to return if nimbusDetails which include the details of the current nimbus host port data are   not initialized as a part of the test. Moreover, this applies to only local blobstore when used along with   nimbus ha. 
// get spread components 
//  shared off heap node memory 
// re-emit second batch 
/*  Thread safe. Same instance can be used across multiple threads  */
//  a "topology source" is a class that can produce a `StormTopology` thrift object. 
//  For the case that ack_fail message arrives before ack_init 
//  properties file substitution 
//  number of spout executors 
//  JDK 7 tries to automatically drain the input streams for us   when the process exits, but since close is not synchronized,   it creates a race if we close the stream first and the same   fd is recycled.  the stream draining thread will attempt to   drain that fd!!  it may block, OOM, or cause bizarre behavior   see: https://bugs.openjdk.java.net/browse/JDK-8024521        issue is fixed in build 7u60 
//  INITIAL_STATUS 
/* solo */
//  this is a sink and no result to emit. 
//  yes it's just a test purpose 
//  retrieving is encapsulated in Retrieval interface 
//  MEMORY_GUARANTEE 
// We don't really care too much about the scheduling of topology-gpu-0, because it was scheduled. 
// Ignored, will go with default timeout 
// where state is stored in zookeeper (only for batch spout types) 
// The blobstore is good, now lets get the list of all topo Ids 
//  Method which initializes nimbus admin 
//  case 1: Control message 
//  then (exception) 
//  NUM_ERR_CHOICE 
//  kinesis stream name to read from 
// The spout must respect maxUncommittedOffsets after committing a set of records 
//  other builder functions not exposed:    * createsObjectNamesWith(ObjectNameFactory onFactory)    * registerWith (MBeanServer)    * specificDurationUnits (Map<String,TimeUnit> specificDurationUnits)    * specificRateUnits(Map<String,TimeUnit> specificRateUnits) 
//  Restart topology with the same topology id, which mimics the behavior of partition reassignment 
// Everything is scheduled correctly, so no need to search any more. 
/*              * Nodes in the descending order of priority.             * ProcessorNode has higher priority than partition and window nodes             * so that the topological order iterator will group as many processor nodes together as possible.             * UpdateStateByKeyProcessor has a higher priority than StateQueryProcessor so that StateQueryProcessor             * can be mapped to the same StatefulBolt that UpdateStateByKeyProcessor is part of.              */
//  2) If no abandoned files, then pick oldest file in sourceDirPath, lock it and rename it 
//  ack to not process the record again on restart and move on to next message 
//  then 
/*  validate cpu settings  */
//  release both locks 
// children is only ever null if topologyBasicBlobsRootDir does not exist.  This happens during unit tests   And because a non-existant directory is by definition clean we are ignoring it. 
//  last partition is not evicted 
//  ASSIGNED_SHARED_ON_HEAP_MEMORY 
// We have something to schedule... 
// Support for worker tokens Similar to an IAutoCredentials implementation 
//  unexpected error
//  NUM_EXECUTORS 
//  A blank result communicates that there are no more blobs. 
//  Files/move with non-empty directory doesn't work well on Windows   FileUtils.moveDirectory is not atomic 
// play 2nd tuple 
// each thread will have its own request context 
/*          * if the current processor preserves the key and is         * already partitioned on key, skip the re-partition.          */
// Test for Nimbus itself as a user 
//  register most recent relogin attempt 
//  Decoder 
// This is allowed because the committed message brings the numUncommittedOffsets below the cap 
//         ret.numSupervisors = clusterSummary.get_supervisors_size(); 
//  only enable cleanup of blobstore on nimbus 
//  disregard first line because it has header, already read 
// Sleep for 5 mins 
//  It is imperative to not run the function   inside the timer lock. Otherwise, it is   possible to deadlock if the fn deals with   other locks, like the submit lock. 
//  ID_TO_BOLT_AGG_STATS 
// Lets use the number of actually scheduled workers as a way to bridge RAS and non-RAS 
//  AcceptedBlockTimeRatios obtained by empirical testing (see comment block above) 
// Verify if WorkerTokenManager recognizes the expired WorkerToken. 
//  good to go, increment # of tasks this component is being executed on 
// order executors to be scheduled 
//  if it does not exist. 
//  Metadata information to commit to Kafka. It is unique per spout instance. 
//  Number of times we had to backtrack. 
/*                  * if a null tuple is not configured to be emitted, it should be marked as emitted and acked immediately to allow its offset                 * to be commited to Kafka                  */
// non impersonating request, should be permitted. 
// set the number of workers to be the same as partition number.  the idea is to have a spout and a partial count bolt co-exist in one  worker to avoid shuffling messages across workers in storm cluster. 
// topology being null is used for tests  We probably should fix that at some point,   but it is not trivial to do... 
/* ==================================================     * Helper Classes     *================================================== */
//  Stop searching as soon as passed current time 
/*      * Register a IMetric instance.     *     * Storm will then call `getValueAndReset()` on the metric every `timeBucketSizeInSecs`     * and the returned value is sent to all metrics consumers.     *     * You must call this during `IBolt.prepare()` or `ISpout.open()`.     * @return The IMetric argument unchanged.      */
//  1 expired 
// Commit polled records immediately to ensure delivery is at-most-once. 
// nimbus:num-blacklisted-supervisor + non-blacklisted supervisor = nimbus:num-supervisors 
// In distributed mode, send heartbeat directly to master if local supervisor goes down. 
//  class BatchInserter 
//  pause other topic-partitions to only poll from current topic-partition 
// A map of assignments organized by node with the following format: 
//  Earliest start 
//  can be regular nodes (static state) or processor nodes 
// rotateOutputFile(writer) has closed the writer. It's safe to remove the writer from the map here. 
//  ensure that the first three tasks have been selected before 
//  swapping two arrays 
//  if the streamId is defined, use it for the grouping, otherwise assume storm's default stream 
//  GROUPS 
/*                 * print the results to stdout                 */
// Verify that bob's token has expired 
//  just make a note of the oldest expired lock now and check if its still unmodified after lockTimeoutSec 
//  take the batched metric data and write to the database 
//  assume message is immediately ACKed in non-ack mode 
//  EXECUTORS 
/*  debug only! Once we have confidence, can lose this.  */
//  enable ACKing 
//  required   required   required   required 
//  Identify the join field for the stream, and look it up in 'tuple'. field can be nested field:  outerKey.innerKey 
// just skip when any error happens wait for next round assignments reassign 
//  REQUESTED_CPU 
//  There's enough bytes in the buffer. Read it. 
//  if cb returns false, we are done with this section of rows 
// For others using too much it is really a question of how much memory is free in the system 
//  partition ids 
// Topology may be deployed in deactivated mode, wait for activation 
// random number generator 
//  JUnit ensures that the temporary folder is removed after   the test finishes 
//  Instead of iterating again, it would be possible to commit and update the state for each TopicPartition   in the prior loop, but the multiple network calls should be more expensive than iterating twice over a small loop 
//  -- source dir config 
//  1) read initial lines in file, then check if lock exists 
//  consider all events for the initial window 
// initialize assignment map 
//  save metric key/value to be batched 
//  SIGNATURE 
//  allow blacklist scheduler to cache the supervisor 
//  invoke setter 
//  to be done for SASL_TOKEN_MESSAGE_REQUEST requests. 
// This class assumes that there is at most one retry schedule per message id in this set at a time. 
//  remove the executors cache to let it recompute. 
//  numFails = 1, 2, 3, ... 
// check if annotation is one of our 
// return false if can't increment anymore 
//  Thread Polling every 5 seconds to update the wordSet seconds which is   used in FilterWords bolt to filter the words 
//  RECORDS 
//  for tests, reader will not be null 
//  FULL_CLASS_NAME 
//  Acked messages sorted by ascending order of offset 
//  Advance time and then trigger first call to kafka consumer commit; the commit must progress to offset 9 
//  get topology info 
//  To make logic simple, it assumes that all the tables have one PK (which it should be extended to support composed key), 
// We are not really running anything so make this   simple to check for 
//  select tasks once more than the number of tasks available 
//  IAutoCredentials   ICredentialsRenewer   INimbusCredentialPlugin    IPrincipalToLocal    IGroupMappingServiceProvider  
// Remove any configs that are specific to a host that might mess with the running topology. 
//  ################# Subscribe Callback Implementation ###################### 
//  Removing self so as not to create a deadlock where a nimbus is trying to download a missing blob   from itself 
//  the states to be recovered 
// topo-3 evicted since user bobby don't have any resource guarantees and topo-3 is the lowest priority for user bobby 
//  always provide mocked HiveWriter 
//  to be use. If we cannot calculate it assume that it is bad 
//  Report messageSizes metric, if enabled (non-null). 
//  NODES 
//  perf critical check to avoid unnecessary allocation 
// Not symmetric difference. Performing A.entrySet() - B.entrySet() 
//  and provides PairStream(KeyedStream) to consumer bolt. 
/*                 * The elements having the same key within the window will be grouped                * together and the corresponding values will be merged.                *                * The result is a PairStream<String, Iterable<Double>> with                * 'stock symbol' as the key and 'stock prices' for that symbol within the window as the value.                 */
//  returns nil if doesn't exist 
//  these triggers will be retried as part of batch retries 
// Emit and ack some tuples, ensure that some polled tuples remain cached in the spout by emitting less than maxPollRecords 
//  nature of join   field for the current stream   field for the other (2nd) stream 
// if a new document should be inserted if there are no matches to the query filter  updateBolt.withUpsert(true); 
//  Schedule Nimbus inbox cleaner 
//  2 - Setup Topology  -------- 
//  Key has not been created yet and it is the first time it is being created 
//  cleanup thread killing topology in b/w assignment and starting the topology 
//  no offset commits have ever been done for this consumer group and topic-partition,   so start at the beginning or end depending on FirstPollOffsetStrategy 
//  The current executor we are trying to schedule 
//  TOTAL_TASKS 
//  METRIC_VALUE 
// find the smallest offset in toResend list 
//  if reached so far, add it to the set of messages waiting to be retried with next retry time based on how many times it failed 
// end of Test2 
//  onheap and offheap memory requirement 
// Emit the messages 
//  Wrapper class handy for the client code to use the JSON parser to build to use with JSON parser 
// <ExecutorDetails - Task, Map<String - Type of resource, Map<String - type of that resource, Double - amount>>> 
//  Remove it from failedPerShard anyway 
//  SERIALIZED_PARTS 
//  remove contiguous elements from the head of the heap 
//  merge and push unions rules 
//  test read 
// find the smallest offset in pending list 
//  create root directory if not exist 
//  shard iterator type based on kinesis api - beginning of time, latest, at timestamp are only supported 
// Log Writer Command... 
//  COMPONENT_TO_NUM_TASKS 
//  JSONAware not working for nested element on Map so write JSON format from here 
//  2) Join the streams in order of streamJoinOrder 
//  explicitly anchor emits to corresponding input tuples only, as default window anchoring will anchor them to all tuples   in window 
//  The following tests are run for both hdfs and local store to test the 
//  It covers scenarios expalined in scenario 3 when nimbus-1 holding the latest   update goes down before it is downloaded by nimbus-2. Nimbus-2 gets elected as a leader 
// end of Test3 
//  can be null for things like partitionPersist occuring off a DRPC stream 
//  key1 shouldn't be in iterator since it's marked as deleted 
// defaults to seconds 
//  the combination of the lock and the finished flag ensure that   an id is never timed out if it has been finished 
// NOTE: @IsImplementationOfClass(implementsClass = IStrategy.class) is enforced in DaemonConf, so   an error will be thrown by nimbus on topology submission and not by the client prior to submitting   the topology. 
/*                 * convert the state back to a stream and print the results                 */
//  make sure we've handled all supervisors on the host before we break 
//  EXEC_SUMMARY 
//  will be used instead 
//  WORKERS 
//  other members 
// Worker Command... 
// reset all the weights 
// When nid and zid are not equal, nid is attempting to impersonate zid, We 
//  offset was previously committed for this consumer group and topic-partition, either by this or another topology. 
// The user here from the jaas conf is bob.  No impersonation is done, so verify that 
//  Tar is not native to Windows. Use simple Java based implementation for   tests and simple tar archives 
//  This means we are pointing at a file. 
//  user configurable 
//  wildcard is given in file 
//  DETAILS 
//  check adding reference to local resource with topology of same name 
//  supervisor health check 
//  TOPOLOGY_RESOURCES_OVERRIDES 
// check if exec can be on worker based on user defined component exclusions 
// First verify that if something has a high load it's distribution will drop over time 
//  daemons can only be 'nimbus', 'supervisor', or 'worker' 
//  Ignore NoNodeExists exceptions because when sync() it may populate curr with stale data since   zookeeper reads are eventually consistent. 
//  Spout internals 
//  get tasks if the user is authorized for this topology 
//  check for sub-struct validity 
// this takes care of setting up coord streams for spouts and bolts 
//  Note: We don't return from this method on ParseException to avoid triggering the   spout wait strategy (due to no emits). Instead we go back into the loop and   generate a tuple from next file 
// This happens when the key is not found, the cache loader returns a null and this exception is thrown.   because the cache cannot store a null. 
//  Rack id to list of host names in that rack 
//  1) First re-emit any previously failed tuples (from retryList) 
// NImbus metrics distribution 
//  authz = authn 
// Check if the user is allowed to read this 
//  Now parse it and return the map. 
//  After this, resources should contain all the kinds of resources   we can count for the group. If we see a kind of resource in another   node not in resources.keySet(), we'll throw. 
// When using the no guarantee mode, the spout must commit tuples periodically, regardless of whether they've been acked 
// we expect to notify supervisors at almost the same time 
//  MASTER_CODE_DIR 
//  increment the fail count as we started with 0 
// When reading the conf in nimbus we want to fall back to our own settings 
// May be null if worker tokens are not supported by the thrift transport. 
//  Read 1 line 
// Expected 
// TODO 
// construct the final Assignments by adding start-times etc into it 
//  SHARED_MEMORY 
//  this is just in case supervisor is down so that disk doesn't fill up.   it shouldn't take supervisor 120 seconds between listing dir and reading it 
//  components 
//  For testing, be careful as it doesn't clone 
// If the write failed, try to sync anything already written 
// Schema change should have forced a rotation 
//  These can be chained (like with setting the CPU requirement) 
//  =============================================================================   ============================ getter main.methods =================================   =============================================================================
// add the nid as the real user in reqContext's subject which will be used during authorization. 
//  Always empty if processing guarantee is none or at-most-once 
//  set to keep 2 blobs (each of size 34) 
//  seek next offset after last offset from previous batch 
// This is not atomic (so if something bad happens in the middle we need to be able to recover 
// The unique topology id for the topology that created this metadata 
//  locate oldest expired lock file (if any) and take ownership 
// populating request context  
//  Check last block scheduling time does not get significantly slower 
//  null if it wasn't sampled 
//  expecting AlreadyBeingCreatedException inside RemoteException 
// now pending: [3], toResend: [1,2] 
//  can't leave choices to be empty, so initiate it similar as ShuffleGrouping 
//  min-heap 
//  Retries management 
//  close all the created hTable instances 
// The service must be able to remove retry schedules for unnecessary partitions 
// the default is false.  the default is false. 
//  use redis based state store for persistence 
//  EXPIRATION_TIME_MILLIS 
//  ackers==null when ackerCount not explicitly set on the topology 
//  on whether they are IBasicBolt or IRichBolt instances 
// This really should be impossible, because we go off of the min load, and inc anything within 5% of it.   But just to be sure it is never an issue, especially with float rounding etc. 
//  Therefore the timer in newAssignment won't be invoked 
/*      * performs a hash-join by constructing a hash map of the smaller set, iterating over the     * larger set and finding matching rows in the hash map.      */
//  if all go down which is unlikely. Hence there might be a need to update the blob if all go down. 
//  watermark interval 
//  We can't move this to outside without breaking backward compatibility. 
//  Tuples that were successfully acked/emitted. These tuples will be committed periodically when the commit timer expires, 
//  inner join - core implementation 
//  This should throw AuthorizationException because auth failed 
/*  IPersistentMap  */
// race condition with delete 
// The tick should cause tuple1 to be ack'd 
// The consumer should not be seeking to retry the failed tuple, it should just be continuing from the current position 
// Interrupted is thrown when we are shutting down.   So just ignore it for now... 
// Worker launched through external commands, hence we count their exceptions toward shell exceptions 
// JSTACK DUMP 
/*                 * The result of aggregation is forwarded to                * the RedisStoreBolt. The forwarded tuple is a                * key-value pair of (word, count) with ("key", "value")                * being the field names.                 */
// Waiting for spout tuples isn't strictly necessary since we also wait for bolt emits, but do it anyway  Allow two minutes for topology startup, then wait for at most the time it should take to produce 10 windows 
/*                  * NO-OP: the events are ack-ed in execute                  */
//  list files 
// worker.childopts validates 
//  We ignore workers that are still bound to a slot, which is monitored by a supervisor 
// all events are sent successfully, return last sent offset 
//  metric, timestamp, value, Map of tagK/tagV respectively. 
// host-topoId-port-fileName 
//  the first field is the batch id 
//  STATS 
// The writer must be closed before removed from the map.  If it failed, we might lose some data. 
//  realm is ignored 
//  -- clocks in sync 
// In some cases the new LocalAssignment may be equivalent to the old, but   It is not equal.  In those cases we want to update the current assignment to   be the same as the new assignment 
//  Validate a single task id return 
// Now also check that no more tuples are polled for, since both partitions are at their limits 
//  Force a send error 
//  Overloading the assertStoreHasExactly method accomodate Subject in order to check for authorization 
// null worker id means generate one... 
// There is a race on credentials where they can be leaked in some versions of storm. 
// play all tuples 
// this will fail the test since user derek does not have an entry for memory 
//  5   0   0   4   1 
//  MESSAGE_ID 
//  REBALANCE_OPTIONS 
//  512: for most scenes to avoid inner array resizing 
//  early return as no shard is assigned - probably because number of executors > number of shards 
//  acquire another lock on file1 and verify it failed 
// Commit offsets 
//  Catching and logging KeyNotFoundException because, if   there is a subsequent update and delete, the non-leader   nimbodes might throw an exception. 
// Nimbus Compatibility 
//  Returns either the source component name or the stream name for the tuple 
// We want to be able to select the measurement interval   reporting window (We don't need 3 different reports)   We want to be able to specify format (and configs specific to the format)   With perhaps defaults overall 
// Modifies justAssignedKeys 
// This is called async so lets assume that it is something we care about 
//  remove uploaded jars blobs, not artifacts since they're shared across the cluster   Note that we don't handle TException to delete jars blobs   because it's safer to leave some blobs instead of topology not running 
// get storm values and emit 
//  Complete the send 
// Initial delay for the commit and assignment refresh timers 
//  TODO enable if setStateSpout gets implemented      @Test(expected = IllegalArgumentException.class) 
/*                  * Here we don't set the tuples in windowedOutputCollector's context and emit un-anchored.                 * The checkpoint tuple will trigger a checkpoint in the receiver with the emitted tuples.                  */
//  storm configuration 
//  Looks for files in the directory with .current suffix 
/*                  * This test sends a broadcast to all connected clients from the server, so we need to wait until the server has registered                 * the client as connected before sending load metrics.                 *                 * It's not enough to wait until the client reports that the channel is open, because the server event loop may not have                 * finished running channelActive for the new channel. If we send metrics too early, the server will broadcast to no one.                 *                 * By waiting for the response here, we ensure that the client will be registered at the server before we send load metrics.                  */
//  Checks for assertion when we turn on security 
//  ################# Listener Implementation ###################### 
//  not required   not required 
// This will only get updated once 
//  determine how long to sleep from looking at ticket's expiry.   We should not allow the ticket to expire, but we should take into consideration   MIN_TIME_BEFORE_RELOGIN. Will not sleep less than MIN_TIME_BEFORE_RELOGIN, unless doing so   would cause ticket expiration. 
// 2 try to append to a closed file 
//  =====================================================================================   convert thrift stats to java maps   ===================================================================================== 
//  SHARED_RESOURCES 
// Make the spout commit any acked tuples 
//  USERS 
//  no heartbeat for this one, should be 0 
// clear up the kerberos state. But the tokens are not cleared! As per  the Java kerberos login module code, only the kerberos credentials 
//  copy constructor 
// Writes the offsets in the new format to the /user partitions paths 
// specify a configuration object to be used 
// The spout should emit at most one message per call to nextTuple  This is necessary for Storm to be able to throttle the spout according to maxSpoutPending 
// reserved for future 
//  same set of events part of three windows 
//         bd.shuffleGrouping(SPOUT_ID); 
//  a stream of stock quotes 
//  If MapFunction is aware of cleanup, let it handle cleaning up 
//  stream name unspecified 
//  topo3 has 4 large tasks 
//  SUCCESS 
//  set watermark interval to a high value and trigger manually to fix timing issues 
// fail 1st tuple 
//  Tuples that have been emitted but that are "on the wire", i.e. pending being acked or failed. 
// Ensure Nimbus has leadership, otherwise topology submission will fail. 
//  ignore... 
//  if this latch is closed, we need to create new instance. 
//  actually Map<String, Map<String, Map<String, Long/Double>>> 
//  Objects are absent if they were zero both this iteration   and the last -- if only this one, we need to report zero. 
// with realm e.g. hdfs@WITZEND.COM   /etc/security/keytabs/storm.keytab 
// We get a new random number and seed it to make sure that runs are consistent where possible. 
//  if tuple arrives from a spout, it can be passed as is   otherwise the value is in the first field of the tuple 
//  TOTAL_TOPOLOGIES 
//  simulate the time trigger by setting the reference time and invoking onTrigger() manually 
//  returns paused topic-partitions. 
//  The free pool never has anything running 
// The time is now twice the message timeout, the second tuple should expire since it was not acked 
//  Define our taskIds 
//  wrap KeyNotFoundException in an InvalidTopologyException 
//  user class supplied...   this also provides a bridge to Trident... 
// Trigger manually to avoid timing issues 
//  EXECUTOR_ID 
// Create links to artifacts dir 
//  other classes from config 
//  0 means delegate batch size = trident batch size. 
// Initial delay for the assignment refresh timer 
//  names 
//  wait for lock to expire 
//  If that fails, use config 
// Verify simple is rejected... 
//  filter ExecutorSummary's with empty stats 
//  backward compatibility 
//  Verify recorded messages size metrics  
// 2 on first partition, 0-2 on second partition 
//  Ranked third since rack-4 has a lot of cpu but not a lot of memory 
//  FRAGMENTED_CPU 
/*              * Last batch meta is null, but this is not the first batch emitted for this partition by this emitter instance. This is             * a replay of the first batch for this partition. Use the offset the consumer started at.              */
//  Don't start new requests if there is an exception 
// We require there to be both a topology and a component in this case, so parse it out as such. 
//  There is one task inside one executor for each worker of the topology.   TaskID is always -1, therefore you can only send-unanchored tuples to co-located SystemBolt.   This bolt was conceived to export worker stats via metrics api. 
//  now scan all metadata and remove any matching string Ids from this list 
//  Ignored 
// Save the memory limit so we can enforce it less strictly 
//  executor resources 
// We could not recover container will be null. 
// returns null if it's not a drpc group 
//  pump more msgs than Q size & verify msg count is as expexted 
//  check negative resource count 
// ack 2nd tuple 
//  Log any info sent on the error stream 
//  Fill the 2nd half with new bytes from the stream. 
// A DRPC token only works for the invocations transport, not for the basic thrift transport. 
//  (merge-with merge-agg-comp-stats-topo-page-bolt/spout (acc-stats comp-key) cid->statk->num) 
// Map the key if needed 
//  2 -  Setup DevNull Bolt   -------- 
//  failed_with_exit_code is OK. We're mimicing Hadoop's health checks.   We treat non-zero exit codes as indicators that the scripts failed   to execute properly, not that the system is unhealthy, in which case   we don't want to start killing things. 
// Fail both emitted tuples 
//  there's not enough bytes in the buffer. 
//         oneProducer1Consumer();          twoProducer1Consumer();          threeProducer1Consumer();          oneProducer2Consumers();          producerFwdConsumer(); 
// Should be allowed to retry 3 times, in addition to original try 
//  check allowedWorkers only if the scheduler is not the Resource Aware Scheduler 
//  check that tryPublish() & tryOverflowPublish() work as expected 
// Put in a tuple to cause the first tuple to be acked 
//  failures happen you don't get an explosion in memory usage in the tasks 
//     httpServer.destroy();  } 
// Seek directly to the earliest retriable message for each retriable topic partition 
//  Hide the dead-ports from the all-ports   these dead-ports can be reused in next round of assignments 
// Resets the last access time for key1 
//  ACTION 
//  A singleton instance allows us to mock delegated static main.methods in our
/*          * When we click a link to the logviewer, we expect the match line to be somewhere near the middle of the page. So we subtract half         * of the default page length from the offset at which we found the match.          */
// generate topologies 
// Allowing keytab based login for backward compatibility. 
//  OWNER 
//  map of tag value pairs 
/*  * Just for testing purpose. After the migration of testing.clj. This class could be removed.  */
// Only 1 rack is in use  r001 is the second rack with GPUs  r000 is the first rack with no GPUs 
//  used in worker only, keep it as a latch 
//  throws ParseException. Effectively produces 3 lines (1,2 & 3) from each file read 
//  RAS (Resource Aware Scheduler) 
//  Starting empty 
// no ack, so return the first of pending list 
// storm configuration 
//  REQUESTED_SHARED_OFF_HEAP_MEMORY 
//  highest sequence number that can be committed for this shard 
// The batch size can be no larger than half the full recvQueue size, to avoid contention issues. 
// exact variable time, that is added to the current bucket 
//  COMPLETE_MS_AVG 
//  events 8, 9, 10 should not be scanned at all since TimeEvictionPolicy lag 5s should break 
// get it from cluster state/zookeeper every time to collect the UI stats, may replace it with other StateStore later 
//  Note that allSlotsAvailableForScheduling   only uses the supervisor-details. The rest of the arguments   are there to satisfy the INimbus interface. 
//  the key and value of txIds are guaranteed to be converted to UTF-8 encoded String 
//  clear workers off all hosts that are not blacklisted 
//  Creating blob again before launching topology 
//  ASSIGNED_MEMONHEAP 
// Commit 
// user jerry submits topo2 
//  optional   optional   optional   optional   optional   optional 
//  10,000,000 records => 80MBs of memory footprint in the worst case 
//  This is updated by the Worker and the topology has shared access to it 
// we need to release resources associated with the worker event loop group 
//  assertion 
// no-op 
//  protected using the Object Lock 
//  distribution should be even for all nodes when loads are even 
//  implementation for converting a Kinesis record to a storm tuple 
//  A callback that does nothing. 
// In case deactivate was called before 
//  close the socket, which releases connection if it has created any. 
//  remove any entries in the cache 
//  (acc-stats comp-key) ==> bolt2stats/spout2stats 
//  Update current key list inside the blobstore if the version changes 
//  lock protects against multiple topologies being submitted at once and 
//  WINDOW_TO_FAILED 
//  one or more column families 
//  SUPERVISOR_ID 
//  2) wait for all 3 locks to expire then heart beat on 2 locks 
//  create sequence format instance. 
//  blob key not specified, use file 
//  1 lock log entry every 2 tuples   Effectively disable commits based on time 
//  TODO: finish 
// this is to prevent the potential bug that  if the Login Cache is (1) enabled, and then (2) disabled and then (3) enabled again,  and if the LoginCacheKey remains unchanged, (3) will use the Login cache from (1), which could be wrong,  because the TGT cache (as well as the principle) could have been changed during (2) 
//  Some other form of Unix 
// element sojourn time in milliseconds 
//  Suppressing exceptions as we don't care for errors on abort 
// need to get the next node iterator 
//  Creating a blacklist file to read from the disk 
// TreeSet uses compareTo instead of equals() for the Set contract  Ensure that we can save two retry schedules with the same timestamp 
//  thread object 'thread' will be null if a refresh thread is not needed. 
//  Encoder 
//  Don't ack tick tuples 
// Don't let the user set who we launch as 
//  acls for the blob are set to WORLD_EVERYTHING 
//  4)  --- Create another input file and reverify same behavior --- 
//  setup default Producer 
//  pregenerate commonly used keys for scans 
//  if SASL authentication is disabled, saslChannelReady is initialized as true; otherwise false 
//  number of values must be odd to compute median as below 
// NOOP, if windows gets support for run as user we will need to find a way to support this 
//  wordSpout ==> countBolt ==> RedisBolt 
//  RESET_LOG_LEVEL_TIMEOUT_EPOCH 
//  storm blobstore create --file blacklist.txt --acl o::rwa key 
//  3 - Setup Topology  -------- 
//  EMITTED 
// the body of the message is "message" + currentOffset, e.g. "message123" 
//  object capturing all zk related information for storing committed sequence numbers 
//  there's a race condition with a delete: either blobstore or blobstoremaxsequence   this should be thrown to the caller to indicate that the key is invalid now 
//  HOSTNAME 
//  Stop services without killing the process instead. 
//  $PATH should be defined on most systems 
//  setup ZK 
// No backoff for test retry service, just check that messages will retry immediately 
// just need an id 
//  Javac option: remove these when the javac zip impl is fixed   (http://b/issue?id=1822932) 
// Both assignments are null, just wait 
/*         For each partition the spout is allowed to retry all tuples between the committed offset, and maxUncommittedOffsets ahead.        It must retry tuples within that limit, even if more tuples were emitted.          */
//  for local cluster 
// Check that null meta makes the spout seek to EARLIEST, and that the returned meta is correct 
//  messages[next] == null can happen if we lost the connection and subsequently reconnected or timed out. 
//  emits sliding window and global averages 
// Topologies that were deemed to be invalid 
// in addition, add all the owners with guarantees 
//  or after a consumer rebalance, or during close/deactivate. Always empty if processing guarantee is none or at-most-once. 
// for local test 
//  window should be compacted and events should be expired. 
//  add more events with current ts 
//  The manually set STORM_WORKER_CGROUP_MEMORY_MB_LIMIT config on supervisor will overwrite 
//  left join - core implementation 
//  returns true if pendingEmits is empty 
//  now ack msg 5 and check 
// Race with delete  If it is not here the replication is 0  
//  ignore 
// This needs to be thread safe 
//  In the worst case we will return a serialized name after a password provider said that the password   was okay.  In that case the ACLs are likely to prevent the request from going through anyways. 
//  enable blobstore acl validation 
//  NODE_INFO 
//     Metrics 
// Tests that isScheduled, isReady and earliestRetriableOffsets are mutually consistent when there are messages from multiple partitions scheduled 
// This might be a partial key grouping.. 
//  make sure resources dir is created. 
//  make sure we support different user reading same blob 
//  validate search by metric id 
// A map of node ids to node objects 
//  pass cases 
// construct a transport plugin 
//  no reference to key1 
//  initialize slots for this node 
//  Quoting Javadoc in File.listFiles(FilenameFilter filter):   Returns {@code null} if this abstract pathname does not denote a directory, or if an I/O error occurs. 
//  we're making mock, ignoring... 
// Expire the token 
// Create links to blobs 
//  Maps transaction Ids to JMS message ids. 
//  Testing whether acls are set to WORLD_EVERYTHING, Here we are testing only for LocalFsBlobstore   as the HdfsBlobstore gets the subject information of the local system user and behaves as it is   always authenticated. 
//  Define our taskIds - the test expects these to be incrementing by one up from zero 
//  for kryo 
// static to ensure eventhough the class is created using reflection we can still get  the topology to actions 
/*      * Convenience method for registering ReducedMetric.      */
//  one instance per executor avoids false sharing of CPU cache 
//  FUNCTION_NAME 
//  there's a race condition with a delete: blobstore   this should be thrown to the caller to indicate that the key is invalid now 
//  deletes metrics matching the filter options 
// must specify column schema when providing custom query. 
//  case 4: task Message 
//  populate a metric 
//  The false parameter ensures overwriting the version file, not appending 
// Since the last tuple on the partition is more than maxPollRecords ahead of the failed tuple, it shouldn't be emitted here 
//  if ACLs have two user ACLs for empty user and principal, discard empty user ACL 
//  perform a scan given filter options, and return results in either Metric or raw data. 
//  function called on timer to reset log levels last set to DEBUG 
//         oneProducer1Consumer(1000);  // -- measurement 1          twoProducer1Consumer(1000);    // -- measurement 2          threeProducer1Consumer(1);   // -- measurement 3 
// $set operator: Sets the value of a field in a document. 
//  schedule last block (90% to 100%) 
//   Submit to Storm cluster 
//  batch 3 replayed with 40 tuples 
// add the authNid as the real user in reqContext's subject which will be used during authorization. 
//  return null if it's not single emit 
//  (merge-with (partial merge-with sum-or-0) acc-out spout-out) 
//  0) config spout to log progress in lock file for each tuple 
// defaults to 10 
/*      * the value of m is as follows:     * <pre>     * #org.apache.storm.stats.CommonStats {     *  :executed {     *      ":all-time" {["split" "default"] 18727460},     *      "600" {["split" "default"] 11554},     *      "10800" {["split" "default"] 207269},     *      "86400" {["split" "default"] 1659614}},     *  :execute-latencies {     *      ":all-time" {["split" "default"] 0.5874528633354443},     *      "600" {["split" "default"] 0.6140350877192983},     *      "10800" {["split" "default"] 0.5864434687156971},     *      "86400" {["split" "default"] 0.5815376460556336}}     * }     * </pre>      */
// These should match FixedAvroSerializer.config in the test resources 
/*      * Write a String array as a Nework Int N, followed by Int N Byte Array of     * compressed Strings. Handles also null arrays and null values.     * Could be generalised using introspection.     *      */
//  NUM_WORKERS 
//  Wait for 'ready' (channel connected and maybe authentication) 
//  required for instantiation via reflection. must call prepare() thereafter 
// this will do best effort flushing since the linger period   was set on creation 
//  second predicate for or condition uses the fact that long addition over the limit circles back 
// Yes eat the exception 
// This technically does not conform with rfc1964, but should work so   long as you don't have any really odd names in your KDC. 
//  invalidate the iterator 
// only put this owner to the map 
//  example. spout1: generate random strings   bolt1: get the first part of a string   bolt2: output the tuple 
//  and try to renew the ticket. 
//  Re-load from cached' file. 
//  30 seconds 
// Just throw it away in local mode 
//  fail cases 
// First off we want to verify that ROOT is good 
//  -- max outstanding tuples 
// Nimbus groups admin 
// In almost all cases these should be the same, but warn the user just in case something goes wrong... 
// Fail all emitted messages except the last one. Try to commit. 
//     public SimpleTridentHBaseMapper withTimestampField(String timestampField){          this.timestampField = timestampField;          return this;      } 
// In some cases users will want to drop retrying old batches, e.g. if the topology should start over from scratch.  If the FirstPollOffsetStrategy ignores committed offsets, we should not retry batches for old topologies  The batch retry should be skipped entirely 
// Run until Ctrl-C 
//  automatically turn it into a batch spout, should take parameters as to how much to batch      public Stream newStream(IRichSpout spout) {          Node n = new SpoutNode(getUniqueStreamId(), TridentUtils.getSingleOutputStreamFields(spout), null, spout, SpoutNode   .SpoutType.BATCH);          return addNode(n);      } 
// now login 
/* Will only serialize AMQPValue type */
//  It is necessary that this produce a deterministic assignment based on the key, so seed the Random from the key 
//  given (for this iteration) 
//  check for required fields 
// Global Grouping is fields with an empty list 
//  -- spout id 
//  offset and messageId are used interchangeably 
//  O(Log N) 
//  msec 
//  4 - Print metrics every 30 sec, kill topology after 20 min 
// This histogram reflects the data distribution across only one ClusterSummary, i.e.,   data distribution across all entities of a type (e.g., data from all nimbus/topologies) at one moment.   Hence we use half of the CACHING_WINDOW time to ensure it retains only data from the most recent update 
//  blocking call under the hood, must invoke after launch cause some services must be initialized 
// start the threads 
/*          * The inputWindow gives a view of         * (a) all the events in the window         * (b) events that expired since last activation of the window         * (c) events that newly arrived since last activation of the window          */
//  MEM_ON_HEAP 
// The old token could not be deserialized.  This is bad, but we are going to replace it anyways so just keep going. 
//  then (for this iteration) 
// in case we didn't fill in enough 
// Cycle spout activation 
//  Check for latest sequence number of a key inside zookeeper and return nimbodes containing the latest sequence number 
//  we need to download to temp file and then unpack into the one requested 
//  add identity partitions between groups 
// Acking tuples for partitions that are no longer assigned is useless since the spout will not be allowed to commit them 
//  read few lines from file1 dont ack 
//  Check for Blobstore with authentication 
/*      * key1 -> (val1, val2 ..)     * key2 -> (val3, val4 ..)      */
//  1 -  Setup Hdfs Spout   -------- 
//  read lines 3..7, don't ACK .. commit pos should remain same 
// if playing from the repl and defining functions, file won't exist 
// Under RAS the number of workers is determined by the scheduler and the settings in the conf are ignored  conf.setNumWorkers(3); 
// conf.put(Config.TOPOLOGY_STATE_PROVIDER, "org.apache.storm.redis.state.RedisKeyValueStateProvider"); 
//  check for null which can exist because of a race condition in which nimbus nodes in zk may have been   removed when connections are reconnected after getting children in the above line 
//  SPECIFIC_STATS 
//  Register file cleanup after jvm shutdown 
//  TODO conditionally load properties from a file our resource 
//  We don't need to sleep here because the IPartitionManager.receive() is   a blocked call so it's fine to call this function in a tight loop. 
//  Triggers when an assignment should be refreshed 
//  2 - if clocks are in sync then simply take ownership of the oldest expired lock 
//  first sync assignments to local, then sync processes. 
//  SAMPLINGPCT 
//  Check for new file every so often 
// We waited for 1 second loop around and try again.... 
//  topo3 should not be able to scheduled 
//  create a new config. Make it additive (true) s.t. inherit parents appenders 
// prints the total with low probability. 
// if this task containing worker will be killed by a assignments sync,  taskToNodePort will be an empty map which is refreshed by WorkerState 
//  ALL 
//  class JoinInfo 
// Supervisor metrics distribution 
//  inner join of 'age' and 'gender' records on 'id' field 
// With uncommitted earliest the spout should pick up where it left off when reactivating. 
//  Test1: If RAS spreads executors across multiple workers based on the set limit for a worker used by the topology 
/* filter supervisor */
//  DO NOT include the success stream as part of the batch. it should not trigger coordination tuples,   and is just a metadata tuple to assist in cleanup, should not trigger batch tracking 
//  Load PMML Model from File 
//  this code here handles a case where a previous commit failed, and the partitions   changed since the last commit. This clears out any state for the removed partitions   for this txid.   we make sure only a single task ever does this. we're also guaranteed that   it's impossible for there to be another writer to the directory for that partition   because only a single commit can be happening at once. this is because in order for   another attempt of the batch to commit, the batch phase must have succeeded in between.   hence, all tasks for the prior commit must have finished committing (whether successfully or not) 
//  SHELL 
// Clean up some things the user should not set.  (Not a security issue, just might confuse the topology) 
//  prevent timer to check heartbeat based on last thing before activate 
//  DATA 
//  only holds msgs from other workers (via WorkerTransfer), when recvQueue is full 
// HEAP DUMP 
//  Wait for all tasks to complete 
// ack rest 
// Check that only two message ids were generated 
//  Login will sleep until 80% of time from last refresh to   ticket's expiry has been reached, at which time it will wake 
// above for-loop has closed all the writers. It's safe to clear the map here. 
//  get executor heartbeat 
// name list is empty, return an empty map 
/* include sys (should not matter) */
//  2- BP detected (i.e MainQ is full). So try adding to overflow 
//  Finds the metadata string that matches the string Id and type provided.  The string should exist, as it is 
//  validate search by time 
//  checks if the tasks which had back pressure are now free again. if so, sends an update to other workers 
//   Submit topology to Storm cluster 
//  FRAGMENTED_MEM 
//  Distributed mode 
// We need to read a new one 
//  SID_TO_OUTPUT_STATS 
// return the smaller of pending and toResend 
//  ASSIGNED_REGULAR_OFF_HEAP_MEMORY 
// The spout must respect maxUncommittedOffsets when requesting/emitting tuples 
// 1 day values 
//  TOPOLOGY_CONF_OVERRIDES 
//  get num_executors 
// By default this is a NOOP 
//  Normalize state 
//  JAVA_OBJECT 
//  for an acked message add it to acked set and remove it from emitted and failed 
//  so we can track when it was last used for later deletion on database cleanup. 
//  avoid buffering 
//  the new-timeouts map now contains logger => timeout 
//  4 try locking again 
// construct THsHaServer 
//  VERSION 
//  if the acked message was in emittedPerShard that means we need to remove it from the emittedPerShard (which 
//  Serializes a Java object to JSON 
//  Resume polling at the last committed offset, i.e. the first offset that is not marked as processed. 
//  invalid key, remove it from blobstore 
//  =====================================================================================   thriftify stats main.methods   =====================================================================================
//  wordSpout ==> countBolt ==> MongoUpdateBolt 
//  size if IDENTIFIER 
//  calc sid->output-stats 
// NOOP 
// Ranked last since rack-5 has neither CPU nor memory available 
//  If FlatMapFunction is aware of cleanup, let it handle cleaning up 
//  Construct a message containing the SASL response and send it to the 
//  this bolt does not emit tuples 
// Don't try to move the JAR file in local mode, it does not exist because it was not uploaded 
// Nothing is scheduled here so throw away all of the profileActions 
// logger.info("emitted new batches: " + listEvents.size()); 
//  not used, placeholder for GUI, etc. 
// Extend the config with defaults and the command line 
//  setting value to any non-null string 
// any stop profile actions that hadn't timed out yet, we should restart after the worker is running again. 
// Remove existing schedule for the message id 
//  totals 
/*      * Asserts that commitSync has been called once,      * that there are only commits on one topic,     * and that the committed offset covers messageCount messages      */
//  STORM_ASSIGNMENT 
//  find homedir 
/*              * Delete the current index file and rename the tmp file to atomically             * replace the index file. Orphan .tmp files are handled in getTxnRecord.              */
//  validate search by host id 
// Avoid case of different blob version   when blob is not downloaded (first time download) 
//  iterate the tuples 
//  create thread to delete old metrics and metadata 
//  we want to register a topo directory getChildren callback for all workers of this dir 
//  e.g.:  1,2,3,4,10,11,12,15  =>  4,10,11,12,15 
//  expecting this exception 
//  case 3: BackPressureStatus 
//  METRIC_LIST 
// topology.worker.childopts validates 
// Give the topology time to come up without using it to wait for the spouts to complete 
/* include sys */
// Sometimes external things used with testing don't shut down all the way 
// key is supervisor key ,value is supervisor ports 
//  run aggregator to compute the result 
// This is a NOOP 
//  if re-partitioning is involved, does a per-partition reduce by key before emitting the results downstream 
//  if no configs from user-resource-pools.yaml, get configs from conf 
// password 
//  set acl so user doesn't have read access 
//  scans from key start to the key before end, calling back until callback indicates not to process further 
//  If unsuccesful fail the pending tuples 
// This test is in two phases.  The first phase fills up the 10 buckets with 10 tuples each 
//  reset property 
//  version 
//  PULSE_IDS 
// This method enables the metrics to be accessed from outside of the JCQueue class 
//  verify lock file location   verify lock filename 
//  pass in componentToSortedTasks for the case of running tons of tasks in single executor 
//  an acked message should not be in failed since if it fails and gets re-emitted it moves to emittedPerShard   from failedPerShard. Defensive coding. 
// All null tuples should be commited, meaning they were considered by to be emitted and acked 
//  The start index is positioned to find any possible   occurrence search string that did not quite fit in the   buffer on the previous read. 
// generate another rack of supervisors with less resources 
// Scheduling changed while running... 
//  load from state 
//  boolean to indicate whether timer is active 
//  a valid javac XD option, which is another bug 
//  A map of the worker to the components in the worker to be able to enforce constraints. 
// any receive call after exceeding max pending messages results in null 
// invoke service handler 
//  Finally delete any baseName.<VERSION> files that are not pointed to by the current version 
//  Choosing atmost 5 words to update the blacklist file for filtering 
//  Assume the recvQueue is stable, in which the arrival rate is equal to the consumption rate.   If this assumption does not hold, the calculation of sojourn time should also consider   departure rate according to Queuing Theory. 
//  SERIALIZED_JAVA 
//  Generate SASL response to server using Channel-local SASL client. 
// Gets Nimbus Subject with NimbusPrincipal set on it 
//  TOPOLOGY_STATUS 
/*  We take the max of the default and whatever the user put in here.           Each node's resources can be the sum of several operations, so the simplest           thing to do is get the max.           The situation we want to avoid is that the user sets low resources on one           node, and when that node is combined with a bunch of others, the sum is still           that low resource count. If any component isn't set, we want to use the default.           Right now, this code does not check that. It just takes the max of the summed           up resource counts for simplicity's sake. We could perform some more complicated           logic to be more accurate, but the benefits are very small, and only apply to some           very odd corner cases.  */
/*  Indexed  */
// Unblock downloading by accepting the futures. 
//  this finds all dependency blob keys from active topologies from all local blob keys 
//  NIMBUSES 
// Emit all messages and check that they are emitted. Ack the messages too 
// rest of jerry's running topologies 
// Since we asked for tuples starting at seekOffset, some retriable records must have been compacted away.  Ack up to the first offset received if the record is not already acked or currently in the topology 
//  always retain resources in use 
// Update worker tokens if needed 
//  Consumer 
/*  Not a Blocking call. If cannot emit, will add 'tuple' to pendingEmits and return 'false'. 'pendingEmits' can be null  */
// cannot be (cpuResourcePoolUtilization + memoryResourcePoolUtilization)/2  since memoryResourcePoolUtilization or cpuResourcePoolUtilization can be Double.MAX_VALUE  Should not return infinity in that case 
// The failed tuples are ready for retry. Make it appear like 0 and 1 were compacted away. 
//  helpful for debugging tests 
//  WORKER_RESOURCES 
//  else use the exponential backoff logic and handle long overflow 
//  complete access to the blob 
//  STREAMS 
//  Reasonable size for a simple .class. 
// In order to avoid going over _maxNodes I may need to steal from   myself even though other pools have free nodes. so figure out how   much each group should provide 
//  =================  Factory Methods Declaring ModelOutputs to Default Stream  ================== 
//  persist the window in state 
//  Hdfs related settings 
// race condition with a delete 
//  TIME_SECS 
// 3 hour values 
//  No-op 
//  gets min/max task pairs (executors): [1 1] [2 3] ... 
// Tick should have flushed it 
//  Test for subject with no principals and acls set to WORLD_EVERYTHING 
//  create empty files in filesDir 
// Initialize a worker slot for every port even if there is no assignment to it 
//  explicit delete for ephmeral node to ensure this session creates the entry. 
//  for global cleanup   for an active worker's dir, make sure for the last "/" 
//  repartition so that state query fields grouping works correctly. this can be optimized further 
// The LogWriter in turn launches the actual worker. 
//  If the config consists of a single key 'config', its values are used   instead. This means that the same config files can be used with Flux   and the ConfigurableTopology. 
//  offset was not committed by this topology, therefore FirstPollOffsetStrategy applies   (only when the topology is first deployed). 
//  This is likely to happen when we try to commit something that   was cleaned up.  This is expected and acceptable. 
// cred-update-lock is not needed here because creds are being added for the first time. 
//  required   required   optional   optional   optional   optional   optional   optional   optional   optional 
//  ==================   Get PMML Model from Blobstore ================== 
/*      * Bolt-specific configuration for windowed bolts to specify the name of the field in the tuple that holds     * the message id. This is used to track the windowing boundaries and avoid re-evaluating the windows     * during recovery of IStatefulWindowedBolt      */
//  create metric for memory 
//  Kafka 
//  delete and recreate lock file   returns false if somebody else already deleted it (to take ownership) 
//  Maps a storm tuple to redis key and value 
// this will fail since jerry doesn't have either cpu or memory entries 
// Remove/Clean up changed requests that are not for us 
//  bolt that subscribes to the intermediate bolt, and auto-acks 
//  1) Delete absent file - should return false 
//  TRANSFERRED 
//  Test with a dummy test_subject for cases where subject !=null (security turned on) 
//  should synchronize supervisor so it doesn't launch anything after being down (optimization) 
//  default parallelism to 1 so if it's omitted, the topology will still function. 
//  pendingCommit has no entries 
//  query the streamState for each input task stream and compute recoveryStates 
//  required   optional   required 
//  reset for next run 
//  If that fails, fall back on the multitenant-scheduler.yaml file 
//  Back off 
//  SETTABLE 
//  if first retry then retry time  = current time  + initial delay 
// on windows, the host process still holds lock on the logfile 
//  each evicted partition has MAX_EVENTS_PER_PARTITION 
// The response should be empty, since you should not be able to list files outside the worker log root. 
//  TOTAL_EXECUTORS 
// Try to create the parent directory, may not work 
// Simulate time starts out at 0, so we are going to just leave it here. 
//  optional   optional 
//  required   optional   optional   optional 
//  disable it   log every 2 sec 
//  Handles tuple events (emit, ack etc.) 
//  BOOLVAL 
// Only log accesses that fetched something 
//  make sure that the error thread exits 
/*  LOOK AT HDFS BLOBSTORE AGAIN  */
//  check avoids multiple log msgs when spinning in a idle loop 
//  add 10 events 
// create test DNSToSwitchMapping plugin 
//  for the currently tested assignment a Map of the node to the components on it to be able to enforce constraints 
//  otherwise, tuples were emitted directly 
//  expiry is before next scheduled refresh). 
// If the system still has some free memory give them a grace period to 
//  Tuple contains String Object in JSON format   Tuple contains Java object that must be serialized to JSON by SolrJsonMapper 
//  topo4 has 12 small tasks, whose mem usage does not exactly divide a node's mem capacity 
//  bolt1, bolt2 should also receive from checkpoint spout 
//  Cache the msgs grouped by destination node 
//  Add unique identifier to each tuple, which is helpful for debugging 
//  The redis bolt (sink) 
/*                  * Since this is a tumbling window calculation,                 * we use all the tuples in the window to compute the avg.                  */
//  respectively. 
//  sleep for 10 seconds. 
//  create an array of the right type 
// Storm will try to get metrics from the spout even while deactivated, the spout must be able to handle this 
//  Initialization is only complete after the first call to  KafkaSpoutConsumerRebalanceListener.onPartitionsAssigned() 
//  Mock failure 
// update nextOffset; 
//  Based on how Java handles the classpath   https://docs.oracle.com/javase/8/docs/technotes/tools/unix/classpath.html 
//  ordered partition keys 
//  This should mean that we were pointed at a directory. 
//  renames files and returns the new file path 
//  leave the acked offsets and consumer position as they were to resume where it left off 
// Test for a user having read or write or admin access to read replication for a blob 
// Instead the scheduler lets you set the maximum heap size for any worker. 
/*      * Test time to schedule large cluster scheduling with fragmentation      */
//  when (for this iteration) 
//  can happen during shutdown of drpc while topology is still up 
//  Map[StreamName -> JoinInfo] 
//  size of the resource 
//  Leadership coordination may be incomplete when launchServer is called. Previous behavior did a one time check   which could cause Nimbus to not process TopologyActions.GAIN_LEADERSHIP transitions. Similar problem exists for 
//  should promote: only fetch storm bases of topologies that need scheduling. 
//  for faster insertion to RocksDB. 
//  Map[StreamName -> Map[Key -> List<Tuple>]  ] 
//  move of tmp to current so that the operation is atomic. 
//  SUPERVISOR_SUMMARIES 
// This needs to be appropriately large to drown out any time advances performed during topology boot 
//  not scheduled <=> never failed (i.e. never emitted), or scheduled and ready to be retried 
// If the node does not exist, then the version must be 0 
//  Wait interfal for retrying after first failure 
// Only add topologies that are not sharing nodes with other topologies 
//  read and ack remaining lines 
/*  CREATE THE BLOBSTORES  */
/*  Seqable  */
// new topology needs to be scheduled.  topo-4 should be evicted. Even though topo-1 from user jerry is older, topo-1 will not be evicted 
/* with auth */
//  warm up 60 seconds 
//  number of evicted events 
//  add in spouts as groups so we can get parallelisms 
//  1 -  Setup Kafka Spout   -------- 
//  Offset management 
// have the new credentials (pass it to the LoginContext constructor) 
// Check to see if the CGroup is mounted at all 
// emit cross-join of all emitted tuples 
/*  COPY EVERYTHING FROM LOCAL BLOBSTORE TO HDFS  */
// Check that the spout will reemit all 3 failed tuples and no other tuples 
//  Every executor has an instance of this class 
//  CUSTOM_OBJECT 
// Test substitution where the target type is List 
//  implementation for handling the failed messages retry logic 
/*          * The KafkaConsumer commitSync API docs: "The committed offset should be the next message your application will consume, i.e.         * lastProcessedMessageOffset + 1. "          */
//  should not throw 
//  Log the connection error only once 
//  try to get BlobMeta   This will check if the key exists and if the subject has authorization 
// TODO batch updating 
//  Configured for achieving max throughput in single worker mode (empirically found).    For reference : numbers taken on MacBook Pro mid 2015      -- ACKer=0:  ~8 mill/sec (batchSz=2k & recvQsize=50k).  6.7 mill/sec (batchSz=1 & recvQsize=1k)      -- ACKer=1:  ~1 mill/sec,   lat= ~1 microsec  (batchSz=1 & bolt.wait.strategy=Park bolt.wait.park.micros=0)      -- ACKer=1:  ~1.3 mill/sec, lat= ~11 micros   (batchSz=1 & receive.buffer.size=1k, bolt.wait & bp.wait =   Progressive[defaults])      -- ACKer=1:  ~1.6 mill/sec, lat= ~300 micros  (batchSz=500 & bolt.wait.strategy=Park bolt.wait.park.micros=0) 
// insure that if keytab is used only one login per process executed 
//  0 means DEFAULT_EVENT_LOOP_THREADS 
// test the happy path, emit batches in sequence 
//  print the values to stdout 
//  wait strategy when the netty channel is not writable 
// The following come from the JVm Specification table 4.4 
//  Called by flush-tuple-timer thread 
//  if an ack is received for a message then add it to the ackedPerShard TreeSet. TreeSet because while   committing we need to figure out what is the 
//  get trigger count value from store 
// Passed to workers in local clusters, exposed by thrift server in distributed mode 
//  Topology will not be able to be successfully scheduled: Config TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB=128.0 < 129.0,   Largest memory requirement of a component in the topology). 
//  IGroupMappingServiceProvider  
//  netty TimerTask is already defined and hence a fully   qualified name 
//  executor id is in form [start_task_id end_task_id] 
//  get per task components 
//  BOLTS 
/*                  * Verify that some ticks are received. The interval between ticks is validated by the bolt.                 * Too few and the checks will time out. Too many and the bolt may crash (not reliably, but the test should become flaky).                  */
//         oneProducer2Consumers();     // -- measurement 4 
//  add more events with a gap in ts 
// Error should not be leaked according to the code, but they are not important enough to fail the build if 
//  SUPERVISORS 
//  end of Test1 
// A copy of cluster that we can modify, but does not get committed back to cluster unless scheduling succeeds 
//  The jitter allows the clients to get the data at different times, and avoids thundering herd 
// Ranked last since rack-2 has not cpu resources 
//  read 1st line and ack 
//  FileContext supports atomic rename, whereas FileSystem doesn't
// 3 seconds in milliseconds 
//  Use default, Storm-generated file names 
//  nothing expired yet 
//  should pass now 
//  filtered out 
//  sometimes Leader election indicates the current nimbus is leader, but the host was recently restarted,   and is currently not a leader. 
//  show a progress bar so we know we're not stuck (especially on slow connections) 
//  end of Test2 
//  TODO timestamps 
// ignore 
// the timeout thread handling 
// No slots to schedule for some reason, so skip it. 
//  end of Test3 
//  PARALLELISM_HINT 
//  Commit offsets that are ready to be committed for every topic partition 
//  It's likely that Bolt is shutting down so no need to throw RuntimeException   just ignore 
//  create empty file 
//  Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because   the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have   complete access to the blob 
//  this assumes that inFields and outFields are the same for combineragg   assumption also made above 
//  converts a metadata string into a unique integer.  Updates the timestamp of the string 
//  if there will be legacy values they will be in the outer conf 
// wait until all workers, supervisors, and nimbus is waiting 
//  Construct a groups mapping for the FixedGroupsMapping class 
//  This is a test where we are configured to point right at an artifact dir 
//  2nd cond prevents staying stuck with consuming overflow 
/*                 * The elements having the same key within the window will be grouped                * together and their values will be reduced using the given reduce function.                *                * Here the result is a PairStream<String, Double> with                * 'stock symbol' as the key and the maximum price for that symbol within the window as the value.                 */
//  SECRET_VERSION 
//  get the directory to put uncompressed archives in 
// Expecting 4*2^(failCount-1) 
// Now we know for sure that this is a bad id 
//  Redis has a chunk but no more 
//  File order of MD5 calculation is significant. Sorting is done on   unix-format names, case-folded, in order to get a platform-independent   sort and calculate the same MD5 on all platforms. 
//  Result.create() states that "You must ensure that the keyvalues are already sorted." 
//  if one group subscribes to the same stream with same partitioning multiple times,   merge those together (otherwise can end up with many output streams created for that partitioning   if need to split into multiple output streams because of same input having different   partitioning to the group) 
//  global variables only used internally in class 
//  shard iterator corresponding to position in shard for failed messages 
//  1 - acquire lock on dir 
//  Find the most recent child and load that. 
// At-most-once mode must commit tuples before they are emitted to the topology to ensure that a spout crash won't cause replays. 
// cached supervisor doesn't show up 
// else leader (NOOP) 
//  val xor value 
//  2) check log file content line count == tuples emitted + 1 
//  return the first message to be retried from the set. It will return the message with the earliest retry time <= current time 
//  amount of data written and rotation policies 
// This is possibly lossy in the case where a value is deleted   because it has received no messages over the metrics collection   period and new messages are starting to come in.  This is   because I don't want the overhead of a synchronize just to have   the metric be absolutely perfect. 
// locate login configuration  
// The key was removed so we should delete it too. 
//  namely the two eds on the orphaned worker and the healthy worker 
//  No instantiation 
// Compute the stats for these and save them 
// Not for this topology so skip it 
// Generate some that have neither resource, to verify that the strategy will prioritize this last 
// not_jump (open in not strict mode) 
//  REQUESTED_ON_HEAP_MEMORY 
//  Split up GZIP_MAGIC into readable bytes 
//  6) check log file content line count == tuples emitted + 1 
//  after which nimbus-1 comes back up and a read or update is performed. 
//  get existing assignment (just the topologyToExecutorToNodePort map) -> default to {}   filter out ones which have a executor timeout   figure out available slots on cluster. add to that the used valid slots to get total slots. figure out how many executors   should be in each slot (e.g., 4, 4, 4, 5)   only keep existing slots that satisfy one of those slots. for rest, reassign them across remaining slots   edge case for slots with no executor timeout but with supervisor timeout... just treat these as valid slots that can be   reassigned to. worst comes to worse the executor will timeout and won't assign here next time around 
//  REQUESTED_OFF_HEAP_MEMORY 
//  first block (0% - 10%) 
//  END Metrics 
// if any error/exception thrown, just ignore. 
//  start threads after metadata cache created 
// No node for basePath is OK, nothing to remove 
//  Test with long value 
/*                 * create a stream of (word, 1) pairs                 */
// HBASE tokens are not renewable so we always have to get new ones. 
// task to run 
//  class 
// initialize with storm configuration 
// Kafka throws their own type of exception when interrupted.  Throw a new Java InterruptedException to ensure Storm can recognize the exception as a reaction to an interrupt. 
//  {metric -> win -> value} ==> {win -> metric -> value} 
// else local (NOOP) 
// return null to mount options in string that is not part of cgroups 
//  past limit, quit 
//  The whole bytes were not received yet - return null. 
//  Executor summaries 
// metrics rpc  
//  net_cls,ns is not supported in ubuntu 
/*      * Returns map from task -> componentId      */
//  very stream name matches, it stream name was specified 
//  build new metadata based on emitted records 
// Drop the change notifications we are not running anything right now 
// Lets build a topology. 
//  iterate over tuples in the current window 
// ignored 
// Supervisor admin 
//  while holding currentLock to avoid deadlocks 
// obtain a context object 
//  protected using a lock on this counter 
// Again we don't want to exit because of logging issues. 
//  validate search by topology id 
//  canonically the metrics data exported is time bucketed when doing counts.   convert the absolute values here into time buckets. 
//  1) Simulate lock file lease expiring and getting closed by HDFS 
//  tuple values are mapped with 
//  refresh interval in msec   last time the command was performed   env for the command execution 
//  max rounds of scanning the dirs 
//  if no configs from loader, try to read from user-resource-pools.yaml 
//  REQUEST_ID 
//  service is off now just interrupt it. 
// Fail tuple 5 and 3, call nextTuple, then fail tuple 2 
//  this bolt dosen't emit to downstream bolts 
// Check that a reemit emits exactly the same tuples as the last batch, even if Kafka returns more messages 
//  EXECUTE_LATENCY_MS 
// each context will have a single client channel worker event loop group 
//  try to find a way to merge this code with what's already done in TridentBoltExecutor 
//  {component id -> metric -> value}, note that input may contain both long and double values 
//  common fields 
// 0-4 compacted away 
//  A stream of words 
//  this is fine b/c we still have a watch from the successful exists call 
//  Now, let's advance time. 
//  IOException from reading the version files to be ignored 
//  does not block 
// Check that non-null meta makes the spout seek according to the provided metadata, and that the returned meta is correct 
//  class MockCollector 
// The global stream id is this + the from component it must be a part of. 
// Invalidate the cache as something on the node changed 
//  Suppressing exceptions as we don't care for errors on connection close 
// Ignored 
//  STORM_ID 
//  List to array conversion 
// locate our thrift transport plugin 
// Switch to CachedGauge if this starts to hurt performance 
/*                  * Declare a separate 'punctuation' stream per output stream so that the receiving bolt                 * can subscribe to this stream with 'ALL' grouping and process the punctuation once it                 * receives from all upstream tasks.                  */
//  sync sending, will return a SendResult 
//  ACKED 
// DO NOT CHANGE UNLESS WE ADD IN STATE NOT STORED IN THE PARENT CLASS 
// STORM-3372: Rotation policy other than TimedRotationPolicy causes NPE on cleanup 
// user with no impersonation acl should be reject 
//  this method expected to be thread safe 
// We are done, nothing that short is going to work here 
// if it is is 0 or less it really is 1 per 10 seconds. 
//  preCommit can be invoked during recovery before the state is initialized 
// If it is not set a lot of things are not really going to work all that well 
//  distributed mode 
//  add special pathspec of static content mapped to the homePath 
//  INimbusCredentialPlugin 
// Ack both emitted tuples 
//  RESET_LOG_LEVEL 
// use JsonSerializer as the default serializer 
//  We call fireChannelRead since the client is allowed to   perform this request. The client's request will now proceed   to the next pipeline component namely StormClientHandler. 
//  sliding interval 
// if DRPC spout then id contains function 
//  partial writes of prior lines 
//  Deletes the state inside the zookeeper for a key, for which the 
/*  ILookup  */
//  Disconnects don't fail. 
// Nimbus itself 
//  make the new assignments for topologies 
//  late tuple emitted 
// Was a try-cause but I looked at the code around this and key not found is not wrapped in runtime,   so it is not needed 
// user jerry submits another topology 
//  key1 shouldn't in iterator 
//  blobstore directory is private! 
//  the merged configs are only for the reset logic 
// Now see if we can create a new token for bob and try again. 
// create a socket with server 
//  remove offsetManagers for all partitions that are no longer assigned to this spout 
// We need to have 3 slots on 3 separate hosts. The topology needs 6 GPUs 3500 MB memory and 350% CPU   The bolt-3 instances must be on separate nodes because they each need 2 GPUs.   The bolt-2 instances must be on the same node as they each need 1 GPU   (this assumes that we are packing the components to avoid fragmentation).   The bolt-1 and spout instances fill in the rest. 
//  when (empty) 
// Remove any expired keys after possibly inserting new ones. 
// We're either going to empty, or starting fresh blob download. Either way, the changing blob notifications are outdated. 
/*  Verify that the following acked (now committed) tuples are not emitted again         * Since the consumer position was somewhere in the middle of the acked tuples when the commit happened,         * this verifies that the spout keeps the consumer position ahead of the committed offset when committing          */
// flushed the buffers completely 
//  go to next file 
//  subscribe to parent's punctuation stream 
/*                      * set the current timestamp as the reference time for the eviction policy                     * to evict the events                      */
//  RESOURCES 
// We cannot connect if there is no client section in the jaas conf... 
//  Metrics 
// Skip any resources where the total is 0, the percent used for this resource isn't meaningful.  We fall back to prioritizing by cpu, memory and any other resources by ignoring this value 
// initialCapacity set to 11 since its the default inital capacity of PriorityBlockingQueue 
//  Raw input data to be scored (predicted)   PMML Model read from file - null if using Blobstore   PMML Model downloaded from Blobstore - null if using File 
//  ======== poll ========= 
//         ackingProducerSimulation(); // -- measurement 6 
//  if we can't find the resources directory in a resources jar or in the classpath just create an empty   resources directory. This way we can check later that the topology jar was fully downloaded. 
//  1) acquire locks on file1,file2,file3 
//  Successfully decoded a frame. 
//  we'll assume the metadata was recently used if still in the cache. 
//  number of events per window-partition 
//  baseDir/supervisor/usercache/user1/filecache 
//  reuse the retrieved iterator 
//  Add cassandra cluster contact points 
// Fail all emitted messages except the first. Commit the first. 
//  string does not exist, create using an unique string id and add to cache 
//  this class should be combined with RegisteredGlobalState 
/* Keeping it for backward compatibility */
//  required   required   optional   optional   optional 
//  use zk servers as backup if they exist 
// It's more likely to be a file read exception here, so we don't differentiate 
//  3) Select a new file if one is not open already 
// In some cases we might be limiting memory in the supervisor and not in the cgroups 
// It is possible that this component is already scheduled on this node or worker.  If so when we backtrack we cannot remove it 
// test we can re-emit the second batch 
//  outputsFields can be empty if this bolt acts like a sink in topology. 
//  iterating multiple times should produce same events 
//  executor0 resides one one worker (on one), executor1 and executor2 on another worker (on the other node) 
//  RESET_LOG_LEVEL_TIMEOUT_SECS 
//  optional 
//  We are pretending to be nimbus here. 
// Wait for a leader to be elected (or topology submission can be rejected) 
//  a stateful processor immediately follows a window specification 
/* user NOT authorized */
//  STORM_VERSION 
/*      * Validator definitions      */
//  should fail 
//  batch sync sending 
//  don't emit anything .. allow configured spout wait strategy to kick in 
// Downloading of all blobs finished. This is the precondition for all codes below. 
// Initial timeout 1 second.  Workers commit suicide after this 
//  since s2 read last, it should be evicted, s1 and s3 should exist 
//  run() 
//  numMatchesSought, numMatchesFound, expectedNextByteOffset 
/*  validate memory settings  */
//  Set principal in RebalanceOptions to nil because users are not suppose to set this 
//  add tick tuple each second to force acknowledgement of pending tuples. 
//  has successfully authenticated with this server. 
//  if state found for this shard in zk, then set the sequence number in fetchedSequenceNumber 
// Failing tuples for partitions that are no longer assigned is useless since the spout will not be allowed to commit them if they later pass 
//  required   required   required   required   required   optional   optional   optional   optional   optional   optional 
// errors is a bit special because in older versions of storm the worker created the parent directories lazily   because of this it means we need to auto create at least the topo-id directory for all running topos. 
//  look  Impt: HDFS timestamp may not reflect recent appends, so we double check the   timestamp in last line of file to see when the last update was made 
// 2) Clear cache 
//  throwing an exception 
//  set upper limit to how much cpu can be used by all workers running on supervisor node.   This is done so that some cpu cycles will remain free to run the daemons and other miscellaneous OS   operations. 
//  this gets called repeatedly for no apparent reason, don't do anything 
//  join users and stores on city name 
//  For testing only.. invoked via reflection 
//  first stream's data goes into the probe 
//  fail fast 
//  host,port => WorkerSummary 
//  files are world-wide readable and owner writable 
//  Storm tuple to redis key-value mapper 
//  add configs from resources like hdfs-site.xml 
// / oldComponentDebug.keySet()/ newComponentDebug.keySet() maybe be APersistentSet, which don't support addAll 
//  since we made user not authorized, component map is empty 
//  create StateUpdater with the given windowStoreFactory to remove triggered aggregation results form store 
//  get all metadata from the cache to put into the database   use a new map to prevent threading issues with writer thread 
//  Avro strings are stored using a special Avro Utf8 type instead of using Java primitives 
// Don't error if timer is shut down, happens when the elector is closed. 
//  with 20 tuples per second.  
//  comma separated list of zk connect strings to connect to zookeeper e.g. localhost:2181 
//  we lose the race, but it doesn't matter 
// fall throw on purpose 
// Yes we can delete something that is not 0 because of races, but that is OK for metrics 
//  Remove old connections atomically 
//  enum conversion 
//  NOOP 
// Don't permit path traversal for calls intended to read from the daemon logs 
//  put bolt message first, then put task ids 
// Make sure the parent directory is there and ready to go 
// Nothing is assigned yet, should emit nothing 
//  filter sys streams if necessary 
//  calling chooseTasks should be finished before refreshing ring   adjusting groupingExecutionsPerThread might be needed with really slow machine   we allow race condition between refreshing ring and choosing tasks   so it will not make exact even distribution, though diff is expected to be small   given that all threadTasks are finished before refreshing ring, 
// There could be collisions if keyToString returns only part of a result. 
//  object handling zk interaction 
//  search for the remaining bucket metrics. 
// GAIN_LEADERSHIP is a system event so don't log an issue 
// Only keep important conf keys 
//  javax.jms objects 
//  get the last successfully committed state from state store 
// Since we only give up leadership if we're waiting for blobs to sync,  it makes sense to wait a full sync cycle before trying for leadership again. 
//  keys shouldn't appear twice 
//  filter 
// If any error/exception thrown, report directly to nimbus. 
// all time 
//  This should not happen (localhost), but if it does we are still OK 
/*                  * May be the task restarted in the middle and the state needs be initialized.                 * Fail fast and trigger recovery.                  */
//  async sending 
//  'generateOutputFields' enables us to avoid projection unless it is the final stream being joined 
// when topology was launched 
//  -- lock timeout 
//  =====================================================================================   helper main.methods   =====================================================================================
// Something odd happened try again later 
// Groups this user is authorized to impersonate. 
//  zk session timeout in milliseconds 
// stream overrides 
// We evicted enough topologies to have a hope of scheduling, so try it now, and don't evict more   than is needed 
//  ERROR 
//  attempt to find it in the string cache 
//  validate search by executor id 
//  DATA_SIZE 
//  intended to not guarding with try-with-resource since otherwise test will fail 
// since using StringWriter, we do not need to close it. 
// Waiting for spout tuples isn't strictly necessary since we also wait for bolt emits, but do it anyway 
// storm-core is needed here for backwards compatibility. 
//  EVENTLOG_HOST 
/*              Below regex uses negative lookahead to not split in the middle of json objects '{}'             or json arrays '[]'. This is needed to parse valid json object/arrays passed as options             via 'storm.cmd' in windows. This is not an issue while using 'storm.py' since it url-encodes             the options and the below regex just does a split on the commas that separates each option.             Note:- This regex handles only valid json strings and could produce invalid results             if the options contain un-encoded invalid json or strings with unmatched '[, ], { or }'. We can             replace below code with split(",") once 'storm.cmd' is fixed to send url-encoded options.               */
//  node -> topologyId -> double 
//  Return a SaslTokenMessageRequest object 
// The OutputStream is done 
//  ASSIGNED_OFF_HEAP_MEMORY 
//  level 2 - parkNanos(1L) 
// create a framed transport 
//  Wait until time out 
// clean up for resource isolation if enabled 
//  2 -   DevNull Bolt   -------- 
//  release the reference on all blobs associated with this topology. 
//  LAST_ERROR 
// Check that a blob can be deleted when a temporary file exists in the blob directory 
//  Fields with embedded commas or double-quote characters 
// This means 1 to maxUncommitteddOffsets, but not maxUncommittedOffsets+1...maxUncommittedOffsets+maxPollRecords-1 
//  MESSAGE_BLOB 
//  no-op for zookeeper implementation 
//  max number of files to delete for every round 
//  print lookup result with low probability 
// Only go off of the topology id for now. 
//  never rollback 
//  create another spout to take over processing and read a few lines 
//  if delay or delay + current time are bigger than long max value 
// In this case we are ignoring the coord stuff, and only looking at 
//  Maximum number of retries 
//  2 -  Setup IdBolt & DevNullBolt   -------- 
// {nodeId -> {topologyId -> {workerId -> {execs}}}} 
//  all the JMS un-acked messages are going to be re-delivered   so clear the pendingAcks 
//  add all the owners to the map 
//  Prefer local tasks as target tasks if possible 
// latest offset 
//  if first failure add it to the count map 
//  handle no input 
/* user authorized */
//  drop back down. 
//  imperative that don't emit any tuples from here, since output factory cannot be gotten until   preparation is done, therefore, receivers won't be ready to receive tuples yet   can't emit tuples from here anyway, since it's not within a batch context (which is only   startBatch, execute, and finishBatch 
//  topology to tracking of topology dir and resources 
//  we skip uploading first one since we don't test upload for now 
// This can happen if the supervisor crashed after launching a   worker that never came up. 
// generate some that has alot of cpu but little of memory 
//  loop through the arguments, if we hit a list that has to be convered to an array,   perform the conversion 
// In the second part of the test the rate doubles to 20 per second but the rate tracker   increases its result slowly as we push the 10 tuples per second buckets out and relpace them 
//  process stream definitions 
//  FAILED 
//  validate port 
//  not configurable to prevent change between topology restarts 
// No topologies running so NOOP 
//  for windowed bolt, windowed output collector will do the anchoring/acking 
//  inject output bolt 
//  Non Blocking. returns true/false indicating success/failure. Fails if full. 
// scan.setCaching(1000); 
// Fail only the last tuple 
//  required   required   required   required   required   required   required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
//  consume file 1 partially 
//  Generate SASL response (but we only actually send the response if   it's non-null. 
//  generate random numbers 
//  do nothing except validate heartbeat for now. 
// retrieve authentication configuration  
//  send expire command for hash only once   it expires key itself entirely, so use it with caution 
//  Retry forever 
//  check is a minor optimization 
//  All writes/syncs will fail so this should cause a RuntimeException 
//  Now create a params map to put it in to our conf 
// By convention each share corresponds to 1% of a CPU core   or 100 = 1 core full time. So the guaranteed number of ms   (approximately) should be ... 
//  optional   optional   optional   optional 
// Returns messageIds in order of emission 
//  required   required   required   required   required   optional 
//  if fail count is greater than maxRetries, discard or ack. for e.g. for maxRetries 3, 4 failures are allowed at maximum 
/*  * EventHubs bolt configurations * * Partition mode: * With partitionMode=true you need to create the same number of tasks as the number of * EventHubs partitions, and each bolt task will only send data to one partition. * The partition ID is the task ID of the bolt. * * Event format: * The formatter to convert tuple to bytes for EventHubs. * if null, the default format is common delimited tuple fields.  */
//  guard NPE 
//  replaces normal aggregation here with a global grouping because it needs to be consistent across batches  
//  Add to our collection. 
// login our principal 
// find document from mongodb 
//  MSG 
// NO-OP 
// login our user 
// First check that maxUncommittedOffsets is respected when emitting from scratch 
//  1) Perform Join 
//  only host3 should be returned given filter 
// Verify correct wrapping/unwrapping of partition and delegation of partition assignment 
// for ChainedAggregator 
// Have a crummy cache to show off shared memory accounting 
//  if key already exists while creating the blob else update it 
// ensure that serializations are same for all tasks no matter what's on 
//  zk connection timeout in milliseconds 
// The following are required for backwards compatibility with clojure code 
//  spawn tar utility to untar archive for full fledged unix behavior such   as resolving symlinks in tar archives 
//        The contract of Rankable#copy() returns a Rankable value, not a RankableObjectWithFields. 
//  unauthorized 
// 1 create an already existing open file w/o override flag 
//  we could also make this static, but not to do it due to mock 
//  blobstore state 
//  key dir needs to be number 0 to number of buckets, choose one so we know where to look 
//  compute window length adjustment (delta) to account for time drift 
//  PROCESS_MS_AVG 
//  Backtracking algorithm does not take into account the ordering of executors in worker to reduce traversal space 
//  Ignore. 
//  Spout/Bolt object 
//  "************ Sampling Metrics ***************** 
// InputStream is done 
//  check if processor has specific priority first 
//  we skip uploading first one since we want to test rollback, not upload 
//  BITS 
//  Bolt implementation 
// read all the topologies 
//  required   optional   optional   optional   optional   optional   optional 
/*                  * Split the stream of numbers into streams of                 * even and odd numbers. The first stream contains even                 * and the second contains odd numbers.                  */
//  has been emitted and it is pending ack or fail 
//  if value is greater than Long.MAX_VALUE it truncates to Long.MAX_VALUE 
// use JsonScheme as the default scheme 
//  build components that may be referenced by spouts, bolts, etc.   the map will be a String --> Object where the object is a fully   constructed class instance 
//  If MapFunction is aware of prepare, let it handle preparation 
// two constructor signatures used to initialize validators.  One constructor takes input a Map of arguments, the other doesn't take any arguments (default constructor)  If validator has a constructor that takes a Map as an argument call that constructor 
// connect 
//  Configure for achieving max throughput in single worker mode (empirically found).       -- Expect ~5.3 mill/sec (3.2 mill/sec with batchSz=1)       -- ~1 mill/sec, lat= ~20 microsec  with acker=1 & batchSz=1 
//  Return a TaskMessage object 
//  static fields 
//  METRICS 
//  if maxRetries is 0, dont retry and return false as per interface contract 
//  find the number of data bytes + length byte 
//  SHARED_MEM_OFF_HEAP 
//  Add new connections atomically 
//  1 -  Setup StringGen Spout   -------- 
//  get num_tasks 
//  watermark events are not added to the queue. 
// Some mount options (i.e. rw and relatime) in type are not cgroups related 
// Bolts 
/*                          * Update histograms based on the new summary. Most common implementation of Reporter reports Gauges before                         * Histograms. Because DerivativeGauge will trigger cache refresh upon reporter's query, histogram will also be                         * updated before query                          */
//  employed when no incoming data   employed when outbound path is congested 
// Update the OffsetManager for each committed partition, and update numUncommittedOffsets 
//  Create keyspace not supported in the current datastax driver 
//  Suppressing exceptions as we don't care for errors on heartbeats 
// Schema changes should have forced file rotations 
//  Test3: When a supervisor and a worker on it fails, RAS does not alter existing assignments 
//  finish up reading the file 
//  baseDir/supervisor/usercache/ 
/*      * Passing a factory rather than the actual object to avoid enforcing the strong     * requirement of having to have ModelRunner to be Serializable      */
//  SCRIPT 
// Initial Setup 
//  string does not exist in database 
//  We don't use the classpath part of this, so just an empty list 
// Treat reassign as remove and add 
//  Get latest sequence number of the blob present in the zookeeper --> possible to refactor this piece of code 
// Throttle this spout a bit to avoid maxing out CPU 
//  whatever remains in the tab are non matching left rows. 
//  explicit anchoring emits to corresponding input tuples only, as default window anchoring will anchor them to all   tuples in window 
// Scheduling Status set to FAIL_OTHER so no eviction policy will be attempted to make space for this topology 
//  banner 
// We didn't find it, but there are races, so we want to check again after a sync 
//  empty if no factoryArgs 
//  Make the assignment null to let slot clean up the disk assignment. 
//  If authentication of client is complete, we will also send a   SASL-Complete message to the client. 
// race condition with another thread, and we lost   try again 
//  Convenience alternative to prepare() for use in Tests 
//  Use while loop, try to decode as more messages as possible in single call 
/*          * Windows should be aligned to the slide size, starting at firstWindowEndTime - windowSec.         * Because all windows are aligned to the slide size, we can partition the spout emitted timestamps by which window they should fall in.         * This checks that the partitioned spout emits fall in the expected windows, based on the logs from the spout and bolt.          */
//  it shouldn't propagate any exceptions 
/* Get one message at a time for backward compatibility behaviour */
// 3 seconds from now 
// Spout 
//  update the shard iterator to next one in case this fetch does not give the message. 
//  if it is local mode, just get the local nimbus instance and set the heartbeats 
//  ensure that choice1 and choice2 are not the same task 
// No need to extract it, it is not what we are looking for. 
/*                 * Queries the state and emits the                * matching (key, value) as results. The stream state returned                * by the updateStateByKey is passed as the argument to stateQuery.                 */
//  keep committing when topology is deactivated since ack and fail keep getting called on deactivated topology 
//  =====================================================================================   heartbeats related   ===================================================================================== 
//  Join helper to concat fields to the record 
//  convert all strings to numeric Ids for the metric key and add to the metadata cache 
// Once Storm is baselined to Java 11, we can use URLDecoder.decode(String, Charset) instead, which obsoletes this method. 
// make sure that defined key is string in case wrong stuff got put into Config.java 
//  Writing random words to be blacklisted 
//  more ACLs can be added here 
//  2) wait for all 3 locks to expire then heart beat on 2 locks and verify stale lock 
//  ID_TO_SPOUT_AGG_STATS 
// Emit all messages and fail the first one while acking the rest 
//  =====================================================================================   aggregation stats main.methods   =====================================================================================
//  1 - parse cmd line args 
//  schedule 1st topology 
//  A singleton instance allows us to mock delegated static main.methods in our   tests by subclassing.
//  We purposely simulate a 1 second bucket size so the rate will always be 10 per second. 
// wordspout -> lookupbolt 
//  STRING_ARG 
//  you cant define a topologySource and a DSL topology at the same time... 
//  can be null;   nested field "x.y.z"  becomes => String["x","y","z"]   either "stream1:x.y.z" or "x.y.z" depending on whether stream name is present. 
//  Task Id not used, so just pick a static value 
// make sure all workers on scheduled in rack-0 
//  REQUESTED_MEMOFFHEAP 
// For testing... 
// Parallelism is double 
// Two distinct schema should result in only two files 
//  initialize server-side SASL functionality, if we haven't yet   (in which case we are looking at the first SASL message from the 
// This spout owns 1 partitions: 6 
//  Then get it and return the file as string. 
// Sliding windows should produce one window every slideSize tuples  Wait for the spout to emit at least enough tuples to get minBoltEmit windows and at least one full window 
// Add in all of the components 
// Monitor for assignment changes as often as possible, so e.g. shutdown happens as fast as possible. 
//  User defined Callback 
// Let worker tokens work on insecure ZK... 
// We were rescheduled while waiting for the resources to be updated,   but the container is already not running. 
// nodeHost is not null here, as newConnections is only non-empty if assignment was not null above.   Host   Port 
//  supervisor port should be only presented to worker which supports RPC heartbeat 
//  update the value in state 
//  The first seek offset for each topic partition, i.e. the offset this spout instance started processing at. 
//  Cassandra daemon calls System.exit() on windows, which kills the test. 
// Rotate once per timeout period that has passed since last time this was called.  This is necessary since this method may be called at arbitrary intervals. 
// Timer is discarded after the initial launch of an assignment 
// check resources 
//  later on, this will be joined back with return-info and all the results 
//  based on transactional topologies 
// Thread.sleep(120); 
//  JMS Queue Provider 
// remove resources 
// This is to verify that a low maxPollRecords does not interfere with reemitting failed tuples 
// If the node does not exist it is fine/expected... 
//  default is a sliding window of count 1 
/*           Verify that failed offsets will only retry if the corresponding message exists.           When log compaction is enabled in Kafka it is possible that a tuple can fail,           and then be impossible to retry because the message in Kafka has been deleted.          The spout needs to quietly ack such tuples to allow commits to progress past the deleted offset.          */
//  BOOL_ARG 
//  distribution should be exactly even 
//  attempt to find it in callers cache 
//  benchmark label 
//  messages. 
// Merge the old credentials so creds nimbus created are not lost.   And in case the user forgot to upload something important this time. 
//  generated code will be not compilable since return type of MYPLUS and type of 'x' are different 
//  examine the response message from server 
//  force triggers building ring 
//  Nothing here, could not get message id 
//  required 
//  try to use zookeeper secret 
/*      * Check for uncaught exceptions during the execution     * of the trigger and fail fast.     * The uncaught exceptions will be wrapped in     * ExecutionException and thrown when future.get() is invoked.      */
// We need to re-login some other thread might have logged into hadoop using   their credentials (e.g. AutoHBase might be also part of nimbu auto creds) 
//  When this master is not leader and get a sync request from node,   just return nil which will cause client/node to get an unknown error,   the node/supervisor will sync it as a timer task. 
//  size matches, check if the streams are expected 
//  emulate the call of withLateTupleStream method 
// We set the stddev of the skewed keys to be 1/5 of the length, but then we use the absolute value   of that so everything is skewed towards 0 
//  storm hbase keytab /etc/security/keytabs/storm-hbase.keytab 
//  if there are no windowed/batched processors, we would ack immediately 
// The spout should not emit any more tuples. 
//  Helper static vars for each platform 
//  TASK_END 
//  DO NOT CHANGE THIS TO SYSOUT 
//  1- try adding to main queue if its overflow is not empty 
// bolt-1  bolt-2  bolt-3 
//  if no new assignment. 
//  Generally used to compare how files were actually written and compare to expectations based on total 
// do retries if the connect fails 
//  then stop running it 
//  COMMON_STATS 
//  Find the task the events from this component route to. 
//  handle Ctrl-C 
//  if the empty partition was not invalidated by flush, but evicted from cache 
//  componentExecutors maybe be APersistentMap, which don't support "put" 
// It is possible to get asked about eviction before we have a context, due to WindowManager.compactWindow.  In this case we should hold on to all the events. When the first watermark is received, the context will be set,  and the events will be reevaluated for eviction 
//  add tuple to the batch state 
//  Redis config parameters for the RedisStoreBolt 
//  this is fine because the only time this is shared is when it's a local context, 
//  USED_PORTS 
//  SCHEDULER_META 
//  the oldest pq_size files in this directory will be placed in PQ, with the newest at the root 
//  required   optional 
// make sure all workers on scheduled in rack-1 
//  7) read remaining lines in file, then ensure lock is gone 
//  get a currently unused unique string id 
//  Use streamId, source component name OR field in tuple to distinguish incoming tuple streams 
// The spout must reemit retriable tuples, even if they fail out of order.  The spout should be able to skip tuples it has already emitted when retrying messages, even if those tuples are also retries. 
//  business logic. 
// user jerry submits another topology but this one should be scheduled since it has higher priority than than the 
// The last slot fill it up 
// Operation level IO Exceptions 
//  Extract bolt resource info 
//  EXECUTED 
//  sort by port: from small to large 
// Sort comps by number of constraints 
// Build new metadata based on the consumer position.  We want the next emit to start at the current consumer position,  so make a meta that indicates that position - 1 is the last emitted offset  This helps us avoid cases like STORM-3279, and simplifies the seek logic. 
// Emit 10 empty batches, simulating no new records being present in Kafka 
//  In at-most-once mode the offsets are committed after every poll, and not periodically as controlled by the timer 
// Test if topology is already partially scheduled on one rack 
//  ASSIGNED_SHARED_OFF_HEAP_MEMORY 
/*          * total tuples should be         * recovered (batch-1 + batch-2) + replayed (batch-3)          */
//  It's possible that permissions were not set properly on the directory, and   the user who is *supposed* to own the dir does not. In this case, try the   delete as the supervisor user. 
// Create the directory no matter what. This is so we can check if it was downloaded in the future. 
// Busy wait... 
// short 
// Check that the emitter can handle emitting empty batches on a new partition.  If the spout is configured to seek to LATEST, or the partition is empty, the initial batches may be empty 
//  String metastoreDBLocation = "jdbc:derby:databaseName=/tmp/metastore_db;create=true";   conf.set("javax.jdo.option.ConnectionDriverName","org.apache.derby.jdbc.EmbeddedDriver");   conf.set("javax.jdo.option.ConnectionURL",metastoreDBLocation); 
// This is here to avoid an overridable call in the constructor 
// pending is empty, return the smallest in toResend 
//  attempt scheduling both topologies.   this triggered negative resource event as the second topology incorrectly scheduled with the first in place 
// Ignored the file system we are on does not support this, so don't do it. 
//  kill process when timeout 
// find login file configuration from Storm configuration 
//  compareAndSet added because of https://issues.apache.org/jira/browse/STORM-1535 
// Test for subject with no principals 
//  ################# Connect Callback Implementation ###################### 
//  in other case, just set this to 0 to trigger re-sync later 
//  force process to be terminated 
//  Acquire a slot 
//  returns list of list of slots, reverse sorted by number of slots 
// play and fail 1 tuple 
//  delete 
//  doing below because its affecting storm metrics most likely   had to make tick tuple a mandatory argument since its positional 
// Thread safety is mostly around acl.  If acl needs to be updated it is changed atomically  More then one thread may be trying to update it at a time, but that is OK, because the  change is atomic 
// Could be replaced when metrics support remove all functions   https://github.com/dropwizard/metrics/pull/1280 
//  required   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional   optional 
// else we assume it is the same as teh SUPER_ACL which is covered by CREATOR_ALL 
//  topo5 has 40 small tasks, it should be able to exactly use up both the cpu and mem in the cluster 
//  Ensure that the original batch1 is discarded and new one is persisted. 
// It was not there before so we need to run it. 
//  A stream of random sentences 
//  no reference to archive1 
// By default all of the values are 0 
//  object representing information on paramaters to use while connecting to kinesis using kinesis client 
//  1) read 5 lines in file 
//  A no-op grouper 
// Release things that don't need to wait for us 
//  trigger manually to avoid timing issues 
//  actual value of m is: Map<String, Map<String/GlobalStreamId, Long/Double>> ({win -> stream -> value}) 
// File 2 should be read 
//  check to see if any shard has already fetched records waiting to be emitted, in which case dont fetch more 
//  for testing 
// In local mode the main process should never exit on it's own 
// First delete everything that no longer exists... 
//  KILL_OPTIONS 
//  additional tests that go beyond what this test is primarily about 
// Old Buckets and their length are only touched when rotating or gathering the metrics, which should not be that frequent   As such all access to them should be protected by synchronizing with the RateTracker instance 
// a null tuple should be added to the ack list since by definition is a direct ack 
// Don't know a better way to validate that it failed. 
// The tick should NOT cause any acks since the batch was empty except for acking itself 
//  We have three mutually disjoint treesets per shard at any given time to keep track of what sequence number   can be committed to zookeeper.   emittedPerShard, ackedPerShard and failedPerShard. Any record starts by entering emittedPerShard. On ack   it moves from emittedPerShard to   ackedPerShard and on fail if retry service tells us to retry then it moves from emittedPerShard to   failedPerShard. The failed records will move from   failedPerShard to emittedPerShard when the failed record is emitted again as a retry.   Logic for deciding what sequence number to commit is find the highest sequence number from ackedPerShard   called X such that there is no sequence   number Y in emittedPerShard or failedPerShard that satisfies X > Y. For e.g. if ackedPerShard is 1,4,5,   emittedPerShard is 2,6 and   failedPerShard is 3,7 then we can only commit 1 and not 4 because 2 is still pending and 3 has failed 
//  use exhibitor servers 
//  reserved for saving topology data 
//  Map of records  that were fetched from kinesis as a result of failure and are waiting to be emitted 
//  this is so we can do things like have simple DRPC that doesn't need to use batch processing 
//  Stop iterating in the middle of the 10th partition 
//  COMPONENT_TYPE 
//  Configure the server. 
// Assign partitions to the spout 
//  ok if its return null 
// Should assert file size 
//  APPROVED_WORKERS 
//  JMS Queue Spout 
//  First make the cache dir 
// Emit some more messages 
// Just in case we get something we are confused about   we can continue processing the rest of the tasks 
//  We need to add the new futures to the existing ones 
//  Logging an exception while client is connecting 
// The scheduler generally will try to pack executors into workers until the max heap size is met, but   this can vary depending on the specific scheduling strategy selected.   The reason for this is to try and balance the maximum pause time GC might take (which is larger for larger heaps)   against better performance because of not needing to serialize/deserialize tuples. 
//  wordSpout ==> countBolt ==> MongoInsertBolt 
//  IS_LEADER 
// unknown error, just skip 
//  =================  Factory Methods Declaring ModelOutputs to Multiple Streams  ================== 
//  SESSION 
//  EXECUTOR_NODE_PORT 
//  index and fieldIndex are precomputed, delegates built up over many operations using persistent data structures 
//  VALIDATION ONLY CONFIGS   Some configs inside Config.java may reference classes we don't want to expose in storm-client, but we still want to validate   That they reference a valid class.  To allow this to happen we do part of the validation on the client side with annotations on   static final members of the Config class, and other validations here.  We avoid naming them the same thing because clojure code   walks these two classes and creates clojure constants for these values. 
//  PULSES 
//  common 
//  required for jackson serialization 
//  timestamp in millisecond, 0 means   disabling filter 
// TODO batch finding 
//  perf critical loop. dont use iterators 
//  COMPONENT_ID 
//  -- commit frequency - seconds 
// There is a race here that we can still lose 
// JPROFILE STOP 
// class 
/*      * aggWorkerStats tests      */
// Break the code if out of sync to thrift protocol 
//  server. 
// status of scheduling the topology e.g. success or fail? 
// with realm e.g. storm@WITZEND.COM 
//  calc sid->output_stats 
//  MS 
//  The removed writer should have been closed 
//  (JMOCK-263).  See https://github.com/jmock-developers/jmock-library/issues/22 for more information. 
//  convert each record into a HashMap using fieldNames as keys 
// used for local cluster heartbeating 
//  SERVICE_TYPE 
//  apply new log settings we just received 
//  g1 being null means the source is a spout node 
/*  LOOK AT HDFS BLOBSTORE  */
// The first tuple should be acked, and should not have failed 
// This section simply put the formatted log filename and corresponding port in the matching. 
//  TOPO_OWNER 
//  filter configured, should fail all users 
//  Validating class implementation could fail on non-Nimbus Daemons.  Nimbus will catch the class not found on startup   and log an error message, so just validating this as a String for now. 
//  3 - if clocks are not in sync .. 
//  JDK_VERSION 
//  4) Read record from file, emit to collector and record progress 
//  This should always be set to digest. 
// We were rescheduled while waiting for the worker to come up 
//  get from DB and add to lookup cache 
//  list all files for this topology 
// examine the response message from server 
// Alice has no digest jaas section at all... 
//  test basic substitution 
//  monotonically increasing id for correlating sent/recvd msgs. ok if restarts from 0 on crash. 
//  for unit tests 
//  just mkdir STORM_ZOOKEEPER_ROOT dir 
// Nothing else should be emitted, all tuples are acked except for the final tuple, which is pending. 
//  TOPOLOGY_NAME 
//  ensure checkpoint interval is not less than a sane low value. 
//  END TOPOLOGY STATE TRANSITIONS 
// get metric name 
//  matches and matchCount is not changed 
// But throughput is the same 
/* without Auth */
//  CREDS 
//  tracks order in which msg came in 
// The first tuple wil be used to check timeout reset 
//  hbase principal storm-hbase@WITZEN.COM 
//  write the tuple to a JMS destination... 
// call updateMetricFromRPC with params 
// This is here only for testing. 
//  Do nothing 
//  Preserve interrupt status 
//  How many states searched so far. 
//  NOOP no need to create links in local mode 
//  Usage: Let it and then explicitly terminate.   Metrics will be printed when application is terminated. 
/*                      * propagate it so that task gets canceled and the exception                     * can be retrieved from executorFuture.get()                      */
//  allow blacklist scheduler to cache the supervisor with an added port 
//  flushes local and remote messages 
// to allow the revocation hook to commit offsets for the revoked partitions. 
//  roll to next 
//  topology will be scheduled 
// Ensure filename doesn't contain ../ parts  
//  re-init cache and partitions 
//  Boilerplate overrides to cast result from base type to JoinBolt, so user doesn't have to   down cast when invoking these main.methods
// case 2: Non-IContext plugin must have a makeContext(topoConf) method that returns IContext object 
// metrics has just been collected, let's also log it 
//  Not exposed:   * withClock(Clock) 
//  validate search by component id 
//  remove any previously created cache instance 
// release earliest blacklist - but release all supervisors on a given blacklisted host. 
// Stop counting when past current time 
//  this can happen when multiple clients doing mkdir at same time 
//  remove uploaded jars blobs, not artifacts since they're shared across the cluster 
// creates brand new tuples with brand new fields 
// Topology metrics distribution 
//  it's a method with zero args 
// If not call default constructor 
//  it's non-null. 
// STORM-3087. 
// Done capturing topology information... 
//  ======== Next Tuple ======= 
//  the metrics processor is not critical to the operation of the cluster, allow Supervisor to come up 
//  add more events with past ts 
/*      * force create a windowed bolt with identity nodes so that we don't     * have a stateful processor inside a windowed bolt.      */
//  INT_ARG 
//  configs - kafka spout 
//  It's likely that Bolt is shutting down so no need to die.   just ignore and loop will be terminated eventually 
//  get all the tuples in a batch and add it to trident-window-manager 
//  do not process current timestamp since tuples might arrive while the trigger is executing 
// When tuple tracking is enabled, the spout must not replay tuples in at-most-once mode 
//  if there are no records do not call flush 
//  add a null for missing fields (usually in case of outer joins) 
// We don't want to run the test is CGroups are not setup 
// Initialize the network topography 
//  read remaining lines 
//  resolve references 
//  for completeness 
// The topology we are scheduling 
//  if the node is already deleted, do nothing
//  should still fail 
//  ============  Factories  ============ 
//  Test2: When a supervisor fails, RAS does not alter existing assignments 
//  start trigger once the initialization is done. 
//  reference to key2 
// backtracking (If we ever get here there really isn't a lot of hope that we will find a scheduling) 
//  Kafka consumer configuration 
//  ensure 1 instance per producer thd. 
//  WINDOW_TO_COMPLETE_LATENCIES_MS 
// Ignored it is not set 
// Now fail a tuple on partition one and verify that it is allowed to retry, because the failed tuple is below the maxUncommittedOffsets limit 
/*  Ack the tuple, and commit.         *          * The waiting to emit list should now be cleared, and the next emitted tuple should be the last tuple on the partition,         * which hasn't been emitted yet          */
//  be kept in memory to avoid going to kinesis again for retry 
/*  Counted  */
//  isset id assignments 
//  the supervisors. this also allows you to declare the serializations as a sequence 
// Should not show files outside daemon log root. 
//  class FileLockingThread 
//  Closing the channel and reconnecting should be done before handling the messages. 
//  All internal state except for the count of the current bucket are 
//  if # of workers requested is more than we currently have 
//  STREAM_ID 
//  PATH 
// Now add some more events and a new watermark, and check that the previous events are expired 
// tasks figure out what tasks to talk to by looking at topology at runtime 
/*          * Instead of iterating over all the tuples in the window to compute         * the sum, the values for the new events are added and old events are         * subtracted. Similar optimizations might be possible in other         * windowing computations.          */
// Answer when we ask for a private key... 
//  ======== Fail ======= 
//  SCHED_STATUS 
//  pick a worker to mock as failed 
//  batch 1 
//  no data 
// Offset 0 is committed, 1 to maxUncommittedOffsets - 1 are failed but not retriable  The spout should now emit another maxPollRecords messages 
//  only start those requested 
// set to use the default resource aware strategy when using the MultitenantResourceAwareBridgeScheduler 
/*      * Bolt-specific configuration for windowed bolts to specify the window length as a count of number of tuples     * in the window.      */
//  TOPOLOGY_CONF 
// check format 
//  remove the port from the supervisor and make sure the blacklist scheduler can remove the port without 
// create a wrap transport factory so that we could apply user credential during connections 
//  component id -> stats 
//  invalidate after releasing the lock   if the parition is pinned before we could invalidate,   it will get invalidated in the next flush or when the entry gets evicted from the cache. 
// Link the components together 
//  Mark the current buffer position before reading task/len field   because the whole frame might not be in the buffer yet.   We will reset the buffer position to the marked position if 
//  attempt to find it in the string cache, this will update the LRU 
//  Kafka spout configuration 
//  json record doesn't need columns to be in the same order   as table in hive. 
/*      * The interval between retries of an Exhibitor operation.      */
//  value fields 
// Running in daemon mode, we would pass Error to calling thread. 
//  batch 2 
//  resources assigned by RAS (Resource Aware Scheduler) 
//  c.f. org.apache.hadoop.security.UserGroupInformation. 
// We found a good result we are done. 
// pending is still empty 
//  informs other workers about back pressure situation. Runs in the NettyWorker thread. 
// Sleep for 50 mins 
//  Legacy resource parsing 
//  required   required 
//  last offset of this batch 
// given processornodes and static state nodes 
/*  CREATE THE BLOBSTORE  */
// The window boundaries are )windowStart, windowEnd) 
//  batch 3 
//  e.g. [\"a\", \"b\", \"a\"]) => [\"a\", \"b\", \"a#2\"]" 
// STORM-3059 
// We could make this configurable in the future... 
//  sub process used to execute the command 
//  that worker is running and moves on 
//  map to track next retry time for each kinesis message that failed 
//  EXECUTOR_STATS 
//  rwx-------- 
//     long crc32(Tuple tuple); 
//  SPOUT 
//  need more data 
//  wait so all events expire 
// Create a new session id if the user gave an empty session string.   This is the use case when the user wishes to list blobs   starting from the beginning. 
//  HOST 
//  to be called after all Executor objects in the worker are created and before this object is used 
//  Redis has three chunks which last chunk only has entries 
//  Read property file for extra consumer properties 
// If debug logging is turned on we should just log the leader all the time.... 
//  If lock file has expired, then own it 
// iterate of WorkerSummary and find the one with the port 
// Now we can calculate a percentage 
//  Queue of records per shard fetched from kinesis and are waiting to be emitted 
//  Change the '1' to e.g. 5, to change this to 5 minutes. 
//  TIME_STAMP 
// spout overrides 
// merge with existing statuses 
//  at least 4K 
//  sorted set of records to be retrued based on retry time. earliest retryTime record comes first 
// check that only subscribed to one component/stream for statespout  setsubscribedstate appropriately 
//  Factory main.methods
//  Get sequence number details from latest sequence number of the blob 
/*                  * Create a stream of random numbers from a spout that                 * emits random integers by extracting the tuple value at index 0.                  */
//  Schedule topology history cleaner 
//  will commit progress into lock file if commit threshold is reached 
//  2 - Create and configure topology 
// map to hold partition level and topic level metrics 
//  setup some files/dirs to emulate supervisor restart 
// The versions are different so roll back to whatever current is 
// only for test 
// supervisor contains bad slots 
//  could/should use readFully(buffer,0,length)?
// Save the current state for recovery 
//  sub directories to store either files or uncompressed archives respectively 
//  DEPENDENCY_JARS 
//  K/sec 
// The subset of earliest retriable offsets that are on pollable partitions 
// Convert to millis 
//  WORKER_SUMMARIES 
//  class Configs 
//  We call fireChannelRead since the client is allowed to perform   this request. The client's request will now proceed to the next 
//  -- enable/disable ACKing 
//  if no TGT, do not bother with ticket management. 
// fileOffset = the index of last scanned file 
// The failed tuples are ready for retry. Make it appear like 0 and 1 on the first partition were compacted away.  In this case the second partition acts as control to verify that we only skip past offsets that are no longer present. 
// Worker dirs 
// At this point there is nothing to do.  In all likelihood any filesystem operations will fail.  The next tuple will almost certainly fail to write and/or sync, which force a rotation.  That  will give rotateAndReset() a chance to work which includes creating a fresh file handle. 
//  required   required   required   required   required   required 
//     final StormTopology topology; 
// logLevel 
//  in which case it's a noop 
// Verify that the poll started at the earliest retriable tuple offset 
//  now check memory only 
//  continue without idling 
//  init the writer once the cache is setup 
//  this is because can't currently merge splitting logic into a spout   not the most kosher algorithm here, since the grouper indexes are being trounced via the adding of nodes to random groups, but it 
//  Load PMML Model from Blob store 
// add to nimbuses 
// On heap memory is used to help calculate the heap of the java process for the worker   off heap memory is for things like JNI memory allocated off heap, or when using the   ShellBolt or ShellSpout.  In this case the 16 MB of off heap is just as an example   as we are not using it. 
//  expectedOwner being null means that security is disabled (which why are we uploading credentials with security disabled??? 
// AllowedId is null in the constructor, so it can assign what it needs/etc. 
// Verify that no more tuples are emitted and all tuples are committed 
/*                  * Emitted messages for partitions that are no longer assigned to this spout can't be acked and should not be retried, hence                 * remove them from emitted collection.                  */
//  treat supervisor as bad only if all of its slots matched the cached supervisor   track how many times a cached supervisor has been marked bad 
//  spouts 
//  ack-ed once 
//  Get or load from the cache optionally pinning the entry   so that it wont get evicted from the cache 
//  key for hdfs Kerberos configs 
